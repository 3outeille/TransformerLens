[1mdiff --git a/.gitignore b/.gitignore[m
[1mindex 28904d6..a28b8fd 100644[m
[1m--- a/.gitignore[m
[1m+++ b/.gitignore[m
[36m@@ -1,8 +1,24 @@[m
 __pycache__[m
[32m+[m[32m<<<<<<< HEAD[m
[32m+[m[32mTesting_Notebook.ipynb[m
[32m+[m[32m\.*[m
[32m+[m[32measy_transformer/scratch.py[m
[32m+[m[32mwandb*[m
[32m+[m[32measy_transformer\.egg*[m
[32m+[m[32mMANIFEST.in[m
[32m+[m[32msettings.ini[m
[32m+[m[32m_proc[m
[32m+[m[32mcore.py[m
[32m+[m[32m*.yml[m
[32m+[m[32mnbs[m
[32m+[m[32m_modidx.py[m
[32m+[m[32m*.zip[m
[32m+[m[32m=======[m
 *.db [m
 *.pdf[m
 *.svg [m
 *.csv[m
 *.zip[m
 *.json[m
[31m-*.png[m
\ No newline at end of file[m
[32m+[m[32m*.png[m
[32m+[m[32m>>>>>>> alex-global-patching[m
[1mdiff --git a/EasyTransformer_Demo.ipynb b/EasyTransformer_Demo.ipynb[m
[1mindex d3926fb..e0c36b9 100644[m
[1m--- a/EasyTransformer_Demo.ipynb[m
[1m+++ b/EasyTransformer_Demo.ipynb[m
[36m@@ -11,12 +11,12 @@[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "#Setup"[m
[32m+[m[32m    "# Setup"[m
    ][m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 1,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
[36m@@ -25,6 +25,16 @@[m
      "text": [[m
       "Running as a Jupyter notebook - intended for development only!\n"[m
      ][m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stderr",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "/var/folders/4v/0zf3hymd1t15659kvsccpr1r0000gn/T/ipykernel_70281/3930878945.py:12: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",[m
[32m+[m[32m      "  ipython.magic(\"load_ext autoreload\")\n",[m
[32m+[m[32m      "/var/folders/4v/0zf3hymd1t15659kvsccpr1r0000gn/T/ipykernel_70281/3930878945.py:13: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",[m
[32m+[m[32m      "  ipython.magic(\"autoreload 2\")\n"[m
[32m+[m[32m     ][m
     }[m
    ],[m
    "source": [[m
[36m@@ -32,6 +42,7 @@[m
     "  import google.colab\n",[m
     "  IN_COLAB = True\n",[m
     "  print(\"Running as a Colab notebook\")\n",[m
[32m+[m[32m    "\n",[m
     "except:\n",[m
     "  IN_COLAB = False\n",[m
     "  print(\"Running as a Jupyter notebook - intended for development only!\")\n",[m
[36m@@ -45,23 +56,33 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 2,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "DEBUG_MODE = False\n",[m
[32m+[m[32m    "import plotly.io as pio\n",[m
[32m+[m[32m    "if IN_COLAB or not DEBUG_MODE:\n",[m
[32m+[m[32m    "    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n",[m
[32m+[m[32m    "  pio.renderers.default = \"colab\"\n",[m
[32m+[m[32m    "else:\n",[m
[32m+[m[32m    "  pio.renderers.default = \"vscode\"\n"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": 3,[m
    "metadata": {},[m
    "outputs": [],[m
    "source": [[m
     "import os\n",[m
[31m-    "import torch\n",[m
[31m-    "if os.environ[\"USER\"] == \"exx\": # so Arthur can safely use octobox\n",[m
[31m-    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",[m
[31m-    "assert torch.cuda.device_count() == 1\n",[m
[31m-    "\n",[m
     "if IN_COLAB:\n",[m
     "    os.system('pip install git+https://github.com/neelnanda-io/Easy-Transformer.git')"[m
    ][m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 4,[m
    "metadata": {},[m
    "outputs": [],[m
    "source": [[m
[36m@@ -86,8 +107,6 @@[m
     "import matplotlib.pyplot as plt\n",[m
     "%matplotlib inline\n",[m
     "import plotly.express as px\n",[m
[31m-    "import plotly.io as pio\n",[m
[31m-    "pio.renderers.default = \"colab\"\n",[m
     "import plotly.graph_objects as go\n",[m
     "\n",[m
     "from torch.utils.data import DataLoader\n",[m
[36m@@ -101,26 +120,26 @@[m
     "# import comet_ml\n",[m
     "import itertools\n",[m
     "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",[m
[31m-    "import dataclasses\n"[m
[32m+[m[32m    "import dataclasses\n",[m
[32m+[m[32m    "import datasets\n"[m
    ][m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 5,[m
    "metadata": {},[m
    "outputs": [],[m
    "source": [[m
[31m-    "from easy_transformer.utils import gelu_new, to_numpy, get_corner #helper functions\n",[m
[31m-    "from easy_transformer.hook_points import HookedRootModule, HookPoint\n",[m
[31m-    "from easy_transformer.EasyTransformer import EasyTransformer,TransformerBlock, MLP, Attention, LayerNormPre, PosEmbed, Unembed, Embed\n",[m
[31m-    "from easy_transformer.experiments import ExperimentMetric, AblationConfig, EasyAblation, EasyPatching, PatchingConfig\n",[m
[31m-    "from easy_transformer.EasyTransformerConfig import EasyTransformerConfig\n",[m
[31m-    "\n"[m
[32m+[m[32m    "from easy_transformer.utils import gelu_new, to_numpy, get_corner, lm_cross_entropy_loss # Helper functions\n",[m
[32m+[m[32m    "from easy_transformer.hook_points import HookedRootModule, HookPoint # Hooking utilities\n",[m
[32m+[m[32m    "from easy_transformer import EasyTransformer, EasyTransformerConfig\n",[m
[32m+[m[32m    "import easy_transformer\n",[m
[32m+[m[32m    "from easy_transformer.experiments import ExperimentMetric, AblationConfig, EasyAblation, EasyPatching, PatchingConfig\n"[m
    ][m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 6,[m
    "metadata": {},[m
    "outputs": [],[m
    "source": [[m
[36m@@ -131,7 +150,7 @@[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "#Hook Points"[m
[32m+[m[32m    "# Hook Points"[m
    ][m
   },[m
   {[m
[36m@@ -169,7 +188,7 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 7,[m
    "metadata": {},[m
    "outputs": [],[m
    "source": [[m
[36m@@ -217,12 +236,12 @@[m
    "source": [[m
     "We can add a cache, to save the activation at each hook point\n",[m
     "\n",[m
[31m-    "(There's a custom `cache_all` function on the root module as a convenience, which will add hooks to cache every activation at a hook point - we could also manually add hooks with `run_with_hooks`)"[m
[32m+[m[32m    "(There's a custom `run_with_cache` function on the root module as a convenience, which is a wrapper around model.forward that return model_out, cache_object - we could also manually add hooks with `run_with_hooks` that store activations in a global caching dictionary. This is often useful if we only want to store, eg, subsets or functions of some activations.)"[m
    ][m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 8,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
[36m@@ -239,12 +258,11 @@[m
     }[m
    ],[m
    "source": [[m
[31m-    "cache = {}\n",[m
[31m-    "model.cache_all(cache)\n",[m
[31m-    "print('Model output:', model(torch.tensor(5.)).item())\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "out, cache = model.run_with_cache(torch.tensor(5.))\n",[m
[32m+[m[32m    "print('Model output:', out.item())\n",[m
     "for key in cache:\n",[m
[31m-    "    print(f\"Value cached at hook {key}\", cache[key].item())\n",[m
[31m-    "model.reset_hooks()"[m
[32m+[m[32m    "    print(f\"Value cached at hook {key}\", cache[key].item())"[m
    ][m
   },[m
   {[m
[36m@@ -256,7 +274,7 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 9,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
[36m@@ -309,31 +327,22 @@[m
     "                     'facebook/opt-66b', \n",[m
     "                     'EleutherAI/gpt-neo-125M', \n",[m
     "                     'EleutherAI/gpt-neo-1.3B', \n",[m
[31m-    "                     'EleutherAI/gpt-neo-2.7B', \n",[m
[31m-    "                     'EleutherAI/gpt-j-6B', \n",[m
[31m-    "                     'EleutherAI/gpt-neox-20b']\n",[m
[32m+[m[32m    "                     'EleutherAI/gpt-neo-2.7B',]\n",[m
     "                     ```"[m
    ][m
   },[m
[31m-  {[m
[31m-   "cell_type": "code",[m
[31m-   "execution_count": null,[m
[31m-   "metadata": {},[m
[31m-   "outputs": [],[m
[31m-   "source": [][m
[31m-  },[m
   {[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "#Examples"[m
[32m+[m[32m    "# Examples"[m
    ][m
   },[m
   {[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "##Setup"[m
[32m+[m[32m    "## Setup"[m
    ][m
   },[m
   {[m
[36m@@ -345,11 +354,11 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 10,[m
    "metadata": {},[m
    "outputs": [],[m
    "source": [[m
[31m-    "model_name = 'gpt2' #@param ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'facebook/opt-125m', 'facebook/opt-1.3b', 'facebook/opt-2.7b', 'facebook/opt-6.7b', 'facebook/opt-13b', 'facebook/opt-30b', 'facebook/opt-66b', 'EleutherAI/gpt-neo-125M', 'EleutherAI/gpt-neo-1.3B', 'EleutherAI/gpt-neo-2.7B', 'EleutherAI/gpt-j-6B', 'EleutherAI/gpt-neox-20b']\n",[m
[32m+[m[32m    "model_name = 'gpt2' #@param ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'facebook/opt-125m', 'facebook/opt-1.3b', 'facebook/opt-2.7b', 'facebook/opt-6.7b', 'facebook/opt-13b', 'facebook/opt-30b', 'facebook/opt-66b', 'EleutherAI/gpt-neo-125M', 'EleutherAI/gpt-neo-1.3B', 'EleutherAI/gpt-neo-2.7B']\n",[m
     "model = EasyTransformer.from_pretrained(model_name).to(device)"[m
    ][m
   },[m
[36m@@ -357,67 +366,38 @@[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "Create some reference text to run the models on."[m
[32m+[m[32m    "Create some reference text to run the models on. Models come with a `to_tokens` and `to_str_tokens` method, which can convert text to tokens and to a list of individual tokens as strings. Though GPT-2 was not trained with a beginning of string token, we prepend one by default, as the first token is often used as a \"resting position\" by inactive attention heads, and as a result has weird behaviour."[m
    ][m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 11,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "name": "stdout",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "Inter|pret|ability| is| great\n",[m
[31m-      "AI| Al|ignment| is| great\n"[m
[32m+[m[32m      "['<|endoftext|>', 'Inter', 'pret', 'ability', ' is', ' great']\n",[m
[32m+[m[32m      "['<|endoftext|>', 'AI', ' Al', 'ignment', ' is', ' great']\n"[m
      ][m
     }[m
    ],[m
    "source": [[m
     "prompt = 'Interpretability is great'\n",[m
[31m-    "tokens = model.to_tokens(prompt)\n",[m
[32m+[m[32m    "# The model has a method to_str_tokens\n",[m
[32m+[m[32m    "print(model.to_str_tokens(prompt, prepend_bos=True))\n",[m
[32m+[m[32m    "\n",[m
     "prompt_2 = 'AI Alignment is great'\n",[m
[31m-    "tokens_2 = model.to_tokens(prompt_2)\n",[m
[31m-    "def show_tokens(tokens):\n",[m
[31m-    "    # Prints the tokens as text, separated by |\n",[m
[31m-    "    if type(tokens)==str:\n",[m
[31m-    "        # If we input text, tokenize first\n",[m
[31m-    "        tokens = model.to_tokens(tokens)\n",[m
[31m-    "    text_tokens = [model.tokenizer.decode(t) for t in tokens.squeeze()]\n",[m
[31m-    "    print('|'.join(text_tokens))\n",[m
[31m-    "show_tokens(tokens)\n",[m
[31m-    "show_tokens(tokens_2)"[m
[31m-   ][m
[31m-  },[m
[31m-  {[m
[31m-   "cell_type": "code",[m
[31m-   "execution_count": null,[m
[31m-   "metadata": {},[m
[31m-   "outputs": [[m
[31m-    {[m
[31m-     "name": "stdout",[m
[31m-     "output_type": "stream",[m
[31m-     "text": [[m
[31m-      "Top corner of logits\n",[m
[31m-      "tensor([[[-5.0197, -4.0007, -6.4540, -6.6006],\n",[m
[31m-      "         [-4.1477, -2.2966, -7.4325, -6.9754],\n",[m
[31m-      "         [-2.7587,  1.3903, -4.3042, -6.5975],\n",[m
[31m-      "         [-6.1653, -5.0678, -9.1324, -9.5672]]], device='cuda:0',\n",[m
[31m-      "       grad_fn=<SliceBackward0>)\n"[m
[31m-     ][m
[31m-    }[m
[31m-   ],[m
[31m-   "source": [[m
[31m-    "model.reset_hooks()\n",[m
[31m-    "original_logits = model(tokens)\n",[m
[31m-    "print('Top corner of logits')\n",[m
[31m-    "print(get_corner(original_logits, 4))"[m
[32m+[m[32m    "# We can go via the to_tokens method\n",[m
[32m+[m[32m    "tokens_2 = model.to_tokens(prompt_2, prepend_bos=True)\n",[m
[32m+[m[32m    "# to_str_tokens also takes a tensor of tokens as input, though only for a *single* example\n",[m
[32m+[m[32m    "print(model.to_str_tokens(tokens_2))"[m
    ][m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 12,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
[36m@@ -430,30 +410,37 @@[m
     {[m
      "data": {[m
       "text/plain": [[m
[31m-       "{'d_model': 768,\n",[m
[32m+[m[32m       "{'n_layers': 12,\n",[m
[32m+[m[32m       " 'd_model': 768,\n",[m
[32m+[m[32m       " 'n_ctx': 1024,\n",[m
        " 'd_head': 64,\n",[m
        " 'n_heads': 12,\n",[m
[32m+[m[32m       " 'model_name': 'gpt2',\n",[m
        " 'd_mlp': 3072,\n",[m
[31m-       " 'n_layers': 12,\n",[m
[31m-       " 'n_ctx': 1024,\n",[m
[31m-       " 'd_vocab': 50257,\n",[m
        " 'act_fn': 'gelu_new',\n",[m
[32m+[m[32m       " 'd_vocab': 50257,\n",[m
        " 'eps': 1e-05,\n",[m
        " 'use_attn_result': False,\n",[m
        " 'use_attn_scale': True,\n",[m
        " 'use_local_attn': False,\n",[m
[31m-       " 'model_name': 'gpt2',\n",[m
[31m-       " 'model_type': 'gpt2',\n",[m
[32m+[m[32m       " 'model_family': 'gpt2',\n",[m
        " 'checkpoint': None,\n",[m
[31m-       " 'full_model_name': None,\n",[m
        " 'tokenizer_name': 'gpt2',\n",[m
        " 'window_size': None,\n",[m
        " 'attn_types': None,\n",[m
        " 'init_mode': 'gpt2',\n",[m
[31m-       " 'normalization_type': 'LNPre'}"[m
[32m+[m[32m       " 'normalization_type': 'LNPre',\n",[m
[32m+[m[32m       " 'device': 'cuda',\n",[m
[32m+[m[32m       " 'attention_dir': 'causal',\n",[m
[32m+[m[32m       " 'attn_only': False,\n",[m
[32m+[m[32m       " 'seed': 42,\n",[m
[32m+[m[32m       " 'initializer_range': 0.02,\n",[m
[32m+[m[32m       " 'init_weights': False,\n",[m
[32m+[m[32m       " 'scale_attn_by_inverse_layer_idx': False,\n",[m
[32m+[m[32m       " 'positional_embedding_type': 'standard'}"[m
       ][m
      },[m
[31m-     "execution_count": null,[m
[32m+[m[32m     "execution_count": 12,[m
      "metadata": {},[m
      "output_type": "execute_result"[m
     }[m
[36m@@ -467,59 +454,73 @@[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "##Using the model"[m
[32m+[m[32m    "## Using the model"[m
    ][m
   },[m
   {[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "The model can be given either text or tokens as an input (text is automatically converted to a `batch_size=1` batch of tokens)"[m
[32m+[m[32m    "The model can be given either text or tokens as an input (text is automatically converted to a `batch_size=1` batch of tokens). \n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "This time, we'll disable the automatic prepending of a BoS token. Here it doesn't really matter either way."[m
    ][m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 14,[m
    "metadata": {},[m
[31m-   "outputs": [],[m
[32m+[m[32m   "outputs": [[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stdout",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "['Hello', ' World', '!']\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    }[m
[32m+[m[32m   ],[m
    "source": [[m
[32m+[m[32m    "prompt = 'Hello World!'\n",[m
[32m+[m[32m    "print(model.to_str_tokens(prompt, prepend_bos=False))\n",[m
[32m+[m[32m    "tokens = model.to_tokens(prompt, prepend_bos=False)\n",[m
     "logits_tokens = model(tokens)\n",[m
[31m-    "logits_text = model(prompt)"[m
[32m+[m[32m    "logits_text = model(prompt, prepend_bos=False)"[m
    ][m
   },[m
   {[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "The model gives the same log_probs as the original Hugging Face model \n",[m
[32m+[m[32m    "EasyTransformer applies multiple optimizations internally to make the model computationally equivalent to the original model, but to be more interpretable, eg [folding in LayerNorm weights](https://transformer-circuits.pub/2021/framework/index.html#:~:text=Handling%20Layer%20Normalization) and making the unembedding be mean zero (as logits are translation invariant) \n",[m
     "\n",[m
[31m-    "Though *not* the same logits, as we remove a constant offset from $W_U$"[m
[32m+[m[32m    "Importantly, as we remove a constant offset from $W_U$, the log probs (and thus the loss) are unchanged, but the logits are translated."[m
    ][m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 15,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "name": "stdout",[m
      "output_type": "stream",[m
      "text": [[m
[32m+[m[32m      "Logit shape: torch.Size([1, 3, 50257])\n",[m
       "Fraction of log probs the same between easy model and original model:\n",[m
[31m-      "tensor(1.0000)\n",[m
[32m+[m[32m      "tensor(1., device='cuda:0')\n",[m
       "Fraction of logits the same between easy model and original model:\n",[m
[31m-      "tensor(0.)\n"[m
[32m+[m[32m      "tensor(0., device='cuda:0')\n"[m
      ][m
     }[m
    ],[m
    "source": [[m
[31m-    "original_model = AutoModelForCausalLM.from_pretrained(model_name)\n",[m
[31m-    "easy_logits = model(tokens).cpu()\n",[m
[32m+[m[32m    "original_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",[m
[32m+[m[32m    "easy_logits = model(tokens)\n",[m
     "original_model_logits = original_model(tokens).logits\n",[m
     "\n",[m
     "easy_log_probs = F.log_softmax(easy_logits, dim=-1)\n",[m
     "original_model_log_probs = F.log_softmax(original_model_logits, dim=-1)\n",[m
[31m-    "\n",[m
[32m+[m[32m    "print(\"Logit shape:\", easy_logits.shape)\n",[m
     "print('Fraction of log probs the same between easy model and original model:')\n",[m
     "print(torch.isclose(original_model_log_probs, easy_log_probs).sum()/easy_log_probs.numel())\n",[m
     "print('Fraction of logits the same between easy model and original model:')\n",[m
[36m@@ -530,14 +531,14 @@[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "##Basic Examples"[m
[32m+[m[32m    "## Basic Examples"[m
    ][m
   },[m
   {[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "Print the shapes of all activations\n",[m
[32m+[m[32m    "Print the shapes of all activations in the embedding or first layer (other layers are identical to the first layer, just change the index in `blocks.{layer}.`)\n",[m
     "\n",[m
     "**Note:** This cell is a good reference for creating hooks - it's extremely useful to know the shapes of different activations as accessible by each hook!\n",[m
     "\n",[m
[36m@@ -555,7 +556,7 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 16,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
[36m@@ -570,14 +571,14 @@[m
       "torch.Size([4, 50, 768])\n",[m
       "Activation at hook blocks.0.ln1.hook_scale has shape:\n",[m
       "torch.Size([4, 50, 1])\n",[m
[32m+[m[32m      "Activation at hook blocks.0.ln1.hook_normalized has shape:\n",[m
[32m+[m[32m      "torch.Size([4, 50, 768])\n",[m
       "Activation at hook blocks.0.attn.hook_q has shape:\n",[m
       "torch.Size([4, 50, 12, 64])\n",[m
       "Activation at hook blocks.0.attn.hook_k has shape:\n",[m
       "torch.Size([4, 50, 12, 64])\n",[m
       "Activation at hook blocks.0.attn.hook_v has shape:\n",[m
       "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.0.attn.hook_attn_scores has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
       "Activation at hook blocks.0.attn.hook_attn has shape:\n",[m
       "torch.Size([4, 12, 50, 50])\n",[m
       "Activation at hook blocks.0.attn.hook_z has shape:\n",[m
[36m@@ -588,6 +589,8 @@[m
       "torch.Size([4, 50, 768])\n",[m
       "Activation at hook blocks.0.ln2.hook_scale has shape:\n",[m
       "torch.Size([4, 50, 1])\n",[m
[32m+[m[32m      "Activation at hook blocks.0.ln2.hook_normalized has shape:\n",[m
[32m+[m[32m      "torch.Size([4, 50, 768])\n",[m
       "Activation at hook blocks.0.mlp.hook_pre has shape:\n",[m
       "torch.Size([4, 50, 3072])\n",[m
       "Activation at hook blocks.0.mlp.hook_post has shape:\n",[m
[36m@@ -596,348 +599,20 @@[m
       "torch.Size([4, 50, 768])\n",[m
       "Activation at hook blocks.0.hook_resid_post has shape:\n",[m
       "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.1.hook_resid_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.1.ln1.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.1.attn.hook_q has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.1.attn.hook_k has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.1.attn.hook_v has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.1.attn.hook_attn_scores has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.1.attn.hook_attn has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.1.attn.hook_z has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.1.hook_attn_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.1.hook_resid_mid has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.1.ln2.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.1.mlp.hook_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.1.mlp.hook_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.1.hook_mlp_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.1.hook_resid_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.2.hook_resid_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.2.ln1.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.2.attn.hook_q has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.2.attn.hook_k has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.2.attn.hook_v has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.2.attn.hook_attn_scores has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.2.attn.hook_attn has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.2.attn.hook_z has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.2.hook_attn_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.2.hook_resid_mid has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.2.ln2.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.2.mlp.hook_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.2.mlp.hook_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.2.hook_mlp_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.2.hook_resid_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.3.hook_resid_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.3.ln1.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.3.attn.hook_q has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.3.attn.hook_k has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.3.attn.hook_v has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.3.attn.hook_attn_scores has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.3.attn.hook_attn has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.3.attn.hook_z has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.3.hook_attn_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.3.hook_resid_mid has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.3.ln2.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.3.mlp.hook_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.3.mlp.hook_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.3.hook_mlp_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.3.hook_resid_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.4.hook_resid_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.4.ln1.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.4.attn.hook_q has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.4.attn.hook_k has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.4.attn.hook_v has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.4.attn.hook_attn_scores has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.4.attn.hook_attn has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.4.attn.hook_z has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.4.hook_attn_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.4.hook_resid_mid has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.4.ln2.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.4.mlp.hook_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.4.mlp.hook_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.4.hook_mlp_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.4.hook_resid_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.5.hook_resid_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.5.ln1.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.5.attn.hook_q has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.5.attn.hook_k has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.5.attn.hook_v has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.5.attn.hook_attn_scores has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.5.attn.hook_attn has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.5.attn.hook_z has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.5.hook_attn_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.5.hook_resid_mid has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.5.ln2.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.5.mlp.hook_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.5.mlp.hook_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.5.hook_mlp_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.5.hook_resid_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.6.hook_resid_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.6.ln1.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.6.attn.hook_q has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.6.attn.hook_k has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.6.attn.hook_v has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.6.attn.hook_attn_scores has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.6.attn.hook_attn has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.6.attn.hook_z has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.6.hook_attn_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.6.hook_resid_mid has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.6.ln2.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.6.mlp.hook_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.6.mlp.hook_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.6.hook_mlp_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.6.hook_resid_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.7.hook_resid_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.7.ln1.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.7.attn.hook_q has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.7.attn.hook_k has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.7.attn.hook_v has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.7.attn.hook_attn_scores has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.7.attn.hook_attn has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.7.attn.hook_z has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.7.hook_attn_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.7.hook_resid_mid has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.7.ln2.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.7.mlp.hook_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.7.mlp.hook_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.7.hook_mlp_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.7.hook_resid_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.8.hook_resid_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.8.ln1.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.8.attn.hook_q has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.8.attn.hook_k has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.8.attn.hook_v has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.8.attn.hook_attn_scores has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.8.attn.hook_attn has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.8.attn.hook_z has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.8.hook_attn_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.8.hook_resid_mid has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.8.ln2.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.8.mlp.hook_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.8.mlp.hook_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.8.hook_mlp_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.8.hook_resid_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.9.hook_resid_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.9.ln1.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.9.attn.hook_q has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.9.attn.hook_k has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.9.attn.hook_v has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.9.attn.hook_attn_scores has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.9.attn.hook_attn has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.9.attn.hook_z has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.9.hook_attn_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.9.hook_resid_mid has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.9.ln2.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.9.mlp.hook_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.9.mlp.hook_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.9.hook_mlp_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.9.hook_resid_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.10.hook_resid_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.10.ln1.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.10.attn.hook_q has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.10.attn.hook_k has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.10.attn.hook_v has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.10.attn.hook_attn_scores has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.10.attn.hook_attn has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.10.attn.hook_z has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.10.hook_attn_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.10.hook_resid_mid has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.10.ln2.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.10.mlp.hook_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.10.mlp.hook_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.10.hook_mlp_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.10.hook_resid_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.11.hook_resid_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.11.ln1.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.11.attn.hook_q has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.11.attn.hook_k has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.11.attn.hook_v has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.11.attn.hook_attn_scores has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.11.attn.hook_attn has shape:\n",[m
[31m-      "torch.Size([4, 12, 50, 50])\n",[m
[31m-      "Activation at hook blocks.11.attn.hook_z has shape:\n",[m
[31m-      "torch.Size([4, 50, 12, 64])\n",[m
[31m-      "Activation at hook blocks.11.hook_attn_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.11.hook_resid_mid has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.11.ln2.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n",[m
[31m-      "Activation at hook blocks.11.mlp.hook_pre has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.11.mlp.hook_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 3072])\n",[m
[31m-      "Activation at hook blocks.11.hook_mlp_out has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
[31m-      "Activation at hook blocks.11.hook_resid_post has shape:\n",[m
[31m-      "torch.Size([4, 50, 768])\n",[m
       "Activation at hook ln_final.hook_scale has shape:\n",[m
[31m-      "torch.Size([4, 50, 1])\n"[m
[32m+[m[32m      "torch.Size([4, 50, 1])\n",[m
[32m+[m[32m      "Activation at hook ln_final.hook_normalized has shape:\n",[m
[32m+[m[32m      "torch.Size([4, 50, 768])\n"[m
      ][m
     }[m
    ],[m
    "source": [[m
[31m-    "all_hooks_fn = lambda name: True\n",[m
[32m+[m[32m    "embed_or_first_layer = lambda name: (name[:6]!='blocks' or name[:8]=='blocks.0')\n",[m
     "def print_shape(tensor, hook):\n",[m
     "    print(f'Activation at hook {hook.name} has shape:')\n",[m
     "    print(tensor.shape)\n",[m
     "random_tokens = torch.randint(1000, 10000, (4, 50))\n",[m
[31m-    "logits = model.run_with_hooks(random_tokens, fwd_hooks=[(all_hooks_fn, print_shape)])"[m
[32m+[m[32m    "logits = model.run_with_hooks(random_tokens, fwd_hooks=[(embed_or_first_layer, print_shape)])"[m
    ][m
   },[m
   {[m
[36m@@ -951,7 +626,7 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 17,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
[36m@@ -959,785 +634,142 @@[m
      "output_type": "stream",[m
      "text": [[m
       "hook_embed\n",[m
[31m-      "tensor([[[ 0.1600, -0.1444],\n",[m
[31m-      "         [-0.0406, -0.2098]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[32m+[m[32m      "tensor([[[-0.0692, -0.1332,  0.0107],\n",[m
[32m+[m[32m      "         [-0.0903,  0.0212,  0.3009],\n",[m
[32m+[m[32m      "         [-0.1106, -0.0398,  0.0326]]], device='cuda:0',\n",[m
[32m+[m[32m      "       grad_fn=<SliceBackward0>)\n",[m
       "hook_pos_embed\n",[m
[31m-      "tensor([[-0.0134, -0.1920],\n",[m
[31m-      "        [ 0.0250, -0.0528]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[32m+[m[32m      "tensor([[-0.0134, -0.1920,  0.0095],\n",[m
[32m+[m[32m      "        [ 0.0250, -0.0528, -0.0939],\n",[m
[32m+[m[32m      "        [ 0.0065, -0.0825,  0.0568]], device='cuda:0',\n",[m
[32m+[m[32m      "       grad_fn=<SliceBackward0>)\n",[m
       "blocks.0.hook_resid_pre\n",[m
[31m-      "tensor([[[ 0.1466, -0.3363],\n",[m
[31m-      "         [-0.0156, -0.2626]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[32m+[m[32m      "tensor([[[-0.0826, -0.3252,  0.0201],\n",[m
[32m+[m[32m      "         [-0.0653, -0.0316,  0.2071],\n",[m
[32m+[m[32m      "         [-0.1041, -0.1223,  0.0894]]], device='cuda:0',\n",[m
[32m+[m[32m      "       grad_fn=<SliceBackward0>)\n",[m
       "blocks.0.ln1.hook_scale\n",[m
[31m-      "tensor([[[0.3703],\n",[m
[31m-      "         [0.2421]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[32m+[m[32m      "tensor([[[0.3795],\n",[m
[32m+[m[32m      "         [0.2153],\n",[m
[32m+[m[32m      "         [0.1962]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[32m+[m[32m      "blocks.0.ln1.hook_normalized\n",[m
[32m+[m[32m      "tensor([[[-0.2176, -0.8570,  0.0531],\n",[m
[32m+[m[32m      "         [-0.3033, -0.1468,  0.9616],\n",[m
[32m+[m[32m      "         [-0.5308, -0.6233,  0.4556]]], device='cuda:0',\n",[m
[32m+[m[32m      "       grad_fn=<SliceBackward0>)\n",[m
       "blocks.0.attn.hook_q\n",[m
[31m-      "tensor([[[[-0.6830,  0.1875],\n",[m
[31m-      "          [ 0.5510,  0.1701]],\n",[m
[32m+[m[32m      "tensor([[[[ 0.4701, -0.0775,  0.5531],\n",[m
[32m+[m[32m      "          [-1.6738,  0.0741, -2.0410],\n",[m
[32m+[m[32m      "          [ 0.4965,  0.9504,  0.2968]],\n",[m
       "\n",[m
[31m-      "         [[ 0.4226,  0.8636],\n",[m
[31m-      "          [ 0.1361, -0.6476]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[32m+[m[32m      "         [[-0.4076,  1.5079,  0.0580],\n",[m
[32m+[m[32m      "          [ 0.8189,  1.3556, -2.0204],\n",[m
[32m+[m[32m      "          [-0.2553,  0.6799, -0.4413]],\n",[m
[32m+[m[32m      "\n",[m
[32m+[m[32m      "         [[ 0.2009, -0.0724,  0.6384],\n",[m
[32m+[m[32m      "          [ 0.7554,  0.9048, -1.5009],\n",[m
[32m+[m[32m      "          [-0.0154,  1.4663,  0.6373]]]], device='cuda:0',\n",[m
[32m+[m[32m      "       grad_fn=<SliceBackward0>)\n",[m
       "blocks.0.attn.hook_k\n",[m
[31m-      "tensor([[[[-1.1447,  2.1864],\n",[m
[31m-      "          [ 1.4646,  0.4051]],\n",[m
[32m+[m[32m      "tensor([[[[-1.2526,  2.3200,  0.1722],\n",[m
[32m+[m[32m      "          [-0.3975, -0.4736, -0.8473],\n",[m
[32m+[m[32m      "          [ 0.3670, -1.0056,  1.3048]],\n",[m
       "\n",[m
[31m-      "         [[-1.5435,  2.9672],\n",[m
[31m-      "          [ 0.9403, -1.4134]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[32m+[m[32m      "         [[-2.6917,  2.8358,  1.5316],\n",[m
[32m+[m[32m      "          [ 0.6833,  0.6253, -1.6378],\n",[m
[32m+[m[32m      "          [ 1.0142,  1.1044,  1.1745]],\n",[m
[32m+[m[32m      "\n",[m
[32m+[m[32m      "         [[-2.0131,  2.7125,  1.8671],\n",[m
[32m+[m[32m      "          [-0.0490, -0.8074, -1.7336],\n",[m
[32m+[m[32m      "          [ 1.0125, -0.0738,  1.0648]]]], device='cuda:0',\n",[m
[32m+[m[32m      "       grad_fn=<SliceBackward0>)\n",[m
       "blocks.0.attn.hook_v\n",[m
[31m-      "tensor([[[[-0.0110,  0.0460],\n",[m
[31m-      "          [ 0.4635,  0.0313]],\n",[m
[32m+[m[32m      "tensor([[[[ 0.1637,  0.2042, -0.0436],\n",[m
[32m+[m[32m      "          [ 0.3102,  0.1178, -0.0882],\n",[m
[32m+[m[32m      "          [ 0.2191,  0.0087,  0.0735]],\n",[m
       "\n",[m
[31m-      "         [[ 0.1250, -0.3105],\n",[m
[31m-      "          [ 0.3096,  0.2382]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.0.attn.hook_attn_scores\n",[m
[31m-      "tensor([[[[-2.3346e-01, -1.0000e+05],\n",[m
[31m-      "          [ 6.0883e-01, -6.4138e-01]],\n",[m
[32m+[m[32m      "         [[-0.1562,  0.1953, -0.0155],\n",[m
[32m+[m[32m      "          [ 0.4145, -0.0456, -0.1670],\n",[m
[32m+[m[32m      "          [ 0.1130, -0.2758, -0.0888]],\n",[m
       "\n",[m
[31m-      "         [[ 4.0300e+00, -1.0000e+05],\n",[m
[31m-      "          [ 3.6600e+00,  1.2221e+01]]]], device='cuda:0',\n",[m
[32m+[m[32m      "         [[-0.1250, -0.0359, -0.1472],\n",[m
[32m+[m[32m      "          [ 0.3074, -0.1480, -0.0081],\n",[m
[32m+[m[32m      "          [-0.0516, -0.1785,  0.2528]]]], device='cuda:0',\n",[m
       "       grad_fn=<SliceBackward0>)\n",[m
       "blocks.0.attn.hook_attn\n",[m
[31m-      "tensor([[[[1.0000e+00, 0.0000e+00],\n",[m
[31m-      "          [7.7733e-01, 2.2267e-01]],\n",[m
[32m+[m[32m      "tensor([[[[1.0000, 0.0000, 0.0000],\n",[m
[32m+[m[32m      "          [0.9466, 0.0534, 0.0000],\n",[m
[32m+[m[32m      "          [0.7572, 0.1515, 0.0913]],\n",[m
       "\n",[m
[31m-      "         [[1.0000e+00, 0.0000e+00],\n",[m
[31m-      "          [1.9132e-04, 9.9981e-01]]]], device='cuda:0',\n",[m
[31m-      "       grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.0.attn.hook_z\n",[m
[31m-      "tensor([[[[-0.0110,  0.0460],\n",[m
[31m-      "          [ 0.4635,  0.0313]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.0193, -0.0334],\n",[m
[31m-      "          [ 0.3097,  0.2382]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.0.hook_attn_out\n",[m
[31m-      "tensor([[[ 2.8893, -0.2035],\n",[m
[31m-      "         [ 1.4210, -0.6418]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.0.hook_resid_mid\n",[m
[31m-      "tensor([[[ 3.0359, -0.5398],\n",[m
[31m-      "         [ 1.4054, -0.9044]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.0.ln2.hook_scale\n",[m
[31m-      "tensor([[[1.0244],\n",[m
[31m-      "         [1.2495]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.0.mlp.hook_pre\n",[m
[31m-      "tensor([[[-0.0804, -1.9470],\n",[m
[31m-      "         [ 0.2039, -1.0426]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.0.mlp.hook_post\n",[m
[31m-      "tensor([[[-0.0376, -0.0501],\n",[m
[31m-      "         [ 0.1184, -0.1551]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.0.hook_mlp_out\n",[m
[31m-      "tensor([[[ 1.4300, -1.3753],\n",[m
[31m-      "         [-1.1018, -1.7305]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.0.hook_resid_post\n",[m
[31m-      "tensor([[[ 4.4659, -1.9152],\n",[m
[31m-      "         [ 0.3036, -2.6350]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.hook_resid_pre\n",[m
[31m-      "tensor([[[ 4.4659, -1.9152],\n",[m
[31m-      "         [ 0.3036, -2.6350]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.ln1.hook_scale\n",[m
[31m-      "tensor([[[5.2533],\n",[m
[31m-      "         [2.0368]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.attn.hook_q\n",[m
[31m-      "tensor([[[[ 0.1313, -0.2223],\n",[m
[31m-      "          [-0.2588, -0.5800]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-2.5284, -0.2904],\n",[m
[31m-      "          [ 0.1441,  0.2812]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.attn.hook_k\n",[m
[31m-      "tensor([[[[-0.9837,  1.5757],\n",[m
[31m-      "          [-1.5389, -0.6636]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.5076,  1.2804],\n",[m
[31m-      "          [-0.3490,  0.1548]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.attn.hook_v\n",[m
[31m-      "tensor([[[[ 0.1692, -0.0567],\n",[m
[31m-      "          [-0.1223, -0.0142]],\n",[m
[32m+[m[32m      "         [[1.0000, 0.0000, 0.0000],\n",[m
[32m+[m[32m      "          [0.0015, 0.9985, 0.0000],\n",[m
[32m+[m[32m      "          [0.0066, 0.0071, 0.9863]],\n",[m
       "\n",[m
[31m-      "         [[-0.2674, -0.1515],\n",[m
[31m-      "          [-0.1510, -0.4548]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.attn.hook_attn_scores\n",[m
[31m-      "tensor([[[[ 7.9228e-01, -1.0000e+05],\n",[m
[31m-      "          [ 2.2300e+00, -4.0063e+00]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 7.1117e-01, -1.0000e+05],\n",[m
[31m-      "          [ 5.5577e-01, -2.6595e+00]]]], device='cuda:0',\n",[m
[32m+[m[32m      "         [[1.0000, 0.0000, 0.0000],\n",[m
[32m+[m[32m      "          [0.7804, 0.2196, 0.0000],\n",[m
[32m+[m[32m      "          [0.5218, 0.1978, 0.2804]]]], device='cuda:0',\n",[m
       "       grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.attn.hook_attn\n",[m
[31m-      "tensor([[[[1.0000, 0.0000],\n",[m
[31m-      "          [0.9980, 0.0020]],\n",[m
[31m-      "\n",[m
[31m-      "         [[1.0000, 0.0000],\n",[m
[31m-      "          [0.9614, 0.0386]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.attn.hook_z\n",[m
[31m-      "tensor([[[[ 0.1692, -0.0567],\n",[m
[31m-      "          [-0.1223, -0.0142]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.1683, -0.0568],\n",[m
[31m-      "          [-0.1234, -0.0312]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.hook_attn_out\n",[m
[31m-      "tensor([[[ 0.2312, -0.2081],\n",[m
[31m-      "         [ 0.8123,  0.4268]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.hook_resid_mid\n",[m
[31m-      "tensor([[[ 4.6971, -2.1232],\n",[m
[31m-      "         [ 1.1160, -2.2082]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.ln2.hook_scale\n",[m
[31m-      "tensor([[[6.2248],\n",[m
[31m-      "         [2.2192]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.mlp.hook_pre\n",[m
[31m-      "tensor([[[ 0.2623, -1.0880],\n",[m
[31m-      "         [-0.3285, -3.8161]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.mlp.hook_post\n",[m
[31m-      "tensor([[[ 0.1583, -0.1507],\n",[m
[31m-      "         [-0.1220, -0.0002]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.hook_mlp_out\n",[m
[31m-      "tensor([[[-1.2580, -0.0978],\n",[m
[31m-      "         [-0.4065,  0.5446]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.1.hook_resid_post\n",[m
[31m-      "tensor([[[ 3.4391, -2.2210],\n",[m
[31m-      "         [ 0.7095, -1.6636]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.hook_resid_pre\n",[m
[31m-      "tensor([[[ 3.4391, -2.2210],\n",[m
[31m-      "         [ 0.7095, -1.6636]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.ln1.hook_scale\n",[m
[31m-      "tensor([[[21.6901],\n",[m
[31m-      "         [ 2.1257]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.attn.hook_q\n",[m
[31m-      "tensor([[[[ 0.1714, -0.3255],\n",[m
[31m-      "          [-0.0372, -0.2601]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.0202,  1.7867],\n",[m
[31m-      "          [-0.2622, -0.5518]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.attn.hook_k\n",[m
[31m-      "tensor([[[[-0.1275, -1.1551],\n",[m
[31m-      "          [-0.4914,  0.5046]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.0608, -3.2119],\n",[m
[31m-      "          [-2.3510, -0.1151]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.attn.hook_v\n",[m
[31m-      "tensor([[[[-0.0364,  0.0073],\n",[m
[31m-      "          [ 0.0049, -0.0400]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.2338, -0.0141],\n",[m
[31m-      "          [-0.1485, -0.0420]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.attn.hook_attn_scores\n",[m
[31m-      "tensor([[[[ 1.4845e-01, -1.0000e+05],\n",[m
[31m-      "          [-4.0606e-01, -4.5755e+00]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-6.5884e-01, -1.0000e+05],\n",[m
[31m-      "          [ 2.4257e+00, -1.6846e+00]]]], device='cuda:0',\n",[m
[31m-      "       grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.attn.hook_attn\n",[m
[31m-      "tensor([[[[1.0000, 0.0000],\n",[m
[31m-      "          [0.9848, 0.0152]],\n",[m
[31m-      "\n",[m
[31m-      "         [[1.0000, 0.0000],\n",[m
[31m-      "          [0.9839, 0.0161]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.attn.hook_z\n",[m
[31m-      "tensor([[[[-0.0364,  0.0073],\n",[m
[31m-      "          [ 0.0049, -0.0400]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.0394,  0.0070],\n",[m
[31m-      "          [ 0.0024, -0.0400]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.hook_attn_out\n",[m
[31m-      "tensor([[[ 0.0806, -0.1475],\n",[m
[31m-      "         [ 0.0894, -0.1102]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.hook_resid_mid\n",[m
[31m-      "tensor([[[ 3.5198, -2.3686],\n",[m
[31m-      "         [ 0.7989, -1.7739]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.ln2.hook_scale\n",[m
[31m-      "tensor([[[21.7232],\n",[m
[31m-      "         [ 2.1729]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.mlp.hook_pre\n",[m
[31m-      "tensor([[[ 0.6213, -1.7141],\n",[m
[31m-      "         [-0.4335, -2.3515]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.mlp.hook_post\n",[m
[31m-      "tensor([[[ 0.4553, -0.0743],\n",[m
[31m-      "         [-0.1441, -0.0216]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.hook_mlp_out\n",[m
[31m-      "tensor([[[-3.5084, -3.0720],\n",[m
[31m-      "         [-0.8282, -0.4658]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.2.hook_resid_post\n",[m
[31m-      "tensor([[[ 0.0113, -5.4405],\n",[m
[31m-      "         [-0.0292, -2.2397]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.hook_resid_pre\n",[m
[31m-      "tensor([[[ 0.0113, -5.4405],\n",[m
[31m-      "         [-0.0292, -2.2397]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.ln1.hook_scale\n",[m
[31m-      "tensor([[[92.2422],\n",[m
[31m-      "         [ 2.2299]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.attn.hook_q\n",[m
[31m-      "tensor([[[[-0.3975, -0.0909],\n",[m
[31m-      "          [-0.1682,  0.0364]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.0580, -1.6875],\n",[m
[31m-      "          [-0.3656,  0.9330]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.attn.hook_k\n",[m
[31m-      "tensor([[[[ 0.0097, -0.1900],\n",[m
[31m-      "          [ 0.8230,  0.2071]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.2231, -1.0661],\n",[m
[31m-      "          [ 1.4244, -2.4477]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.attn.hook_v\n",[m
[31m-      "tensor([[[[ 0.0393,  0.0578],\n",[m
[31m-      "          [-0.0519,  0.0038]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.0018, -1.0726],\n",[m
[31m-      "          [-0.2352, -0.5483]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.attn.hook_attn_scores\n",[m
[31m-      "tensor([[[[ 1.3210e+00, -1.0000e+05],\n",[m
[31m-      "          [ 7.4256e+00,  1.7582e+00]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 3.3527e-01, -1.0000e+05],\n",[m
[31m-      "          [-2.0173e+00, -5.8889e+00]]]], device='cuda:0',\n",[m
[31m-      "       grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.attn.hook_attn\n",[m
[31m-      "tensor([[[[1.0000, 0.0000],\n",[m
[31m-      "          [0.9966, 0.0034]],\n",[m
[31m-      "\n",[m
[31m-      "         [[1.0000, 0.0000],\n",[m
[31m-      "          [0.9796, 0.0204]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.attn.hook_z\n",[m
[31m-      "tensor([[[[ 0.0393,  0.0578],\n",[m
[31m-      "          [-0.0519,  0.0038]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.0391,  0.0539],\n",[m
[31m-      "          [-0.0557, -0.0074]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.hook_attn_out\n",[m
[31m-      "tensor([[[ 0.0786,  0.0225],\n",[m
[31m-      "         [ 0.0839, -0.0046]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.hook_resid_mid\n",[m
[31m-      "tensor([[[ 0.0900, -5.4180],\n",[m
[31m-      "         [ 0.0547, -2.2443]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.ln2.hook_scale\n",[m
[31m-      "tensor([[[92.1455],\n",[m
[31m-      "         [ 2.2206]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.mlp.hook_pre\n",[m
[31m-      "tensor([[[-2.4675,  0.0471],\n",[m
[31m-      "         [-0.9537, -0.4964]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.mlp.hook_post\n",[m
[31m-      "tensor([[[-0.0164,  0.0244],\n",[m
[31m-      "         [-0.1624, -0.1538]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.hook_mlp_out\n",[m
[31m-      "tensor([[[-0.3291, -0.2717],\n",[m
[31m-      "         [ 0.2559, -0.0947]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.3.hook_resid_post\n",[m
[31m-      "tensor([[[-0.2391, -5.6897],\n",[m
[31m-      "         [ 0.3105, -2.3390]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.hook_resid_pre\n",[m
[31m-      "tensor([[[-0.2391, -5.6897],\n",[m
[31m-      "         [ 0.3105, -2.3390]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.ln1.hook_scale\n",[m
[31m-      "tensor([[[97.8012],\n",[m
[31m-      "         [ 2.2842]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.attn.hook_q\n",[m
[31m-      "tensor([[[[ 0.0176,  0.3904],\n",[m
[31m-      "          [-0.3139, -0.2219]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.5400, -0.4764],\n",[m
[31m-      "          [-0.1174,  0.7665]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.attn.hook_k\n",[m
[31m-      "tensor([[[[-0.8765, -0.1873],\n",[m
[31m-      "          [ 0.3582, -0.0534]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 2.0230, -0.7626],\n",[m
[31m-      "          [-1.2262,  0.6144]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.attn.hook_v\n",[m
[31m-      "tensor([[[[ 0.0016, -0.0403],\n",[m
[31m-      "          [-0.0679, -0.0094]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.3058,  0.1767],\n",[m
[31m-      "          [ 0.0705, -0.0585]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.attn.hook_attn_scores\n",[m
[31m-      "tensor([[[[ 5.3280e-01, -1.0000e+05],\n",[m
[31m-      "          [-2.3807e+00, -5.7206e+00]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 3.4953e-01, -1.0000e+05],\n",[m
[31m-      "          [-1.3918e+00, -7.3798e+00]]]], device='cuda:0',\n",[m
[31m-      "       grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.attn.hook_attn\n",[m
[31m-      "tensor([[[[1.0000, 0.0000],\n",[m
[31m-      "          [0.9658, 0.0342]],\n",[m
[31m-      "\n",[m
[31m-      "         [[1.0000, 0.0000],\n",[m
[31m-      "          [0.9975, 0.0025]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.attn.hook_z\n",[m
[31m-      "tensor([[[[ 0.0016, -0.0403],\n",[m
[31m-      "          [-0.0679, -0.0094]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.0089, -0.0329],\n",[m
[31m-      "          [-0.0675, -0.0095]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.hook_attn_out\n",[m
[31m-      "tensor([[[-0.1218, -0.1480],\n",[m
[31m-      "         [-0.1804, -0.1694]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.hook_resid_mid\n",[m
[31m-      "tensor([[[-0.3609, -5.8377],\n",[m
[31m-      "         [ 0.1301, -2.5084]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.ln2.hook_scale\n",[m
[31m-      "tensor([[[97.7664],\n",[m
[31m-      "         [ 2.2812]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.mlp.hook_pre\n",[m
[31m-      "tensor([[[ 0.8037, -2.7673],\n",[m
[31m-      "         [-0.8216, -0.8877]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.mlp.hook_post\n",[m
[31m-      "tensor([[[ 0.6342, -0.0074],\n",[m
[31m-      "         [-0.1691, -0.1664]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.hook_mlp_out\n",[m
[31m-      "tensor([[[-0.2850, -0.1238],\n",[m
[31m-      "         [-0.4809, -0.6506]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.4.hook_resid_post\n",[m
[31m-      "tensor([[[-0.6459, -5.9615],\n",[m
[31m-      "         [-0.3508, -3.1590]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.hook_resid_pre\n",[m
[31m-      "tensor([[[-0.6459, -5.9615],\n",[m
[31m-      "         [-0.3508, -3.1590]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.ln1.hook_scale\n",[m
[31m-      "tensor([[[102.8123],\n",[m
[31m-      "         [  2.6391]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.attn.hook_q\n",[m
[31m-      "tensor([[[[-0.0060,  0.1887],\n",[m
[31m-      "          [-0.3377,  0.1366]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.9963,  0.3743],\n",[m
[31m-      "          [ 2.4280,  2.2084]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.attn.hook_k\n",[m
[31m-      "tensor([[[[ 0.0142, -0.3002],\n",[m
[31m-      "          [ 0.1472,  0.9930]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 2.2038, -1.0087],\n",[m
[31m-      "          [-3.0268, -6.6457]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.attn.hook_v\n",[m
[31m-      "tensor([[[[-0.0215, -0.0149],\n",[m
[31m-      "          [ 0.0025, -0.0147]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 1.0960, -0.3051],\n",[m
[31m-      "          [-0.0638, -1.3372]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.attn.hook_attn_scores\n",[m
[31m-      "tensor([[[[ 5.3814e-01, -1.0000e+05],\n",[m
[31m-      "          [ 2.7285e+00, -1.9519e+00]],\n",[m
[32m+[m[32m      "blocks.0.attn.hook_z\n",[m
[32m+[m[32m      "tensor([[[[ 0.1637,  0.2042, -0.0436],\n",[m
[32m+[m[32m      "          [ 0.3102,  0.1178, -0.0882],\n",[m
[32m+[m[32m      "          [ 0.2191,  0.0087,  0.0735]],\n",[m
       "\n",[m
[31m-      "         [[ 3.0952e-01, -1.0000e+05],\n",[m
[31m-      "          [ 4.5407e+00, -7.1070e+00]]]], device='cuda:0',\n",[m
[31m-      "       grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.attn.hook_attn\n",[m
[31m-      "tensor([[[[1.0000e+00, 0.0000e+00],\n",[m
[31m-      "          [9.9081e-01, 9.1899e-03]],\n",[m
[32m+[m[32m      "         [[ 0.1466,  0.2037, -0.0421],\n",[m
[32m+[m[32m      "          [ 0.4144, -0.0454, -0.1669],\n",[m
[32m+[m[32m      "          [ 0.1958, -0.0538,  0.0379]],\n",[m
       "\n",[m
[31m-      "         [[1.0000e+00, 0.0000e+00],\n",[m
[31m-      "          [9.9999e-01, 8.7391e-06]]]], device='cuda:0',\n",[m
[32m+[m[32m      "         [[ 0.0889,  0.1809, -0.0488],\n",[m
[32m+[m[32m      "          [ 0.3082, -0.1456, -0.0097],\n",[m
[32m+[m[32m      "          [ 0.1222, -0.1001,  0.0917]]]], device='cuda:0',\n",[m
       "       grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.attn.hook_z\n",[m
[31m-      "tensor([[[[-0.0215, -0.0149],\n",[m
[31m-      "          [ 0.0025, -0.0147]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.0112, -0.0176],\n",[m
[31m-      "          [ 0.0025, -0.0147]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.hook_attn_out\n",[m
[31m-      "tensor([[[-0.0162, -0.1259],\n",[m
[31m-      "         [ 0.0186, -0.1219]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.hook_resid_mid\n",[m
[31m-      "tensor([[[-0.6621, -6.0874],\n",[m
[31m-      "         [-0.3322, -3.2810]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.ln2.hook_scale\n",[m
[31m-      "tensor([[[102.7626],\n",[m
[31m-      "         [  2.6281]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.mlp.hook_pre\n",[m
[31m-      "tensor([[[-1.6365,  0.3580],\n",[m
[31m-      "         [-0.0223, -0.9029]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.mlp.hook_post\n",[m
[31m-      "tensor([[[-0.0834,  0.2290],\n",[m
[31m-      "         [-0.0109, -0.1656]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.hook_mlp_out\n",[m
[31m-      "tensor([[[-0.1288, -0.1271],\n",[m
[31m-      "         [-0.1899,  0.7349]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.5.hook_resid_post\n",[m
[31m-      "tensor([[[-0.7910, -6.2145],\n",[m
[31m-      "         [-0.5220, -2.5461]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.hook_resid_pre\n",[m
[31m-      "tensor([[[-0.7910, -6.2145],\n",[m
[31m-      "         [-0.5220, -2.5461]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.ln1.hook_scale\n",[m
[31m-      "tensor([[[105.9458],\n",[m
[31m-      "         [  2.8768]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.attn.hook_q\n",[m
[31m-      "tensor([[[[-0.1297, -0.2325],\n",[m
[31m-      "          [-0.2744,  0.5696]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-1.8815,  1.5393],\n",[m
[31m-      "          [ 1.0142,  0.3528]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.attn.hook_k\n",[m
[31m-      "tensor([[[[-0.3286,  0.8828],\n",[m
[31m-      "          [ 0.0438,  0.8537]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.2936, -4.4944],\n",[m
[31m-      "          [-0.2204, -1.2930]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.attn.hook_v\n",[m
[31m-      "tensor([[[[ 0.0514, -0.0394],\n",[m
[31m-      "          [ 0.0726,  0.0171]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.5484,  0.2877],\n",[m
[31m-      "          [ 0.8865, -0.2907]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.attn.hook_attn_scores\n",[m
[31m-      "tensor([[[[ 1.5434e-01, -1.0000e+05],\n",[m
[31m-      "          [ 1.0863e-01, -3.7348e+00]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 7.0014e-01, -1.0000e+05],\n",[m
[31m-      "          [ 1.0427e+00, -2.2651e+00]]]], device='cuda:0',\n",[m
[32m+[m[32m      "blocks.0.hook_attn_out\n",[m
[32m+[m[32m      "tensor([[[ 0.2413, -0.4078, -0.2467],\n",[m
[32m+[m[32m      "         [-1.2967, -0.0258, -0.2561],\n",[m
[32m+[m[32m      "         [-0.4190, -0.3495,  0.0070]]], device='cuda:0',\n",[m
       "       grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.attn.hook_attn\n",[m
[31m-      "tensor([[[[1.0000, 0.0000],\n",[m
[31m-      "          [0.9790, 0.0210]],\n",[m
[31m-      "\n",[m
[31m-      "         [[1.0000, 0.0000],\n",[m
[31m-      "          [0.9647, 0.0353]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.attn.hook_z\n",[m
[31m-      "tensor([[[[ 0.0514, -0.0394],\n",[m
[31m-      "          [ 0.0726,  0.0171]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.0388, -0.0325],\n",[m
[31m-      "          [ 0.1013,  0.0063]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.hook_attn_out\n",[m
[31m-      "tensor([[[-0.0118, -0.0110],\n",[m
[31m-      "         [-0.0058, -0.0594]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.hook_resid_mid\n",[m
[31m-      "tensor([[[-0.8027, -6.2255],\n",[m
[31m-      "         [-0.5278, -2.6055]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.ln2.hook_scale\n",[m
[31m-      "tensor([[[105.9772],\n",[m
[31m-      "         [  2.8952]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.mlp.hook_pre\n",[m
[31m-      "tensor([[[ 0.2320, -0.9757],\n",[m
[31m-      "         [-0.1395, -2.6417]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.mlp.hook_post\n",[m
[31m-      "tensor([[[ 0.1373, -0.1608],\n",[m
[31m-      "         [-0.0620, -0.0104]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.hook_mlp_out\n",[m
[31m-      "tensor([[[-0.1333, -0.0200],\n",[m
[31m-      "         [-0.5677, -0.4207]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.6.hook_resid_post\n",[m
[31m-      "tensor([[[-0.9360, -6.2455],\n",[m
[31m-      "         [-1.0955, -3.0261]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.hook_resid_pre\n",[m
[31m-      "tensor([[[-0.9360, -6.2455],\n",[m
[31m-      "         [-1.0955, -3.0261]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.ln1.hook_scale\n",[m
[31m-      "tensor([[[107.8283],\n",[m
[31m-      "         [  3.1934]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.attn.hook_q\n",[m
[31m-      "tensor([[[[ 0.0529, -0.3191],\n",[m
[31m-      "          [ 0.1380, -0.1388]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 1.1379,  0.0989],\n",[m
[31m-      "          [-1.1619,  1.7103]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.attn.hook_k\n",[m
[31m-      "tensor([[[[ 1.0303, -0.2729],\n",[m
[31m-      "          [-0.1340, -0.0716]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-5.2789, -2.9104],\n",[m
[31m-      "          [-1.1835,  0.7608]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.attn.hook_v\n",[m
[31m-      "tensor([[[[-0.0354,  0.0511],\n",[m
[31m-      "          [-0.0016, -0.0331]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.0496,  0.5697],\n",[m
[31m-      "          [ 0.1556, -0.5996]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.attn.hook_attn_scores\n",[m
[31m-      "tensor([[[[ 6.4755e-01, -1.0000e+05],\n",[m
[31m-      "          [-1.4314e+00, -3.4261e+00]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 6.9034e-01, -1.0000e+05],\n",[m
[31m-      "          [ 4.5207e+00, -1.7921e-01]]]], device='cuda:0',\n",[m
[32m+[m[32m      "blocks.0.hook_resid_mid\n",[m
[32m+[m[32m      "tensor([[[ 0.1587, -0.7330, -0.2266],\n",[m
[32m+[m[32m      "         [-1.3620, -0.0574, -0.0490],\n",[m
[32m+[m[32m      "         [-0.5232, -0.4718,  0.0964]]], device='cuda:0',\n",[m
       "       grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.attn.hook_attn\n",[m
[31m-      "tensor([[[[1.0000, 0.0000],\n",[m
[31m-      "          [0.8802, 0.1198]],\n",[m
[31m-      "\n",[m
[31m-      "         [[1.0000, 0.0000],\n",[m
[31m-      "          [0.9910, 0.0090]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.attn.hook_z\n",[m
[31m-      "tensor([[[[-0.0354,  0.0511],\n",[m
[31m-      "          [-0.0016, -0.0331]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.0252,  0.1133],\n",[m
[31m-      "          [-0.0002, -0.0382]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.hook_attn_out\n",[m
[31m-      "tensor([[[-0.0668,  0.0197],\n",[m
[31m-      "         [-0.0512, -0.0352]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.hook_resid_mid\n",[m
[31m-      "tensor([[[-1.0028, -6.2257],\n",[m
[31m-      "         [-1.1467, -3.0613]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.ln2.hook_scale\n",[m
[31m-      "tensor([[[107.8495],\n",[m
[31m-      "         [  3.2176]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.mlp.hook_pre\n",[m
[31m-      "tensor([[[ 0.1101,  0.3920],\n",[m
[31m-      "         [-0.1142, -0.2265]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.mlp.hook_post\n",[m
[31m-      "tensor([[[ 0.0599,  0.2558],\n",[m
[31m-      "         [-0.0519, -0.0930]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.hook_mlp_out\n",[m
[31m-      "tensor([[[-0.1073,  0.0198],\n",[m
[31m-      "         [-0.5983,  1.3603]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.7.hook_resid_post\n",[m
[31m-      "tensor([[[-1.1101, -6.2059],\n",[m
[31m-      "         [-1.7449, -1.7010]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.hook_resid_pre\n",[m
[31m-      "tensor([[[-1.1101, -6.2059],\n",[m
[31m-      "         [-1.7449, -1.7010]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.ln1.hook_scale\n",[m
[31m-      "tensor([[[108.9520],\n",[m
[31m-      "         [  3.5777]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.attn.hook_q\n",[m
[31m-      "tensor([[[[ 0.1549,  0.4809],\n",[m
[31m-      "          [ 0.0734,  0.0527]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.1936, -2.7093],\n",[m
[31m-      "          [ 0.3946,  0.3873]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.attn.hook_k\n",[m
[31m-      "tensor([[[[-0.0238, -2.3359],\n",[m
[31m-      "          [-0.8001,  0.2133]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-1.2419,  4.4339],\n",[m
[31m-      "          [ 1.4451, -0.7873]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.attn.hook_v\n",[m
[31m-      "tensor([[[[ 0.0624, -0.0107],\n",[m
[31m-      "          [ 0.0244,  0.0348]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.2527,  1.3336],\n",[m
[31m-      "          [-0.6872,  1.2470]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.attn.hook_attn_scores\n",[m
[31m-      "tensor([[[[ 1.1109e-01, -1.0000e+05],\n",[m
[31m-      "          [ 1.1687e+00, -3.3149e+00]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 9.2875e-01, -1.0000e+05],\n",[m
[31m-      "          [ 4.6011e+00, -7.5771e-01]]]], device='cuda:0',\n",[m
[32m+[m[32m      "blocks.0.ln2.hook_scale\n",[m
[32m+[m[32m      "tensor([[[1.1683],\n",[m
[32m+[m[32m      "         [1.2482],\n",[m
[32m+[m[32m      "         [1.0558]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[32m+[m[32m      "blocks.0.ln2.hook_normalized\n",[m
[32m+[m[32m      "tensor([[[ 0.1359, -0.6274, -0.1939],\n",[m
[32m+[m[32m      "         [-1.0912, -0.0460, -0.0393],\n",[m
[32m+[m[32m      "         [-0.4955, -0.4468,  0.0913]]], device='cuda:0',\n",[m
       "       grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.attn.hook_attn\n",[m
[31m-      "tensor([[[[1.0000, 0.0000],\n",[m
[31m-      "          [0.9888, 0.0112]],\n",[m
[31m-      "\n",[m
[31m-      "         [[1.0000, 0.0000],\n",[m
[31m-      "          [0.9953, 0.0047]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.attn.hook_z\n",[m
[31m-      "tensor([[[[ 0.0624, -0.0107],\n",[m
[31m-      "          [ 0.0244,  0.0348]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.0589,  0.0043],\n",[m
[31m-      "          [ 0.0211,  0.0405]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.hook_attn_out\n",[m
[31m-      "tensor([[[-0.0411, -0.0498],\n",[m
[31m-      "         [-0.0290, -0.0137]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.hook_resid_mid\n",[m
[31m-      "tensor([[[-1.1513, -6.2557],\n",[m
[31m-      "         [-1.7739, -1.7147]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.ln2.hook_scale\n",[m
[31m-      "tensor([[[109.0143],\n",[m
[31m-      "         [  3.6601]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.mlp.hook_pre\n",[m
[31m-      "tensor([[[ 0.1417, -1.0045],\n",[m
[31m-      "         [-0.8862, -0.7948]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.mlp.hook_post\n",[m
[31m-      "tensor([[[ 0.0788, -0.1584],\n",[m
[31m-      "         [-0.1665, -0.1697]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.hook_mlp_out\n",[m
[31m-      "tensor([[[0.0293, 0.0111],\n",[m
[31m-      "         [0.4555, 0.9246]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.8.hook_resid_post\n",[m
[31m-      "tensor([[[-1.1220, -6.2446],\n",[m
[31m-      "         [-1.3184, -0.7901]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.hook_resid_pre\n",[m
[31m-      "tensor([[[-1.1220, -6.2446],\n",[m
[31m-      "         [-1.3184, -0.7901]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.ln1.hook_scale\n",[m
[31m-      "tensor([[[109.6327],\n",[m
[31m-      "         [  4.2052]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.attn.hook_q\n",[m
[31m-      "tensor([[[[ 0.3234,  0.2621],\n",[m
[31m-      "          [-0.1864,  0.2236]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.4602, -0.8625],\n",[m
[31m-      "          [ 0.9171,  1.9632]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.attn.hook_k\n",[m
[31m-      "tensor([[[[ 0.0595, -0.2623],\n",[m
[31m-      "          [-0.2702,  0.1587]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.0263, -0.3892],\n",[m
[31m-      "          [ 1.0577,  0.7877]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.attn.hook_v\n",[m
[31m-      "tensor([[[[-0.0445, -0.0842],\n",[m
[31m-      "          [ 0.0314, -0.0117]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.2324, -0.9131],\n",[m
[31m-      "          [ 1.0140,  1.5225]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.attn.hook_attn_scores\n",[m
[31m-      "tensor([[[[ 1.2355e+00, -1.0000e+05],\n",[m
[31m-      "          [ 2.8021e+00, -1.0077e+00]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 1.2389e+00, -1.0000e+05],\n",[m
[31m-      "          [ 3.8741e+00, -1.7935e+00]]]], device='cuda:0',\n",[m
[32m+[m[32m      "blocks.0.mlp.hook_pre\n",[m
[32m+[m[32m      "tensor([[[-0.1052, -0.2519, -0.5952],\n",[m
[32m+[m[32m      "         [ 0.3006, -0.7056, -1.2101],\n",[m
[32m+[m[32m      "         [ 0.1016, -1.3998, -0.5468]]], device='cuda:0',\n",[m
       "       grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.attn.hook_attn\n",[m
[31m-      "tensor([[[[1.0000, 0.0000],\n",[m
[31m-      "          [0.9783, 0.0217]],\n",[m
[31m-      "\n",[m
[31m-      "         [[1.0000, 0.0000],\n",[m
[31m-      "          [0.9966, 0.0034]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.attn.hook_z\n",[m
[31m-      "tensor([[[[-0.0445, -0.0842],\n",[m
[31m-      "          [ 0.0314, -0.0117]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.0385, -0.1021],\n",[m
[31m-      "          [ 0.0348, -0.0064]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.hook_attn_out\n",[m
[31m-      "tensor([[[-1.0932e-01,  8.4637e-02],\n",[m
[31m-      "         [-1.1288e-01,  2.5295e-05]]], device='cuda:0',\n",[m
[32m+[m[32m      "blocks.0.mlp.hook_post\n",[m
[32m+[m[32m      "tensor([[[-0.0482, -0.1009, -0.1642],\n",[m
[32m+[m[32m      "         [ 0.1858, -0.1696, -0.1371],\n",[m
[32m+[m[32m      "         [ 0.0549, -0.1133, -0.1598]]], device='cuda:0',\n",[m
       "       grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.hook_resid_mid\n",[m
[31m-      "tensor([[[-1.2313, -6.1599],\n",[m
[31m-      "         [-1.4313, -0.7900]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.ln2.hook_scale\n",[m
[31m-      "tensor([[[109.6836],\n",[m
[31m-      "         [  4.3473]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.mlp.hook_pre\n",[m
[31m-      "tensor([[[-1.9287,  0.1085],\n",[m
[31m-      "         [-1.3524, -0.7526]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.mlp.hook_post\n",[m
[31m-      "tensor([[[-0.0518,  0.0589],\n",[m
[31m-      "         [-0.1194, -0.1700]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.hook_mlp_out\n",[m
[31m-      "tensor([[[ 0.0074,  0.0072],\n",[m
[31m-      "         [-1.5900, -1.5532]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.9.hook_resid_post\n",[m
[31m-      "tensor([[[-1.2239, -6.1527],\n",[m
[31m-      "         [-3.0213, -2.3433]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.hook_resid_pre\n",[m
[31m-      "tensor([[[-1.2239, -6.1527],\n",[m
[31m-      "         [-3.0213, -2.3433]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.ln1.hook_scale\n",[m
[31m-      "tensor([[[109.9465],\n",[m
[31m-      "         [  5.1061]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.attn.hook_q\n",[m
[31m-      "tensor([[[[ 0.1435,  0.6087],\n",[m
[31m-      "          [ 0.4038, -0.5040]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.0268,  0.4666],\n",[m
[31m-      "          [ 0.5838, -2.1377]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.attn.hook_k\n",[m
[31m-      "tensor([[[[-0.5306,  0.4872],\n",[m
[31m-      "          [ 0.8474, -2.0740]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.9402,  1.2441],\n",[m
[31m-      "          [-0.5061,  1.9980]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.attn.hook_v\n",[m
[31m-      "tensor([[[[-0.0204,  0.0575],\n",[m
[31m-      "          [ 0.0239,  0.0285]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.0852, -0.4385],\n",[m
[31m-      "          [-0.8098,  0.1202]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.attn.hook_attn_scores\n",[m
[31m-      "tensor([[[[ 1.8808e+00, -1.0000e+05],\n",[m
[31m-      "          [ 2.7780e+00, -1.0338e+00]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 2.1647e+00, -1.0000e+05],\n",[m
[31m-      "          [ 4.8024e+00,  4.9549e-02]]]], device='cuda:0',\n",[m
[32m+[m[32m      "blocks.0.hook_mlp_out\n",[m
[32m+[m[32m      "tensor([[[ 0.4491, -0.7972,  0.9901],\n",[m
[32m+[m[32m      "         [-0.2644,  0.3986,  0.7740],\n",[m
[32m+[m[32m      "         [ 0.0903, -0.7448,  0.3327]]], device='cuda:0',\n",[m
       "       grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.attn.hook_attn\n",[m
[31m-      "tensor([[[[1.0000, 0.0000],\n",[m
[31m-      "          [0.9784, 0.0216]],\n",[m
[31m-      "\n",[m
[31m-      "         [[1.0000, 0.0000],\n",[m
[31m-      "          [0.9914, 0.0086]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.attn.hook_z\n",[m
[31m-      "tensor([[[[-0.0204,  0.0575],\n",[m
[31m-      "          [ 0.0239,  0.0285]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.0181,  0.0467],\n",[m
[31m-      "          [ 0.0168,  0.0293]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.hook_attn_out\n",[m
[31m-      "tensor([[[-0.0429,  0.0497],\n",[m
[31m-      "         [-0.0530,  0.0353]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.hook_resid_mid\n",[m
[31m-      "tensor([[[-1.2668, -6.1030],\n",[m
[31m-      "         [-3.0743, -2.3080]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.ln2.hook_scale\n",[m
[31m-      "tensor([[[109.9779],\n",[m
[31m-      "         [  5.2732]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.mlp.hook_pre\n",[m
[31m-      "tensor([[[-0.8390, -0.0591],\n",[m
[31m-      "         [-0.5972,  0.2644]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.mlp.hook_post\n",[m
[31m-      "tensor([[[-0.1685, -0.0282],\n",[m
[31m-      "         [-0.1644,  0.1598]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.hook_mlp_out\n",[m
[31m-      "tensor([[[-0.0691,  0.0629],\n",[m
[31m-      "         [ 0.3384,  1.2087]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.10.hook_resid_post\n",[m
[31m-      "tensor([[[-1.3360, -6.0401],\n",[m
[31m-      "         [-2.7359, -1.0993]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.hook_resid_pre\n",[m
[31m-      "tensor([[[-1.3360, -6.0401],\n",[m
[31m-      "         [-2.7359, -1.0993]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.ln1.hook_scale\n",[m
[31m-      "tensor([[[109.9140],\n",[m
[31m-      "         [  8.4298]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.attn.hook_q\n",[m
[31m-      "tensor([[[[-2.0579,  1.9066],\n",[m
[31m-      "          [-1.6736,  0.7516]],\n",[m
[31m-      "\n",[m
[31m-      "         [[-0.6028, -0.4128],\n",[m
[31m-      "          [-0.3271,  0.1361]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.attn.hook_k\n",[m
[31m-      "tensor([[[[-1.7122, -0.3213],\n",[m
[31m-      "          [ 0.1204, -0.0665]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.8030,  0.1721],\n",[m
[31m-      "          [ 0.0151,  0.0570]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.attn.hook_v\n",[m
[31m-      "tensor([[[[ 0.0431, -0.0790],\n",[m
[31m-      "          [ 0.1141, -0.0090]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.5817,  1.1435],\n",[m
[31m-      "          [-0.1152, -0.0842]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.attn.hook_attn_scores\n",[m
[31m-      "tensor([[[[ 9.2617e-01, -1.0000e+05],\n",[m
[31m-      "          [ 1.8761e+00,  8.5164e-01]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 2.5052e+00, -1.0000e+05],\n",[m
[31m-      "          [ 3.9733e+00, -1.7404e+00]]]], device='cuda:0',\n",[m
[32m+[m[32m      "blocks.0.hook_resid_post\n",[m
[32m+[m[32m      "tensor([[[ 0.6078, -1.5302,  0.7635],\n",[m
[32m+[m[32m      "         [-1.6264,  0.3412,  0.7250],\n",[m
[32m+[m[32m      "         [-0.4328, -1.2165,  0.4291]]], device='cuda:0',\n",[m
       "       grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.attn.hook_attn\n",[m
[31m-      "tensor([[[[1.0000, 0.0000],\n",[m
[31m-      "          [0.7358, 0.2642]],\n",[m
[31m-      "\n",[m
[31m-      "         [[1.0000, 0.0000],\n",[m
[31m-      "          [0.9967, 0.0033]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.attn.hook_z\n",[m
[31m-      "tensor([[[[ 0.0431, -0.0790],\n",[m
[31m-      "          [ 0.1141, -0.0090]],\n",[m
[31m-      "\n",[m
[31m-      "         [[ 0.1854,  0.2439],\n",[m
[31m-      "          [ 0.1134, -0.0092]]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.hook_attn_out\n",[m
[31m-      "tensor([[[ 4.3697,  4.2028],\n",[m
[31m-      "         [ 0.7777, -1.0227]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.hook_resid_mid\n",[m
[31m-      "tensor([[[ 3.0337, -1.8373],\n",[m
[31m-      "         [-1.9582, -2.1220]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.ln2.hook_scale\n",[m
[31m-      "tensor([[[16.4910],\n",[m
[31m-      "         [10.3359]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.mlp.hook_pre\n",[m
[31m-      "tensor([[[-0.1672, -0.4566],\n",[m
[31m-      "         [-0.1546, -1.0319]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.mlp.hook_post\n",[m
[31m-      "tensor([[[-0.0725, -0.1479],\n",[m
[31m-      "         [-0.0678, -0.1560]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.hook_mlp_out\n",[m
[31m-      "tensor([[[-0.1139,  0.5125],\n",[m
[31m-      "         [ 0.3237,  0.5095]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[31m-      "blocks.11.hook_resid_post\n",[m
[31m-      "tensor([[[ 2.9198, -1.3248],\n",[m
[31m-      "         [-1.6345, -1.6125]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
       "ln_final.hook_scale\n",[m
[31m-      "tensor([[[12.5450],\n",[m
[31m-      "         [12.2695]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"[m
[32m+[m[32m      "tensor([[[13.2840],\n",[m
[32m+[m[32m      "         [10.0065],\n",[m
[32m+[m[32m      "         [16.5731]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",[m
[32m+[m[32m      "ln_final.hook_normalized\n",[m
[32m+[m[32m      "tensor([[[-0.0008, -0.1285, -0.0748],\n",[m
[32m+[m[32m      "         [ 0.0953,  0.2186,  0.3553],\n",[m
[32m+[m[32m      "         [-0.0653, -0.1689,  0.0867]]], device='cuda:0',\n",[m
[32m+[m[32m      "       grad_fn=<SliceBackward0>)\n"[m
      ][m
     }[m
    ],[m
[36m@@ -1745,7 +777,7 @@[m
     "def print_corner(tensor, hook):\n",[m
     "    print(hook.name)\n",[m
     "    print(get_corner(tensor))\n",[m
[31m-    "logits = model.run_with_hooks(tokens, fwd_hooks=[(all_hooks_fn, print_corner)])"[m
[32m+[m[32m    "logits = model.run_with_hooks(tokens, fwd_hooks=[(embed_or_first_layer, print_corner)])"[m
    ][m
   },[m
   {[m
[36m@@ -1757,207 +789,41 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 18,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "name": "stdout",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "hook_embed torch.Size([1, 5, 768])\n",[m
[31m-      "hook_pos_embed torch.Size([5, 768])\n",[m
[31m-      "blocks.0.hook_resid_pre torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.0.ln1.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.0.attn.hook_q torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.0.attn.hook_k torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.0.attn.hook_v torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.0.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.0.attn.hook_attn torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.0.attn.hook_z torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.0.hook_attn_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.0.hook_resid_mid torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.0.ln2.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.0.mlp.hook_pre torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.0.mlp.hook_post torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.0.hook_mlp_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.0.hook_resid_post torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.1.hook_resid_pre torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.1.ln1.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.1.attn.hook_q torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.1.attn.hook_k torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.1.attn.hook_v torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.1.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.1.attn.hook_attn torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.1.attn.hook_z torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.1.hook_attn_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.1.hook_resid_mid torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.1.ln2.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.1.mlp.hook_pre torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.1.mlp.hook_post torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.1.hook_mlp_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.1.hook_resid_post torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.2.hook_resid_pre torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.2.ln1.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.2.attn.hook_q torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.2.attn.hook_k torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.2.attn.hook_v torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.2.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.2.attn.hook_attn torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.2.attn.hook_z torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.2.hook_attn_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.2.hook_resid_mid torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.2.ln2.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.2.mlp.hook_pre torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.2.mlp.hook_post torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.2.hook_mlp_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.2.hook_resid_post torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.3.hook_resid_pre torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.3.ln1.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.3.attn.hook_q torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.3.attn.hook_k torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.3.attn.hook_v torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.3.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.3.attn.hook_attn torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.3.attn.hook_z torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.3.hook_attn_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.3.hook_resid_mid torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.3.ln2.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.3.mlp.hook_pre torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.3.mlp.hook_post torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.3.hook_mlp_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.3.hook_resid_post torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.4.hook_resid_pre torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.4.ln1.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.4.attn.hook_q torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.4.attn.hook_k torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.4.attn.hook_v torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.4.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.4.attn.hook_attn torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.4.attn.hook_z torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.4.hook_attn_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.4.hook_resid_mid torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.4.ln2.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.4.mlp.hook_pre torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.4.mlp.hook_post torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.4.hook_mlp_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.4.hook_resid_post torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.5.hook_resid_pre torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.5.ln1.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.5.attn.hook_q torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.5.attn.hook_k torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.5.attn.hook_v torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.5.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.5.attn.hook_attn torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.5.attn.hook_z torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.5.hook_attn_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.5.hook_resid_mid torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.5.ln2.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.5.mlp.hook_pre torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.5.mlp.hook_post torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.5.hook_mlp_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.5.hook_resid_post torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.6.hook_resid_pre torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.6.ln1.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.6.attn.hook_q torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.6.attn.hook_k torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.6.attn.hook_v torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.6.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.6.attn.hook_attn torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.6.attn.hook_z torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.6.hook_attn_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.6.hook_resid_mid torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.6.ln2.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.6.mlp.hook_pre torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.6.mlp.hook_post torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.6.hook_mlp_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.6.hook_resid_post torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.7.hook_resid_pre torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.7.ln1.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.7.attn.hook_q torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.7.attn.hook_k torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.7.attn.hook_v torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.7.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.7.attn.hook_attn torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.7.attn.hook_z torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.7.hook_attn_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.7.hook_resid_mid torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.7.ln2.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.7.mlp.hook_pre torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.7.mlp.hook_post torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.7.hook_mlp_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.7.hook_resid_post torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.8.hook_resid_pre torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.8.ln1.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.8.attn.hook_q torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.8.attn.hook_k torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.8.attn.hook_v torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.8.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.8.attn.hook_attn torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.8.attn.hook_z torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.8.hook_attn_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.8.hook_resid_mid torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.8.ln2.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.8.mlp.hook_pre torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.8.mlp.hook_post torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.8.hook_mlp_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.8.hook_resid_post torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.9.hook_resid_pre torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.9.ln1.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.9.attn.hook_q torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.9.attn.hook_k torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.9.attn.hook_v torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.9.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.9.attn.hook_attn torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.9.attn.hook_z torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.9.hook_attn_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.9.hook_resid_mid torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.9.ln2.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.9.mlp.hook_pre torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.9.mlp.hook_post torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.9.hook_mlp_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.9.hook_resid_post torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.10.hook_resid_pre torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.10.ln1.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.10.attn.hook_q torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.10.attn.hook_k torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.10.attn.hook_v torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.10.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.10.attn.hook_attn torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.10.attn.hook_z torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.10.hook_attn_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.10.hook_resid_mid torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.10.ln2.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.10.mlp.hook_pre torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.10.mlp.hook_post torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.10.hook_mlp_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.10.hook_resid_post torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.11.hook_resid_pre torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.11.ln1.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.11.attn.hook_q torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.11.attn.hook_k torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.11.attn.hook_v torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.11.attn.hook_attn_scores torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.11.attn.hook_attn torch.Size([1, 12, 5, 5])\n",[m
[31m-      "blocks.11.attn.hook_z torch.Size([1, 5, 12, 64])\n",[m
[31m-      "blocks.11.hook_attn_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.11.hook_resid_mid torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.11.ln2.hook_scale torch.Size([1, 5, 1])\n",[m
[31m-      "blocks.11.mlp.hook_pre torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.11.mlp.hook_post torch.Size([1, 5, 3072])\n",[m
[31m-      "blocks.11.hook_mlp_out torch.Size([1, 5, 768])\n",[m
[31m-      "blocks.11.hook_resid_post torch.Size([1, 5, 768])\n",[m
[31m-      "ln_final.hook_scale torch.Size([1, 5, 1])\n"[m
[32m+[m[32m      "hook_embed torch.Size([1, 3, 768])\n",[m
[32m+[m[32m      "hook_pos_embed torch.Size([3, 768])\n",[m
[32m+[m[32m      "blocks.0.hook_resid_pre torch.Size([1, 3, 768])\n",[m
[32m+[m[32m      "blocks.0.ln1.hook_scale torch.Size([1, 3, 1])\n",[m
[32m+[m[32m      "blocks.0.ln1.hook_normalized torch.Size([1, 3, 768])\n",[m
[32m+[m[32m      "blocks.0.attn.hook_q torch.Size([1, 3, 12, 64])\n",[m
[32m+[m[32m      "blocks.0.attn.hook_k torch.Size([1, 3, 12, 64])\n",[m
[32m+[m[32m      "blocks.0.attn.hook_v torch.Size([1, 3, 12, 64])\n",[m
[32m+[m[32m      "blocks.0.attn.hook_attn torch.Size([1, 12, 3, 3])\n",[m
[32m+[m[32m      "blocks.0.attn.hook_z torch.Size([1, 3, 12, 64])\n",[m
[32m+[m[32m      "blocks.0.hook_attn_out torch.Size([1, 3, 768])\n",[m
[32m+[m[32m      "blocks.0.hook_resid_mid torch.Size([1, 3, 768])\n",[m
[32m+[m[32m      "blocks.0.ln2.hook_scale torch.Size([1, 3, 1])\n",[m
[32m+[m[32m      "blocks.0.ln2.hook_normalized torch.Size([1, 3, 768])\n",[m
[32m+[m[32m      "blocks.0.mlp.hook_pre torch.Size([1, 3, 3072])\n",[m
[32m+[m[32m      "blocks.0.mlp.hook_post torch.Size([1, 3, 3072])\n",[m
[32m+[m[32m      "blocks.0.hook_mlp_out torch.Size([1, 3, 768])\n",[m
[32m+[m[32m      "blocks.0.hook_resid_post torch.Size([1, 3, 768])\n",[m
[32m+[m[32m      "ln_final.hook_scale torch.Size([1, 3, 1])\n",[m
[32m+[m[32m      "ln_final.hook_normalized torch.Size([1, 3, 768])\n"[m
      ][m
     }[m
    ],[m
    "source": [[m
[31m-    "cache = {}\n",[m
[31m-    "model.reset_hooks()\n",[m
[31m-    "model.cache_all(cache)\n",[m
[31m-    "logits = model(tokens)\n",[m
[32m+[m[32m    "logits, cache = model.run_with_cache(tokens)\n",[m
     "for name in cache:\n",[m
[31m-    "    print(name, cache[name].shape)\n",[m
[31m-    "model.reset_hooks()"[m
[32m+[m[32m    "    if embed_or_first_layer(name):\n",[m
[32m+[m[32m    "        print(name, cache[name].shape)"[m
    ][m
   },[m
   {[m
[36m@@ -1969,7 +835,7 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 19,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
[36m@@ -1977,31 +843,28 @@[m
      "output_type": "stream",[m
      "text": [[m
       "Run time when copying to the CPU\n",[m
[31m-      "60.8 ms ± 1.64 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",[m
[32m+[m[32m      "63.5 ms ± 4.11 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n",[m
       "Run time when just caching on GPU\n",[m
[31m-      "16.6 ms ± 106 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"[m
[32m+[m[32m      "20 ms ± 1.07 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n"[m
      ][m
     }[m
    ],[m
    "source": [[m
     "random_tokens = torch.randint(1000, 10000, (1, 300))\n",[m
[31m-    "cache = {}\n",[m
[31m-    "model.reset_hooks()\n",[m
[31m-    "model.cache_all(cache, device='cpu')\n",[m
[32m+[m[32m    "\n",[m
     "print('Run time when copying to the CPU')\n",[m
[31m-    "%timeit logits = model(random_tokens)\n",[m
[32m+[m[32m    "%timeit -n 3 logits, cache = model.run_with_cache(random_tokens, device='cpu')\n",[m
     "model.reset_hooks()\n",[m
[31m-    "model.cache_all(cache, device='cuda')\n",[m
[31m-    "print('Run time when just caching on GPU')\n",[m
[31m-    "%timeit logits = model(random_tokens)\n",[m
[31m-    "model.reset_hooks()"[m
[32m+[m[32m    "if torch.cuda.is_available():\n",[m
[32m+[m[32m    "    print('Run time when just caching on GPU')\n",[m
[32m+[m[32m    "    %timeit -n 3 logits, cache = model.run_with_cache(random_tokens, device='cuda')"[m
    ][m
   },[m
   {[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "##Editing Activations\n",[m
[32m+[m[32m    "## Editing Activations\n",[m
     "**To change an activation, add a hook to that HookPoint which returns the new activation**"[m
    ][m
   },[m
[36m@@ -2014,7 +877,7 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 20,[m
    "metadata": {},[m
    "outputs": [],[m
    "source": [[m
[36m@@ -2046,7 +909,7 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 21,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
[36m@@ -2054,14 +917,14 @@[m
      "output_type": "stream",[m
      "text": [[m
       "New logits\n",[m
[31m-      "tensor([[[-5.0197, -4.0007, -6.4540],\n",[m
[31m-      "         [-4.1477, -2.2966, -7.4325],\n",[m
[31m-      "         [-2.6209,  2.9651, -4.4607]]], device='cuda:0',\n",[m
[32m+[m[32m      "tensor([[[ 7.5261, 11.1214,  7.8919],\n",[m
[32m+[m[32m      "         [ 5.5660,  6.0116,  6.5193],\n",[m
[32m+[m[32m      "         [10.0751,  6.8902,  3.8754]]], device='cuda:0',\n",[m
       "       grad_fn=<SliceBackward0>)\n",[m
       "Original logits\n",[m
[31m-      "tensor([[[-5.0197, -4.0007, -6.4540],\n",[m
[31m-      "         [-4.1477, -2.2966, -7.4325],\n",[m
[31m-      "         [-2.7587,  1.3903, -4.3042]]], device='cuda:0',\n",[m
[32m+[m[32m      "tensor([[[ 7.5261, 11.1214,  7.8919],\n",[m
[32m+[m[32m      "         [ 5.5660,  6.0116,  6.5193],\n",[m
[32m+[m[32m      "         [10.1794,  6.3186,  3.4537]]], device='cuda:0',\n",[m
       "       grad_fn=<SliceBackward0>)\n"[m
      ][m
     }[m
[36m@@ -2076,13 +939,13 @@[m
     "    n_ctx = attn.size(-2)\n",[m
     "    key_pos = torch.arange(n_ctx)[None, :]\n",[m
     "    query_pos = torch.arange(n_ctx)[:, None]\n",[m
[31m-    "    mask = (key_pos>(query_pos-2)).cuda()\n",[m
[31m-    "    ZERO = torch.tensor(0.)\n",[m
[31m-    "    if torch.cuda.is_available():\n",[m
[31m-    "        ZERO = ZERO.cuda()\n",[m
[32m+[m[32m    "    mask = (key_pos>(query_pos-2)).to(device)\n",[m
[32m+[m[32m    "    ZERO = torch.tensor(0.).to(device)\n",[m
     "    attn = torch.where(mask, attn, ZERO)\n",[m
     "    return attn\n",[m
[31m-    "logits = model.run_with_hooks(tokens, fwd_hooks=[(filter_hook_attn, restrict_attn)])\n",[m
[32m+[m[32m    "text = \"GPU go brrrr\"\n",[m
[32m+[m[32m    "original_logits = model(text)\n",[m
[32m+[m[32m    "logits = model.run_with_hooks(text, fwd_hooks=[(filter_hook_attn, restrict_attn)])\n",[m
     "print('New logits')\n",[m
     "print(get_corner(logits, 3))\n",[m
     "print('Original logits')\n",[m
[36m@@ -2098,7 +961,7 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 22,[m
    "metadata": {},[m
    "outputs": [],[m
    "source": [[m
[36m@@ -2108,17 +971,18 @@[m
     "\n",[m
     "def freeze_attn(attn, hook):\n",[m
     "    return attn_cache[hook.name]\n",[m
[32m+[m[32m    "text = \"Freezing attention is good\"\n",[m
[32m+[m[32m    "text_2 = \"Freezing attention is bad\"\n",[m
[32m+[m[32m    "logits = model.run_with_hooks(text, fwd_hooks=[(filter_hook_attn, cache_attn)])\n",[m
     "\n",[m
[31m-    "logits = model.run_with_hooks(tokens, fwd_hooks=[(filter_hook_attn, cache_attn)])\n",[m
[31m-    "\n",[m
[31m-    "logits_2 = model.run_with_hooks(tokens_2, fwd_hooks=[(filter_hook_attn, freeze_attn)])"[m
[32m+[m[32m    "logits_2 = model.run_with_hooks(text_2, fwd_hooks=[(filter_hook_attn, freeze_attn)])\n"[m
    ][m
   },[m
   {[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "##Using Hook Contexts"[m
[32m+[m[32m    "## Using Hook Contexts"[m
    ][m
   },[m
   {[m
[36m@@ -2137,27 +1001,27 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 23,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "name": "stdout",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "The| dog| was| green\n",[m
[31m-      "Neuron acts: tensor([-0.0099, -0.1396,  0.6829, -0.0826], device='cuda:0',\n",[m
[32m+[m[32m      "['<|endoftext|>', 'The', ' dog', ' was', ' green']\n",[m
[32m+[m[32m      "Neuron acts: tensor([-0.0097, -0.1189, -0.1593,  0.7287, -0.0855], device='cuda:0',\n",[m
       "       grad_fn=<SelectBackward0>)\n",[m
       "Running total: 1\n",[m
[31m-      "The| cat| was| blue\n",[m
[31m-      "Neuron acts: tensor([-0.0099, -0.1045,  0.6142, -0.0625], device='cuda:0',\n",[m
[32m+[m[32m      "['<|endoftext|>', 'The', ' cat', ' was', ' blue']\n",[m
[32m+[m[32m      "Neuron acts: tensor([-0.0097, -0.1189, -0.1408,  0.5849, -0.0564], device='cuda:0',\n",[m
       "       grad_fn=<SelectBackward0>)\n",[m
       "Running total: 2\n",[m
[31m-      "The| squid| was| mag|enta\n",[m
[31m-      "Neuron acts: tensor([-0.0099,  0.7520,  0.7486, -0.0986, -0.0460], device='cuda:0',\n",[m
[32m+[m[32m      "['<|endoftext|>', 'The', ' squid', ' was', ' mag', 'enta']\n",[m
[32m+[m[32m      "Neuron acts: tensor([-0.0097, -0.1189,  0.6551,  0.7568, -0.1282, -0.0406], device='cuda:0',\n",[m
       "       grad_fn=<SelectBackward0>)\n",[m
       "Running total: 4\n",[m
[31m-      "The| blob|fish| was| grey\n",[m
[31m-      "Neuron acts: tensor([-0.0099,  0.0554,  0.4716,  0.7331, -0.0160], device='cuda:0',\n",[m
[32m+[m[32m      "['<|endoftext|>', 'The', ' blob', 'fish', ' was', ' grey']\n",[m
[32m+[m[32m      "Neuron acts: tensor([-0.0097, -0.1189,  0.0608,  0.4433,  0.7619, -0.0141], device='cuda:0',\n",[m
       "       grad_fn=<SelectBackward0>)\n",[m
       "Running total: 7\n"[m
      ][m
[36m@@ -2177,7 +1041,7 @@[m
     "    print('Running total:', hook.ctx['total'])\n",[m
     "\n",[m
     "for animal_text in animal_texts:\n",[m
[31m-    "    show_tokens(animal_text)\n",[m
[32m+[m[32m    "    print(model.to_str_tokens(animal_text))\n",[m
     "    model.run_with_hooks(animal_text, fwd_hooks=[(f'blocks.{layer}.mlp.hook_post', running_total_hook)])"[m
    ][m
   },[m
[36m@@ -2190,26 +1054,26 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 24,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "name": "stdout",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "The| dog| was| green\n",[m
[31m-      "Neuron acts: tensor([-0.0074, -0.1690,  0.0724,  0.0520], device='cuda:0',\n",[m
[32m+[m[32m      "['<|endoftext|>', 'The', ' dog', ' was', ' green']\n",[m
[32m+[m[32m      "Neuron acts: tensor([-0.0064, -0.1598, -0.1656,  0.0594,  0.0503], device='cuda:0',\n",[m
       "       grad_fn=<SelectBackward0>)\n",[m
[31m-      "Updating best act from -1000.0 to 0.07240163534879684\n",[m
[31m-      "The| cat| was| blue\n",[m
[31m-      "Neuron acts: tensor([-0.0074, -0.1681,  0.1947,  0.0884], device='cuda:0',\n",[m
[32m+[m[32m      "Updating best act from -1000.0 to 0.059385620057582855\n",[m
[32m+[m[32m      "['<|endoftext|>', 'The', ' cat', ' was', ' blue']\n",[m
[32m+[m[32m      "Neuron acts: tensor([-0.0064, -0.1598, -0.1520,  0.2058,  0.0560], device='cuda:0',\n",[m
       "       grad_fn=<SelectBackward0>)\n",[m
[31m-      "Updating best act from 0.07240163534879684 to 0.19472454488277435\n",[m
[31m-      "The| squid| was| mag|enta\n",[m
[31m-      "Neuron acts: tensor([-0.0074, -0.1546,  0.0558, -0.1591, -0.1391], device='cuda:0',\n",[m
[32m+[m[32m      "Updating best act from 0.059385620057582855 to 0.2058115005493164\n",[m
[32m+[m[32m      "['<|endoftext|>', 'The', ' squid', ' was', ' mag', 'enta']\n",[m
[32m+[m[32m      "Neuron acts: tensor([-0.0064, -0.1598, -0.1667,  0.0659, -0.1564, -0.1610], device='cuda:0',\n",[m
       "       grad_fn=<SelectBackward0>)\n",[m
[31m-      "The| blob|fish| was| grey\n",[m
[31m-      "Neuron acts: tensor([-0.0074, -0.1700,  0.0445,  0.1009, -0.0254], device='cuda:0',\n",[m
[32m+[m[32m      "['<|endoftext|>', 'The', ' blob', 'fish', ' was', ' grey']\n",[m
[32m+[m[32m      "Neuron acts: tensor([-0.0064, -0.1598, -0.1666,  0.0968,  0.1661, -0.0204], device='cuda:0',\n",[m
       "       grad_fn=<SelectBackward0>)\n",[m
       "\n",[m
       "Maximally activating dataset example: The cat was blue\n"[m
[36m@@ -2232,7 +1096,7 @@[m
     "        hook.ctx['text'] = text\n",[m
     "\n",[m
     "for animal_text in animal_texts:\n",[m
[31m-    "    (show_tokens(animal_text))\n",[m
[32m+[m[32m    "    (print(model.to_str_tokens(animal_text)))\n",[m
     "    # Use partial to give the hook access to the relevant text\n",[m
     "    model.run_with_hooks(animal_text, fwd_hooks=[(f'blocks.{layer}.mlp.hook_post', partial(best_act_hook, text=animal_text))])\n",[m
     "print()\n",[m
[36m@@ -2244,7 +1108,7 @@[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "##Fancier Examples"[m
[32m+[m[32m    "## Fancier Examples"[m
    ][m
   },[m
   {[m
[36m@@ -2256,7 +1120,7 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 25,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
[36m@@ -2273,9 +1137,9 @@[m
        "<head><meta charset=\"utf-8\" /></head>\n",[m
        "<body>\n",[m
        "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",[m
[31m-       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"831eec86-6c05-4fde-ada6-9fcd0361f724\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"831eec86-6c05-4fde-ada6-9fcd0361f724\")) {                    Plotly.newPlot(                        \"831eec86-6c05-4fde-ada6-9fcd0361f724\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"Head 0\",\"Head 1\",\"Head 2\",\"Head 3\",\"Head 4\",\"Head 5\",\"Head 6\",\"Head 7\",\"Head 8\",\"Head 9\",\"Head 10\",\"Head 11\"],\"y\":[\"Layer 0\",\"Layer 1\",\"Layer 2\",\"Layer 3\",\"Layer 4\",\"Layer 5\",\"Layer 6\",\"Layer 7\",\"Layer 8\",\"Layer 9\",\"Layer 10\",\"Layer 11\"],\"z\":[[0.04330053552985191,0.00550873251631856,0.035496022552251816,0.04028499871492386,0.08938205242156982,0.006121978163719177,0.04877782613039017,0.2621280550956726,0.048664435744285583,0.043070416897535324,0.03245258703827858,0.028410302475094795],[0.2693849503993988,0.16094250977039337,0.0753670409321785,0.042232051491737366,0.07642911374568939,0.028102373704314232,0.0376884862780571,0.03528287634253502,0.02962949499487877,0.0218243096023798,0.05361451581120491,0.01274577435106039],[0.17171594500541687,0.020646074786782265,0.5643649101257324,0.2758866250514984,0.2473434954881668,0.2994534969329834,0.07991326600313187,0.05568588897585869,0.28546738624572754,0.3376002609729767,0.11768447607755661,0.07105861604213715],[0.009194403886795044,0.1215815320611,0.4045916497707367,0.4352235794067383,0.016124896705150604,0.03769311681389809,0.28649359941482544,0.4620950222015381,0.3031528890132904,0.12936030328273773,0.08379188179969788,0.19356486201286316],[0.19270047545433044,0.13807789981365204,0.024287128821015358,0.18242503702640533,0.03030792996287346,0.09375006705522537,0.12186257541179657,0.14334487915039062,0.028820758685469627,0.0634676069021225,0.011693443171679974,0.9948963522911072],[0.017133580520749092,0.006785654928535223,0.11190671473741531,0.05564001947641373,0.06949406117200851,0.008386533707380295,0.3256807327270508,0.017767345532774925,0.024510132148861885,0.013441096991300583,0.0247963797301054,0.015407364815473557],[0.09987252205610275,0.03510669618844986,0.01862085424363613,0.02259347029030323,0.043767545372247696,0.04547529295086861,0.01662030816078186,0.05672649294137955,0.3006802499294281,0.00786999799311161,0.01423895638436079,0.05978212133049965],[0.22037476301193237,0.008651780895888805,0.007142616901546717,0.029018815606832504,0.026982810348272324,0.03643398731946945,0.01142472866922617,0.011974974535405636,0.09358850121498108,0.019579574465751648,0.007948832586407661,0.009565788321197033],[0.019380388781428337,0.009191229939460754,0.013338863849639893,0.013080454431474209,0.015057281590998173,0.057125166058540344,0.04468979686498642,0.13413195312023163,0.018738340586423874,0.009870829060673714,0.030577924102544785,0.03827308490872383],[0.012997651472687721,0.006905592046678066,0.014211631380021572,0.07662119716405869,0.010606463067233562,0.012291194871068,0.008052002638578415,0.022822482511401176,0.023396650329232216,0.007236766163259745,0.02323104999959469,0.008725651539862156],[0.010732806287705898,0.008366882801055908,0.009508141316473484,0.007832774892449379,0.02303035743534565,0.03691909834742546,0.009058453142642975,0.012366312555968761,0.010623447597026825,0.08946598321199417,0.011138318106532097,0.009087705984711647],[0.01936451345682144,0.016402389854192734,0.008759288117289543,0.02491079829633236,0.01801411807537079,0.016655711457133293,0.01114252582192421,0.014467432163655758,0.06150252744555473,0.010209205560386181,0.025479016825556755,0.02675681933760643]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\"},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Prev Token Scores\"}},                        {\"responsive\": true}                    ).then(function(){\n",[m
[32m+[m[32m       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"5d0626e3-40ae-41fa-be39-dee69433d51c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5d0626e3-40ae-41fa-be39-dee69433d51c\")) {                    Plotly.newPlot(                        \"5d0626e3-40ae-41fa-be39-dee69433d51c\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"Head 0\",\"Head 1\",\"Head 2\",\"Head 3\",\"Head 4\",\"Head 5\",\"Head 6\",\"Head 7\",\"Head 8\",\"Head 9\",\"Head 10\",\"Head 11\"],\"y\":[\"Layer 0\",\"Layer 1\",\"Layer 2\",\"Layer 3\",\"Layer 4\",\"Layer 5\",\"Layer 6\",\"Layer 7\",\"Layer 8\",\"Layer 9\",\"Layer 10\",\"Layer 11\"],\"z\":[[0.04071792960166931,0.00548033881932497,0.03155295550823212,0.04088346287608147,0.09320303797721863,0.007854319177567959,0.04900716245174408,0.2593047022819519,0.04634791240096092,0.03894207999110222,0.03257119283080101,0.02560642920434475],[0.2719091475009918,0.15578393638134003,0.0733560174703598,0.04138060286641121,0.07454577833414078,0.026253074407577515,0.034641705453395844,0.03045405074954033,0.02736380510032177,0.02137186750769615,0.05220998078584671,0.013655232265591621],[0.17287911474704742,0.019986653700470924,0.5647273063659668,0.27659139037132263,0.2501072883605957,0.2996378242969513,0.08041121065616608,0.0550864040851593,0.2823280990123749,0.3416977524757385,0.11837786436080933,0.06843344867229462],[0.009553219191730022,0.11814410239458084,0.40166550874710083,0.43422675132751465,0.015433945693075657,0.03870631381869316,0.28543251752853394,0.46151378750801086,0.30166611075401306,0.12941767275333405,0.0840010941028595,0.1914789080619812],[0.19919148087501526,0.13682541251182556,0.022940251976251602,0.18374593555927277,0.02900407277047634,0.0958828255534172,0.11825980246067047,0.14245016872882843,0.02694644220173359,0.061658985912799835,0.011134694330394268,0.9950219392776489],[0.01670466549694538,0.0067744809202849865,0.1100136861205101,0.055943943560123444,0.06978314369916916,0.008302617818117142,0.3245086073875427,0.018269222229719162,0.02383163571357727,0.012738578952848911,0.023949293419718742,0.015048270113766193],[0.09830724447965622,0.037384748458862305,0.0179397352039814,0.022641951218247414,0.0435115247964859,0.04529358446598053,0.01650245115160942,0.056769054383039474,0.29949715733528137,0.008225157856941223,0.013980726711452007,0.0614994652569294],[0.22393788397312164,0.008523724041879177,0.007089734077453613,0.02903461456298828,0.02689494378864765,0.03579970449209213,0.010828644968569279,0.01091319415718317,0.09668897837400436,0.019788645207881927,0.008010514080524445,0.009416871704161167],[0.01846681535243988,0.009020846337080002,0.01219002902507782,0.013359317556023598,0.015230715274810791,0.057260069996118546,0.0467565692961216,0.13804413378238678,0.01833275519311428,0.00985728669911623,0.031162220984697342,0.038123033940792084],[0.012759855948388577,0.0068196565844118595,0.013561910949647427,0.0787479430437088,0.010047259740531445,0.012185041792690754,0.007809004280716181,0.02127920836210251,0.021087586879730225,0.007081530522555113,0.022232772782444954,0.008474490605294704],[0.009739519096910954,0.008037477731704712,0.009283564984798431,0.007581216748803854,0.02236115373671055,0.03545723855495453,0.008616069331765175,0.012826113030314445,0.010426311753690243,0.08756376802921295,0.010404875501990318,0.008675087243318558],[0.018549831584095955,0.015671268105506897,0.008250432088971138,0.023618441075086594,0.018235323950648308,0.01600230671465397,0.01060884166508913,0.013668402098119259,0.06189587712287903,0.009314331226050854,0.02467247284948826,0.02687351405620575]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\"},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Prev Token Scores\"}},                        {\"responsive\": true}                    ).then(function(){\n",[m
        "                            \n",[m
[31m-       "var gd = document.getElementById('831eec86-6c05-4fde-ada6-9fcd0361f724');\n",[m
[32m+[m[32m       "var gd = document.getElementById('5d0626e3-40ae-41fa-be39-dee69433d51c');\n",[m
        "var x = new MutationObserver(function (mutations, observer) {{\n",[m
        "        var display = window.getComputedStyle(gd).display;\n",[m
        "        if (!display || display === 'none') {{\n",[m
[36m@@ -2338,19 +1202,19 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 26,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "name": "stdout",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "Bill| Gates| founded\n",[m
[31m-      "Steve| Jobs| founded\n",[m
[31m-      "Uncorrupted log prob for  Microsoft -2.890326499938965\n",[m
[31m-      "Uncorrupted log prob for  Apple -0.5577391386032104\n",[m
[31m-      "Corrupted (Residual) log prob for  Microsoft -0.5379176139831543\n",[m
[31m-      "Corrupted (Residual) log prob for  Apple -4.753260135650635\n"[m
[32m+[m[32m      "['<|endoftext|>', 'Bill', ' Gates', ' founded']\n",[m
[32m+[m[32m      "['<|endoftext|>', 'Steve', ' Jobs', ' founded']\n",[m
[32m+[m[32m      "Uncorrupted log prob for  Microsoft -2.841726303100586\n",[m
[32m+[m[32m      "Uncorrupted log prob for  Apple -0.4552706778049469\n",[m
[32m+[m[32m      "Corrupted (Residual) log prob for  Microsoft -2.785998821258545\n",[m
[32m+[m[32m      "Corrupted (Residual) log prob for  Apple -0.4480985999107361\n"[m
      ][m
     }[m
    ],[m
[36m@@ -2358,17 +1222,13 @@[m
     "prompt_1 = 'Bill Gates founded'\n",[m
     "response_1 = ' Microsoft'\n",[m
     "logit_index_1 = model.to_tokens(response_1)[0][-1]\n",[m
[31m-    "(show_tokens(prompt_1))\n",[m
[32m+[m[32m    "(print(model.to_str_tokens(prompt_1)))\n",[m
     "prompt_2 = 'Steve Jobs founded'\n",[m
     "response_2 = ' Apple'\n",[m
     "logit_index_2 = model.to_tokens(response_2)[0][-1]\n",[m
[31m-    "show_tokens(prompt_2)\n",[m
[32m+[m[32m    "print(model.to_str_tokens(prompt_2))\n",[m
     "\n",[m
[31m-    "model.reset_hooks()\n",[m
[31m-    "uncorrupted_cache = {}\n",[m
[31m-    "model.cache_all(uncorrupted_cache)\n",[m
[31m-    "logits_1 = model(prompt_1)\n",[m
[31m-    "model.reset_hooks()\n",[m
[32m+[m[32m    "logits_1, uncorrupted_cache = model.run_with_cache(prompt_1)\n",[m
     "\n",[m
     "uncorrupted_logits = model(prompt_2)\n",[m
     "uncorrupted_log_probs = F.log_softmax(uncorrupted_logits, dim=-1)\n",[m
[36m@@ -2402,15 +1262,15 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 27,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "name": "stdout",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "Corrupted (MLP) log prob for  Microsoft -1.2620216608047485\n",[m
[31m-      "Corrupted (MLP) log prob for  Apple -3.78312349319458\n"[m
[32m+[m[32m      "Corrupted (MLP) log prob for  Microsoft -1.0558686256408691\n",[m
[32m+[m[32m      "Corrupted (MLP) log prob for  Apple -3.8480467796325684\n"[m
      ][m
     }[m
    ],[m
[36m@@ -2444,7 +1304,7 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 28,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
[36m@@ -2454,9 +1314,9 @@[m
        "<head><meta charset=\"utf-8\" /></head>\n",[m
        "<body>\n",[m
        "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",[m
[31m-       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"3702174d-c05d-4638-971c-1b717d35b1bb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3702174d-c05d-4638-971c-1b717d35b1bb\")) {                    Plotly.newPlot(                        \"3702174d-c05d-4638-971c-1b717d35b1bb\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.003592077177017927,3.518521043588407e-05,0.003891063155606389,2.3212703581521055e-07,5.5285308917518705e-05,5.597308336291462e-05,0.0034332589711993933,0.0002732022840064019,0.0029226646292954683,0.002940545789897442,0.002129850909113884,0.008645051158964634],[0.000758533482439816,0.0002050419570878148,0.0015435100067406893,0.005715663079172373,0.0019767205230891705,0.0038273294921964407,0.005857702344655991,0.0049976324662566185,0.00603820476680994,0.00709359347820282,0.0055811661295592785,0.00013415755529422313],[0.0030308307614177465,0.009245521388947964,0.0016313729574903846,0.000804485403932631,0.007009111810475588,0.0016857172595337033,0.0017185591859743,0.006297556683421135,0.002686383668333292,0.0009893907699733973,0.0001649387995712459,0.0055989609099924564],[0.00745113380253315,0.003913823515176773,0.0017650112276896834,0.005462334491312504,0.010468884371221066,0.007072430104017258,0.001378487329930067,0.00034171732841059566,0.003306157886981964,0.007262366358190775,0.004265911411494017,0.0033513144589960575],[0.007270172704011202,0.006308634765446186,0.008173353038728237,0.004559357650578022,0.009381885640323162,0.006837799679487944,0.004241572692990303,0.0005858899094164371,0.0075047798454761505,0.007575852796435356,0.009465849958360195,6.02799546389754e-11],[0.43123289942741394,0.9347863793373108,0.008394923061132431,0.005297432653605938,0.006230952218174934,0.9316003322601318,0.006270530167967081,0.009024241007864475,0.009596900083124638,0.015167299658060074,0.012205181643366814,0.009143530391156673],[0.005788560025393963,0.008913245983421803,0.008779487572610378,0.007266685366630554,0.01212959736585617,0.006202520802617073,0.01662491261959076,0.0064754788763821125,0.006109603215008974,0.9234176278114319,0.021422293037176132,0.008027559146285057],[0.007558340206742287,0.17399044334888458,0.8636902570724487,0.008730098605155945,0.009428323246538639,0.009196597151458263,0.029916781932115555,0.07774052768945694,0.008698269724845886,0.009560614824295044,0.9105963706970215,0.029399625957012177],[0.00810645055025816,0.37224259972572327,0.006529814098030329,0.0241122804582119,0.009028324857354164,0.006783122196793556,0.10838603973388672,0.00706509780138731,0.01471142377704382,0.018150396645069122,0.030735064297914505,0.011690647341310978],[0.2286057472229004,0.14841189980506897,0.09047843515872955,0.007041255943477154,0.07373134046792984,0.016446616500616074,0.429669588804245,0.013545876368880272,0.03351767733693123,0.4517155587673187,0.009068936109542847,0.031006038188934326],[0.3321884274482727,0.47969114780426025,0.0251490268856287,0.12197700142860413,0.03952478989958763,0.007737621199339628,0.2834658622741699,0.3774978518486023,0.026012636721134186,0.008178871124982834,0.14432686567306519,0.22876039147377014],[0.007672399748116732,0.03639237582683563,0.02239881083369255,0.004354768432676792,0.023708302527666092,0.08434748649597168,0.03613816201686859,0.04914301261305809,0.0031510302796959877,0.28909462690353394,0.34384384751319885,0.011468829587101936]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",[m
[32m+[m[32m       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"69de6740-46af-4cd5-a54b-277b4b33115d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"69de6740-46af-4cd5-a54b-277b4b33115d\")) {                    Plotly.newPlot(                        \"69de6740-46af-4cd5-a54b-277b4b33115d\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.003905994351953268,3.5952209145762026e-05,0.004172171466052532,3.5960187005912303e-07,8.469432214042172e-05,8.72644450282678e-05,0.0034607029519975185,0.00027035674429498613,0.0029116258956491947,0.002915046876296401,0.001976937986910343,0.008738711476325989],[0.0007499679923057556,0.0002407101565040648,0.0015520025044679642,0.0058710770681500435,0.0019586351700127125,0.00378512404859066,0.005762253887951374,0.005063893739134073,0.005927341990172863,0.007333089597523212,0.005492666736245155,0.00015431750216521323],[0.002346034161746502,0.009266036562621593,0.0009861089056357741,0.0009716905187815428,0.0061231437139213085,0.0012554284185171127,0.0019804565235972404,0.006317672319710255,0.002254305873066187,0.001029372215270996,0.0001667551405262202,0.0053647467866539955],[0.008587845601141453,0.003273827489465475,0.0005649325321428478,0.004961057566106319,0.011061185970902443,0.006070497911423445,0.0007202880806289613,0.0004381289181765169,0.002991984598338604,0.0053751906380057335,0.004999542608857155,0.003105966839939356],[0.007263615727424622,0.005148726515471935,0.007358475588262081,0.0033770189620554447,0.009518183767795563,0.004940064158290625,0.003946785815060139,0.0009216555044986308,0.008479395881295204,0.006947316229343414,0.009883657097816467,1.7590306988779503e-10],[0.4208473563194275,0.9326198697090149,0.0064976830035448074,0.00327422097325325,0.004641546867787838,0.9135849475860596,0.005776946898549795,0.008440577425062656,0.00862582866102457,0.016357004642486572,0.013105225749313831,0.008603718131780624],[0.004241031128913164,0.007246289402246475,0.008592112921178341,0.007005278952419758,0.012711173854768276,0.005883857607841492,0.016941338777542114,0.004781298339366913,0.004412583075463772,0.910973846912384,0.019409414380788803,0.005927461199462414],[0.00530365901067853,0.17892055213451385,0.8508840203285217,0.008158307522535324,0.008954262360930443,0.008981097489595413,0.02992396242916584,0.0862240195274353,0.007319137454032898,0.008996153250336647,0.9062303304672241,0.027602288872003555],[0.008063286542892456,0.3763306438922882,0.0056451414711773396,0.027140170335769653,0.008915718644857407,0.0059437560848891735,0.1041891872882843,0.0070628821849823,0.014370367862284184,0.018130021169781685,0.030749574303627014,0.010684099979698658],[0.22695280611515045,0.15891192853450775,0.09303335100412369,0.005677447654306889,0.0688166469335556,0.015321873128414154,0.41197437047958374,0.012579146772623062,0.030041541904211044,0.4503783881664276,0.008621171116828918,0.028231333941221237],[0.32307612895965576,0.4737868010997772,0.024866238236427307,0.1182694062590599,0.041254978626966476,0.006868741009384394,0.2714543044567108,0.3646150529384613,0.034010615199804306,0.00671814801171422,0.1452292501926422,0.2276238352060318],[0.007341722026467323,0.03802214190363884,0.020804867148399353,0.0032212333753705025,0.022609703242778778,0.08749466389417648,0.03609006851911545,0.048217013478279114,0.0032639089040458202,0.2824133336544037,0.3646734654903412,0.00990039762109518]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",[m
        "                            \n",[m
[31m-       "var gd = document.getElementById('3702174d-c05d-4638-971c-1b717d35b1bb');\n",[m
[32m+[m[32m       "var gd = document.getElementById('69de6740-46af-4cd5-a54b-277b4b33115d');\n",[m
        "var x = new MutationObserver(function (mutations, observer) {{\n",[m
        "        var display = window.getComputedStyle(gd).display;\n",[m
        "        if (!display || display === 'none') {{\n",[m
[36m@@ -2490,9 +1350,7 @@[m
    "source": [[m
     "seq_len = 100\n",[m
     "rand_tokens = torch.randint(1000, 10000, (4, seq_len))\n",[m
[31m-    "rand_tokens_repeat = einops.repeat(rand_tokens, 'batch pos -> batch (2 pos)')\n",[m
[31m-    "if torch.cuda.is_available():\n",[m
[31m-    "    rand_tokens_repeat = rand_tokens_repeat.cuda()\n",[m
[32m+[m[32m    "rand_tokens_repeat = einops.repeat(rand_tokens, 'batch pos -> batch (2 pos)').to(device)\n",[m
     "\n",[m
     "induction_scores_array = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",[m
     "def calc_induction_score(attn_pattern, hook):\n",[m
[36m@@ -2519,15 +1377,15 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 29,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "name": "stdout",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "Original loss on repeated sequence: tensor(-0.1799, device='cuda:0', grad_fn=<MeanBackward0>)\n",[m
[31m-      "Loss on repeated sequence without induction heads: tensor(-6.1036, device='cuda:0', grad_fn=<MeanBackward0>)\n"[m
[32m+[m[32m      "Original loss on repeated sequence: tensor(-0.1913, device='cuda:0', grad_fn=<MeanBackward0>)\n",[m
[32m+[m[32m      "Loss on repeated sequence without induction heads: tensor(-6.2510, device='cuda:0', grad_fn=<MeanBackward0>)\n"[m
      ][m
     },[m
     {[m
[36m@@ -2537,9 +1395,9 @@[m
        "<head><meta charset=\"utf-8\" /></head>\n",[m
        "<body>\n",[m
        "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",[m
[31m-       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"a153e30c-bf12-4dec-bb5a-c0cec0400d2e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a153e30c-bf12-4dec-bb5a-c0cec0400d2e\")) {                    Plotly.newPlot(                        \"a153e30c-bf12-4dec-bb5a-c0cec0400d2e\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,255,0,0,0,255,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,255,0,0],[0,0,255,0,0,0,0,0,0,0,255,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Mask\"}},                        {\"responsive\": true}                    ).then(function(){\n",[m
[32m+[m[32m       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"25d4028a-efbb-4bd3-b9d6-3d3f95c7510c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"25d4028a-efbb-4bd3-b9d6-3d3f95c7510c\")) {                    Plotly.newPlot(                        \"25d4028a-efbb-4bd3-b9d6-3d3f95c7510c\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,255,0,0,0,255,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,255,0,0],[0,0,255,0,0,0,0,0,0,0,255,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Mask\"}},                        {\"responsive\": true}                    ).then(function(){\n",[m
        "                            \n",[m
[31m-       "var gd = document.getElementById('a153e30c-bf12-4dec-bb5a-c0cec0400d2e');\n",[m
[32m+[m[32m       "var gd = document.getElementById('25d4028a-efbb-4bd3-b9d6-3d3f95c7510c');\n",[m
        "var x = new MutationObserver(function (mutations, observer) {{\n",[m
        "        var display = window.getComputedStyle(gd).display;\n",[m
        "        if (!display || display === 'none') {{\n",[m
[36m@@ -2596,6 +1454,13 @@[m
     "px.imshow(attn_head_mask, labels={'y':'Layer', 'x':'Head'}, color_continuous_scale='Blues', title='Mask').show()"[m
    ][m
   },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "markdown",[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "# Further Examples + Features"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
   {[m
    "cell_type": "markdown",[m
    "metadata": {},[m
[36m@@ -2612,12 +1477,12 @@[m
     "An `EasyAblation` object is the combinaison of:\n",[m
     "* The `EasyTransformer` model to be ablated\n",[m
     "* An `AblationConfig` object that store all the parameters of the ablation \n",[m
[31m-    "* An `ExperimentMetric` object that define how we will mesure the effect of the ablation. This can be the loss on a given dataset or the attention score of a precise head.\n",[m
[32m+[m[32m    "* An `ExperimentMetric` object that define how we will measure the effect of the ablation. This can be the loss on a given dataset or the attention score of a precise head.\n",[m
     "\n",[m
     "Here, we defined a metric function that takes in inputs the model and the dataset and output a tensor. You can use `model.run_with_hook()` in the metric function, by you *have* to use the option `reset_hooks_start=False`, else the ablation hooks will be ignored. \n",[m
     "\n",[m
     "You can specify the `target_module` from \"mlp\", \"attn_layer\", \"attn_head\". \n",[m
[31m-    "If you chose \"attn_head\" you can define wich part of the head computation to ablate (\"z\", \"q\", \"v\", \"k\", \"attn\", \"attn_scores\") \n",[m
[32m+[m[32m    "If you chose \"attn_head\" you can define which part of the head computation to ablate (\"z\", \"q\", \"v\", \"k\", \"attn\", \"attn_scores\") \n",[m
     "\n",[m
     "The supported ablation types are `mean`, `zero`, `neg` and `custom`. For mean ablations, you can specify a `mean_dataset` in the config that can be different to the one used in the metric. \n",[m
     "\n",[m
[36m@@ -2626,7 +1491,7 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 30,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
[36m@@ -2638,11 +1503,11 @@[m
       "* head_circuit: z\n",[m
       "* layers: all\n",[m
       "* heads: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",[m
[31m-      "* dataset: tensor([9664, 9398, 1651, 4308, 5181, 4839, 8083, 8151, 7112, 5094],\n",[m
[32m+[m[32m      "* dataset: tensor([1145, 7879, 3717, 5613, 7860, 7867, 9639, 6182, 2987, 2705],\n",[m
       "       device='cuda:0') ... \n",[m
[31m-      "tensor([9664, 9398, 1651, 4308, 5181, 4839, 8083, 8151, 7112, 5094],\n",[m
[32m+[m[32m      "tensor([7796, 5309, 6540, 2826, 3098, 9550, 7882, 4919, 7166, 5802],\n",[m
       "       device='cuda:0') ... \n",[m
[31m-      "tensor([5495, 5173, 6459, 3429, 7828, 8608, 3478, 7129, 9832, 3517],\n",[m
[32m+[m[32m      "tensor([5909, 5771, 7850, 8157, 7896, 8485, 3135, 1822, 2248, 8013],\n",[m
       "       device='cuda:0') ... \n",[m
       "* verbose: True\n",[m
       "* beg_layer: 0\n",[m
[36m@@ -2651,7 +1516,7 @@[m
       "* mean_dataset: None\n",[m
       "* cache_means: True\n",[m
       "* compute_means: False\n",[m
[31m-      "* abl_fn: <function zero_fn>\n",[m
[32m+[m[32m      "* abl_fn: <function zero_fn at 0x7f97a46225f0>\n",[m
       "\n"[m
      ][m
     },[m
[36m@@ -2659,7 +1524,7 @@[m
      "name": "stderr",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:06<00:00,  1.89it/s]\n"[m
[32m+[m[32m      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:06<00:00,  1.79it/s]\n"[m
      ][m
     },[m
     {[m
[36m@@ -2669,9 +1534,9 @@[m
        "<head><meta charset=\"utf-8\" /></head>\n",[m
        "<body>\n",[m
        "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",[m
[31m-       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"8fadd42e-70fd-406c-924f-d4c429fa642e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8fadd42e-70fd-406c-924f-d4c429fa642e\")) {                    Plotly.newPlot(                        \"8fadd42e-70fd-406c-924f-d4c429fa642e\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[1.3020412921905518,1.0587255954742432,0.12814700603485107,0.7061997652053833,0.18511295318603516,1.5170233249664307,0.028879880905151367,1.0684013366699219,0.22811269760131836,0.5827785730361938,0.619857907295227,0.16437304019927979],[0.04361236095428467,0.11206948757171631,1.7895886898040771,1.0098443031311035,0.3083902597427368,0.006970405578613281,0.04325509071350098,0.008532404899597168,0.1272125244140625,0.08619022369384766,3.3537726402282715,0.09286808967590332],[0.33551645278930664,-0.013123691082000732,0.027096867561340332,0.5057857036590576,0.05007314682006836,0.7350314855575562,0.3376188278198242,0.08250784873962402,0.25117719173431396,0.6208055019378662,-0.10749566555023193,0.20027554035186768],[0.07184147834777832,0.15297365188598633,0.08050549030303955,0.15680670738220215,0.04139411449432373,0.06077921390533447,0.08515739440917969,-0.015675365924835205,0.10861217975616455,-0.014018058776855469,0.06121265888214111,0.005149960517883301],[-0.01312190294265747,0.023137688636779785,0.009310126304626465,-0.038982093334198,0.013856172561645508,0.025696396827697754,0.029178857803344727,0.012053132057189941,0.25726306438446045,0.0084608793258667,0.007892727851867676,0.10578703880310059],[0.04205358028411865,1.8548169136047363,0.008978843688964844,-0.007146656513214111,-0.019748032093048096,0.10925114154815674,0.03037393093109131,0.03259468078613281,-0.2608652114868164,1.1132173538208008,-0.04250538349151611,0.07358217239379883],[0.004308819770812988,0.005161285400390625,0.08027875423431396,0.14875257015228271,0.06682455539703369,-0.06136369705200195,0.0772707462310791,0.3091837167739868,-0.04232895374298096,0.021047472953796387,0.10309672355651855,-0.04411029815673828],[0.015117168426513672,0.011797785758972168,0.0612025260925293,-0.03385430574417114,-0.003225564956665039,-0.0007331371307373047,1.028465986251831,-0.10037630796432495,0.012833714485168457,0.013250231742858887,-0.1157684326171875,-0.035156846046447754],[-0.009239017963409424,-0.027792036533355713,0.005861043930053711,0.0033625364303588867,0.0014882087707519531,0.23396790027618408,-0.013568222522735596,0.014933109283447266,-0.0014252662658691406,-0.037041544914245605,0.09074282646179199,0.010814070701599121],[0.043901920318603516,-0.004437685012817383,0.009342193603515625,0.052899956703186035,0.02912163734436035,-0.009857773780822754,0.04354870319366455,0.0179135799407959,0.012874007225036621,-0.05335420370101929,0.00789177417755127,-0.013370394706726074],[0.08638322353363037,-0.012689471244812012,0.04838001728057861,0.03233599662780762,-0.004604339599609375,0.010167598724365234,0.11710262298583984,-0.14478355646133423,0.04433035850524902,0.0033799409866333008,0.059772610664367676,0.03197789192199707],[0.38053929805755615,0.010635614395141602,0.006084918975830078,-0.001482248306274414,0.040970683097839355,0.060585737228393555,0.0646742582321167,0.04635584354400635,-0.1784508228302002,-0.00963437557220459,-0.16453319787979126,0.0012677907943725586]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Induction Score Variation after Ablation\"}},                        {\"responsive\": true}                    ).then(function(){\n",[m
[32m+[m[32m       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"584076f1-3763-4024-b3bd-0f02d3364809\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"584076f1-3763-4024-b3bd-0f02d3364809\")) {                    Plotly.newPlot(                        \"584076f1-3763-4024-b3bd-0f02d3364809\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[1.6971933841705322,1.3869616985321045,0.2165292501449585,0.5898165702819824,0.47234082221984863,1.7418458461761475,-0.06042802333831787,0.8928067684173584,0.24465107917785645,0.607535719871521,1.138780117034912,0.13549602031707764],[0.058094143867492676,0.17279589176177979,1.2779083251953125,0.6758456230163574,0.6373288631439209,0.02272975444793701,0.010825634002685547,-0.028103649616241455,-0.039536476135253906,-0.014598190784454346,2.875894069671631,0.10614955425262451],[0.2605001926422119,-0.034604787826538086,0.05356144905090332,0.5490217208862305,0.06708049774169922,0.5508884191513062,0.33639466762542725,0.1811509132385254,0.16837036609649658,0.7119249105453491,-0.017066657543182373,-0.08975845575332642],[0.028551578521728516,0.12160634994506836,0.11696314811706543,0.1710069179534912,0.17406904697418213,0.09080147743225098,0.036742210388183594,0.5137574672698975,0.13817894458770752,0.018657326698303223,0.0009646415710449219,0.04137873649597168],[0.0067555904388427734,0.02411973476409912,0.004266858100891113,-0.04147297143936157,0.04154229164123535,0.006260871887207031,0.014398336410522461,-0.00432586669921875,0.2755924463272095,0.013529658317565918,0.013140320777893066,-0.017647862434387207],[0.026845335960388184,2.618407726287842,0.01647818088531494,-0.03756844997406006,0.004427075386047363,0.03914153575897217,0.15121996402740479,0.05520033836364746,-0.17777788639068604,1.7676055431365967,-0.070090651512146,0.06062638759613037],[-0.004775881767272949,0.01588892936706543,0.05790436267852783,0.12303698062896729,0.04795217514038086,-0.025997281074523926,0.15656495094299316,0.46312177181243896,-0.012411296367645264,0.21032750606536865,0.16043555736541748,-0.06551218032836914],[0.010029911994934082,0.029918909072875977,0.19061172008514404,-0.056829750537872314,-0.021794021129608154,-0.0011345148086547852,1.38846755027771,-0.10011672973632812,0.007178068161010742,0.0070198774337768555,-0.010838150978088379,0.012207269668579102],[-0.024966835975646973,-0.006492137908935547,-0.0006802082061767578,0.003035306930541992,0.0016344785690307617,0.36534881591796875,-0.031663715839385986,-0.037547290325164795,-0.006609559059143066,-0.03297162055969238,0.11164295673370361,0.027256250381469727],[0.044211745262145996,-0.0036894679069519043,0.0005308389663696289,0.07143712043762207,0.009466290473937988,0.11762654781341553,0.005916595458984375,0.02609074115753174,0.004941821098327637,0.00967705249786377,0.0009812116622924805,-0.051555335521698],[0.0658193826675415,-0.006128966808319092,0.02810359001159668,0.021500706672668457,-0.026218414306640625,-0.0004623532295227051,0.09512066841125488,-0.10517477989196777,0.06480729579925537,0.0007592439651489258,0.0581812858581543,0.00851583480834961],[0.34506237506866455,0.019454002380371094,0.01941502094268799,0.017427802085876465,0.04024386405944824,0.04114723205566406,0.07487750053405762,0.03449523448944092,-0.14319437742233276,0.03248262405395508,-0.14486229419708252,-0.00732874870300293]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Induction Score Variation after Ablation\"}},                        {\"responsive\": true}                    ).then(function(){\n",[m
        "                            \n",[m
[31m-       "var gd = document.getElementById('8fadd42e-70fd-406c-924f-d4c429fa642e');\n",[m
[32m+[m[32m       "var gd = document.getElementById('584076f1-3763-4024-b3bd-0f02d3364809');\n",[m
        "var x = new MutationObserver(function (mutations, observer) {{\n",[m
        "        var display = window.getComputedStyle(gd).display;\n",[m
        "        if (!display || display === 'none') {{\n",[m
[36m@@ -2728,14 +1593,14 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 31,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "name": "stdout",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "Loss on the repeated random token after zero-ablations of the induction heads -6.103604793548584\n"[m
[32m+[m[32m      "Loss on the repeated random token after zero-ablations of the induction heads -6.250967979431152\n"[m
      ][m
     }[m
    ],[m
[36m@@ -2756,14 +1621,14 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 32,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "name": "stdout",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "Loss on the repeated random token after mean-ablations of the induction heads -1.886552095413208\n"[m
[32m+[m[32m      "Loss on the repeated random token after mean-ablations of the induction heads -1.8151707649230957\n"[m
      ][m
     }[m
    ],[m
[36m@@ -2805,14 +1670,14 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 33,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "name": "stderr",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:03<00:00,  3.99it/s]\n"[m
[32m+[m[32m      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:03<00:00,  3.52it/s]\n"[m
      ][m
     },[m
     {[m
[36m@@ -2822,9 +1687,9 @@[m
        "<head><meta charset=\"utf-8\" /></head>\n",[m
        "<body>\n",[m
        "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",[m
[31m-       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"ce793e4c-1c38-4a00-b180-0d4bf7a2cb4d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ce793e4c-1c38-4a00-b180-0d4bf7a2cb4d\")) {                    Plotly.newPlot(                        \"ce793e4c-1c38-4a00-b180-0d4bf7a2cb4d\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[-0.005715012550354004,0.20118749141693115,-0.018460571765899658,0.058458685874938965,0.1257801055908203,0.16063523292541504,0.049353599548339844,0.08805644512176514,0.010966897010803223,0.11785244941711426,0.0013451576232910156,-0.012298047542572021],[0.09422862529754639,0.08069741725921631,0.004743456840515137,0.01659071445465088,0.04389679431915283,0.04871857166290283,0.010638594627380371,0.05245411396026611,-0.012806832790374756,-0.009555280208587646,0.006121516227722168,0.18877148628234863],[-0.0030521154403686523,-0.029201924800872803,-0.02726447582244873,0.05180537700653076,-6.824731826782227e-05,0.06184744834899902,0.03715050220489502,-3.057718276977539e-05,0.01715528964996338,0.4065823554992676,0.29954326152801514,0.007559061050415039],[-0.009648382663726807,-0.011611402034759521,0.02335381507873535,0.3247164487838745,-0.010219156742095947,-0.0014047026634216309,0.06753087043762207,0.7906279563903809,0.06046950817108154,0.0011087656021118164,0.055030226707458496,-0.016203343868255615],[0.00734400749206543,0.0009175539016723633,0.0012489557266235352,-0.007320046424865723,-0.0007674098014831543,-0.011662185192108154,-0.024334251880645752,0.14045584201812744,0.057981014251708984,0.019682884216308594,0.03404092788696289,19.77796745300293],[0.07983839511871338,0.2931864261627197,-0.005029201507568359,-0.004603266716003418,-0.01290971040725708,-0.028782010078430176,0.049476027488708496,0.005803465843200684,0.024251341819763184,0.0304793119430542,0.010630369186401367,0.014109015464782715],[0.04488182067871094,0.013062357902526855,0.12291264533996582,-0.0032597780227661133,0.035675644874572754,-0.06021934747695923,-0.0008783936500549316,0.0031195878982543945,-0.0031651854515075684,0.7734339237213135,-0.025703251361846924,0.01769733428955078],[0.0005708932876586914,0.058532118797302246,1.2291977405548096,-0.000775754451751709,0.0011941194534301758,0.00814962387084961,0.03690671920776367,0.0976494550704956,0.0023419857025146484,0.006860136985778809,0.20490491390228271,-0.03513103723526001],[0.0074956417083740234,0.10657286643981934,0.02511894702911377,0.005038857460021973,0.005007386207580566,-0.001976191997528076,0.07026779651641846,0.04336655139923096,0.014369726181030273,0.0037003755569458008,-0.03220111131668091,-0.0005896687507629395],[0.009620189666748047,0.17540907859802246,0.036395907402038574,0.029438138008117676,-8.118152618408203e-05,0.004353165626525879,0.14033818244934082,-0.008132755756378174,0.009514927864074707,0.2304621934890747,0.005598664283752441,0.09044504165649414],[0.28162074089050293,0.5267332792282104,0.03782224655151367,0.151678204536438,0.028636693954467773,-0.0028944015502929688,0.3536280393600464,-0.2279999852180481,0.014330387115478516,0.005995988845825195,0.030382394790649414,0.08678793907165527],[0.04119694232940674,0.01963961124420166,0.052306175231933594,0.008401989936828613,0.044999122619628906,0.09575212001800537,0.07311010360717773,0.08533644676208496,-0.07347571849822998,0.3833467960357666,-0.18008941411972046,0.03286254405975342]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Induction Score Variation after Custom Ablation\"}},                        {\"responsive\": true}                    ).then(function(){\n",[m
[32m+[m[32m       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"16315960-5249-4725-82c9-514d9cd99b0f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"16315960-5249-4725-82c9-514d9cd99b0f\")) {                    Plotly.newPlot(                        \"16315960-5249-4725-82c9-514d9cd99b0f\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.0040547847747802734,0.136976957321167,-0.01201850175857544,-0.02415621280670166,0.14518451690673828,0.00023245811462402344,-0.053121864795684814,0.0034476518630981445,0.027003049850463867,-0.030335605144500732,0.09246742725372314,0.007924675941467285],[0.06818926334381104,0.26531338691711426,0.023389458656311035,-0.0028123855590820312,0.06556487083435059,0.030370116233825684,-0.05295252799987793,0.019021272659301758,0.02004396915435791,-0.014848947525024414,-0.015585124492645264,0.09185338020324707],[0.004446625709533691,-0.09305363893508911,0.0940084457397461,0.07925724983215332,-0.046594321727752686,0.35983431339263916,0.16707229614257812,-0.007771611213684082,-0.0050667524337768555,0.26160120964050293,0.10688662528991699,0.031535983085632324],[-0.06946921348571777,-0.004516899585723877,0.018883824348449707,0.14727413654327393,0.05800366401672363,-0.021434903144836426,0.05245161056518555,0.678423285484314,0.03184151649475098,-0.005196511745452881,0.18436336517333984,-0.005382359027862549],[-0.005620837211608887,0.0038210153579711914,0.008984923362731934,-0.013933420181274414,0.023659825325012207,-0.0043408870697021484,0.03943932056427002,0.16353106498718262,0.009762763977050781,0.0023180246353149414,0.03035879135131836,17.23048973083496],[0.09657931327819824,0.5674428939819336,-0.04217970371246338,0.020028352737426758,-0.006098687648773193,0.12432718276977539,0.4718031883239746,0.00030803680419921875,0.0029298067092895508,0.08565306663513184,-0.0042632222175598145,0.012482643127441406],[0.03220224380493164,0.030820488929748535,0.1315551996231079,-0.007908523082733154,0.024081945419311523,0.011092662811279297,0.023250222206115723,0.026682615280151367,0.018324851989746094,1.1420009136199951,0.053797364234924316,-0.002041935920715332],[-0.0012159347534179688,0.09322631359100342,1.6789963245391846,0.010523557662963867,-0.0013919472694396973,0.011861562728881836,0.004554033279418945,0.06646132469177246,0.008737921714782715,0.0002294778823852539,0.4957001209259033,-0.01977062225341797],[0.020593643188476562,0.18359696865081787,0.025328636169433594,0.008359193801879883,0.005977630615234375,0.01290285587310791,0.06969892978668213,0.01572883129119873,0.01107323169708252,-0.004651427268981934,-0.02744060754776001,0.01793956756591797],[0.03287327289581299,0.28986847400665283,0.02516353130340576,0.01652967929840088,-0.014822959899902344,0.0089339017868042,0.10644280910491943,-0.0025717616081237793,0.008879780769348145,0.3368932008743286,0.0021986961364746094,0.061269283294677734],[0.23576390743255615,0.3857918977737427,0.04165637493133545,0.12947893142700195,0.02391064167022705,-0.0014139413833618164,0.2385958433151245,-0.19227910041809082,0.020760178565979004,0.000840306282043457,0.023846149444580078,0.06390154361724854],[0.0646125078201294,0.05805957317352295,0.05420863628387451,0.03927290439605713,0.014751672744750977,0.09310495853424072,0.07431399822235107,0.08926510810852051,-0.03445369005203247,0.38520896434783936,-0.1635289192199707,0.02159559726715088]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Induction Score Variation after Custom Ablation\"}},                        {\"responsive\": true}                    ).then(function(){\n",[m
        "                            \n",[m
[31m-       "var gd = document.getElementById('ce793e4c-1c38-4a00-b180-0d4bf7a2cb4d');\n",[m
[32m+[m[32m       "var gd = document.getElementById('16315960-5249-4725-82c9-514d9cd99b0f');\n",[m
        "var x = new MutationObserver(function (mutations, observer) {{\n",[m
        "        var display = window.getComputedStyle(gd).display;\n",[m
        "        if (!display || display === 'none') {{\n",[m
[36m@@ -2888,14 +1753,14 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 34,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "name": "stderr",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02<00:00,  4.94it/s]\n"[m
[32m+[m[32m      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:03<00:00,  3.90it/s]\n"[m
      ][m
     },[m
     {[m
[36m@@ -2905,9 +1770,9 @@[m
        "<head><meta charset=\"utf-8\" /></head>\n",[m
        "<body>\n",[m
        "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",[m
[31m-       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"90824368-faa7-4b25-882d-1807d459f7f5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"90824368-faa7-4b25-882d-1807d459f7f5\")) {                    Plotly.newPlot(                        \"90824368-faa7-4b25-882d-1807d459f7f5\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[3.1136646270751953,2.938094139099121,3.0309367179870605,2.833833694458008,2.921074867248535,2.926629066467285,3.183659076690674,3.0834522247314453,3.126741886138916,3.039585590362549,3.2878966331481934,3.3109636306762695],[3.1018333435058594,3.187206745147705,3.111567497253418,3.154606819152832,3.118622303009033,3.2858681678771973,3.1267409324645996,3.180922508239746,3.153449535369873,3.148238182067871,3.179370880126953,3.0993175506591797],[3.146785259246826,3.1638755798339844,3.1542696952819824,3.1417250633239746,3.1314616203308105,3.140338897705078,3.143453598022461,3.1311378479003906,3.139739990234375,3.1561264991760254,3.1596155166625977,3.151765823364258],[3.152698040008545,3.1451950073242188,3.135244369506836,3.145699977874756,3.1402926445007324,3.1618542671203613,3.113875389099121,3.1624183654785156,3.1453518867492676,3.1352763175964355,3.152989387512207,3.1456398963928223],[3.1424741744995117,3.150804042816162,3.1524038314819336,3.1907615661621094,3.1369552612304688,3.14194917678833,3.144876480102539,3.1362805366516113,3.1374940872192383,3.161905288696289,3.180091381072998,3.1556825637817383],[3.142375946044922,3.1445698738098145,3.140155792236328,3.1462602615356445,3.13565731048584,3.1454148292541504,3.1484155654907227,3.135702610015869,3.1433773040771484,3.150801181793213,3.1184983253479004,3.1567087173461914],[3.1462697982788086,3.139594554901123,3.158449649810791,3.138382911682129,3.0864925384521484,3.131495952606201,3.159827709197998,3.1148486137390137,3.148930072784424,3.145641803741455,3.1409287452697754,3.1420035362243652],[3.145998001098633,3.140981674194336,3.1410675048828125,3.146030902862549,3.1583738327026367,3.0560193061828613,3.156465530395508,3.139133930206299,3.129075050354004,3.143261432647705,3.1424078941345215,3.140145778656006],[3.13948392868042,3.141782283782959,3.098599433898926,3.117522716522217,3.103144645690918,3.124969005584717,3.13424015045166,3.131117343902588,3.153909683227539,3.157688617706299,3.3238534927368164,1.9917244911193848],[2.9563522338867188,3.1434922218322754,3.0705037117004395,3.1469473838806152,3.139052391052246,3.1933555603027344,3.0835204124450684,3.1683950424194336,1.9120244979858398,3.097177028656006,3.1362786293029785,3.1441822052001953],[-0.9783134460449219,3.152951717376709,3.130704402923584,3.034371852874756,3.2078027725219727,3.1445508003234863,3.017005443572998,3.559432029724121,3.1451072692871094,3.1589369773864746,3.105111598968506,3.1271238327026367],[3.253276824951172,3.141843318939209,2.9854421615600586,2.9185080528259277,3.1658873558044434,3.1628823280334473,3.1450934410095215,3.1039681434631348,3.154047966003418,3.1475372314453125,3.4216604232788086,3.088712692260742]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Absolute Log Logit Prob Difference After Patching\"}},                        {\"responsive\": true}                    ).then(function(){\n",[m
[32m+[m[32m       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"3c4d30a4-81b8-41fd-a387-b1c68ef4a0fa\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3c4d30a4-81b8-41fd-a387-b1c68ef4a0fa\")) {                    Plotly.newPlot(                        \"3c4d30a4-81b8-41fd-a387-b1c68ef4a0fa\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Absolute Log Logit Prob Difference After Patching\"}},                        {\"responsive\": true}                    ).then(function(){\n",[m
        "                            \n",[m
[31m-       "var gd = document.getElementById('90824368-faa7-4b25-882d-1807d459f7f5');\n",[m
[32m+[m[32m       "var gd = document.getElementById('3c4d30a4-81b8-41fd-a387-b1c68ef4a0fa');\n",[m
        "var x = new MutationObserver(function (mutations, observer) {{\n",[m
        "        var display = window.getComputedStyle(gd).display;\n",[m
        "        if (!display || display === 'none') {{\n",[m
[36m@@ -2950,12 +1815,12 @@[m
     "source_logits = model.to_tokens(source_labels).squeeze()\n",[m
     "target_logits = model.to_tokens(target_labels).squeeze()\n",[m
     "\n",[m
[31m-    "tokens_pos = [2,2] #the position of \"founded\" in the target sentences, where to get the next token prediction\n",[m
[32m+[m[32m    "tokens_pos = [2,2] # The position of \"founded\" in the target sentences, where to get the next token prediction\n",[m
     "\n",[m
     "def fact_transfer_score(model, target_dataset):\n",[m
     "    logits = model(target_dataset)\n",[m
     "    log_probs = F.log_softmax(logits, dim=-1)\n",[m
[31m-    "    logit_diff = (log_probs[torch.arange(len(target_logits)),tokens_pos,target_logits] - #logit target - logit source (positive by default)\n",[m
[32m+[m[32m    "    logit_diff = (log_probs[torch.arange(len(target_logits)),tokens_pos,target_logits] - # logit target - logit source (positive by default)\n",[m
     "                  log_probs[torch.arange(len(source_logits)),tokens_pos,source_logits])\n",[m
     "\n",[m
     "    return logit_diff.mean() \n",[m
[36m@@ -2967,13 +1832,6 @@[m
     "px.imshow(result, labels={'y':'Layer', 'x':'Head'}, color_continuous_scale='Blues', title='Absolute Log Logit Prob Difference After Patching').show()"[m
    ][m
   },[m
[31m-  {[m
[31m-   "cell_type": "code",[m
[31m-   "execution_count": null,[m
[31m-   "metadata": {},[m
[31m-   "outputs": [],[m
[31m-   "source": [][m
[31m-  },[m
   {[m
    "cell_type": "markdown",[m
    "metadata": {},[m
[36m@@ -2983,14 +1841,14 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 35,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "name": "stderr",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02<00:00,  4.83it/s]\n"[m
[32m+[m[32m      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:03<00:00,  3.82it/s]\n"[m
      ][m
     },[m
     {[m
[36m@@ -3000,9 +1858,9 @@[m
        "<head><meta charset=\"utf-8\" /></head>\n",[m
        "<body>\n",[m
        "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",[m
[31m-       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"799c0431-e166-45d5-b3fb-45194eccc4c9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"799c0431-e166-45d5-b3fb-45194eccc4c9\")) {                    Plotly.newPlot(                        \"799c0431-e166-45d5-b3fb-45194eccc4c9\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[3.0957894325256348,2.9346256256103516,3.008780002593994,2.872807502746582,3.0002083778381348,2.9159674644470215,3.112184524536133,3.1129164695739746,3.152657985687256,3.123988151550293,3.2244906425476074,3.236884593963623],[3.142024040222168,3.150479316711426,3.146721363067627,3.161813259124756,3.122818946838379,3.1428470611572266,3.1382827758789062,3.164090156555176,3.1384072303771973,3.1556167602539062,3.177811622619629,3.1275925636291504],[3.144179344177246,3.1526222229003906,3.148252487182617,3.1401047706604004,3.1335792541503906,3.144242286682129,3.148202419281006,3.130039691925049,3.1441268920898438,3.1666817665100098,3.155752182006836,3.1466073989868164],[3.151881217956543,3.147261619567871,3.140348434448242,3.14616060256958,3.141658306121826,3.158475875854492,3.115004539489746,3.1621785163879395,3.147824764251709,3.135806083679199,3.1504530906677246,3.1463708877563477],[3.144176483154297,3.1487293243408203,3.1481542587280273,3.189835548400879,3.140397548675537,3.1409406661987305,3.142721176147461,3.1480817794799805,3.139286994934082,3.1597557067871094,3.1850337982177734,3.157623291015625],[3.143831729888916,3.145629405975342,3.1364636421203613,3.1386170387268066,3.1423497200012207,3.1450257301330566,3.147088050842285,3.1353464126586914,3.1428093910217285,3.1516709327697754,3.119156837463379,3.1610031127929688],[3.148132801055908,3.140882968902588,3.129678726196289,3.142080307006836,3.0904669761657715,3.1359195709228516,3.1530814170837402,3.1149191856384277,3.1477718353271484,3.145562171936035,3.1419310569763184,3.1451473236083984],[3.146822452545166,3.141538143157959,3.143259048461914,3.143968105316162,3.1584653854370117,3.0634517669677734,3.1539626121520996,3.141728401184082,3.1319336891174316,3.1418991088867188,3.141664981842041,3.142333984375],[3.148865222930908,3.143245220184326,3.097961902618408,3.1236729621887207,3.109002113342285,3.1338133811950684,3.1330156326293945,3.1378536224365234,3.1527347564697266,3.1657238006591797,3.307281494140625,1.9903817176818848],[2.957390785217285,3.1433773040771484,3.0726375579833984,3.1450772285461426,3.1448841094970703,3.181609630584717,3.095686435699463,3.1608662605285645,1.916409969329834,3.101694107055664,3.14052152633667,3.1435561180114746],[-0.9743742346763611,3.1524548530578613,3.128714084625244,3.0390677452087402,3.2075276374816895,3.14493465423584,3.019562244415283,3.562903881072998,3.147404193878174,3.1502838134765625,3.1126980781555176,3.1248931884765625],[3.226987838745117,3.1480278968811035,3.0277504920959473,3.0234131813049316,3.158698081970215,3.1616601943969727,3.149402141571045,3.136061668395996,3.144658088684082,3.1419143676757812,3.456454277038574,3.1126770973205566]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Log Logit Prob difference after Patching\"}},                        {\"responsive\": true}                    ).then(function(){\n",[m
[32m+[m[32m       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"88770aa7-97bc-4613-8eec-b230081fb7ef\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"88770aa7-97bc-4613-8eec-b230081fb7ef\")) {                    Plotly.newPlot(                        \"88770aa7-97bc-4613-8eec-b230081fb7ef\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Log Logit Prob difference after Patching\"}},                        {\"responsive\": true}                    ).then(function(){\n",[m
        "                            \n",[m
[31m-       "var gd = document.getElementById('799c0431-e166-45d5-b3fb-45194eccc4c9');\n",[m
[32m+[m[32m       "var gd = document.getElementById('88770aa7-97bc-4613-8eec-b230081fb7ef');\n",[m
        "var x = new MutationObserver(function (mutations, observer) {{\n",[m
        "        var display = window.getComputedStyle(gd).display;\n",[m
        "        if (!display || display === 'none') {{\n",[m
[36m@@ -3035,7 +1893,7 @@[m
    ],[m
    "source": [[m
     "def patch_last_name(z, source_act, hook):\n",[m
[31m-    "    z[:, 1, :] = source_act[:, 1, :] #we patch at the token of the last name \n",[m
[32m+[m[32m    "    z[:, 1, :] = source_act[:, 1, :] # We patch at the token of the last name \n",[m
     "    return z\n",[m
     "\n",[m
     "config = PatchingConfig(\n",[m
[36m@@ -3056,7 +1914,7 @@[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "##Loading Checkpointed Models\n",[m
[32m+[m[32m    "## Loading Checkpointed Models\n",[m
     "Researchers at the Stanford Center for Research on Foundation Models kindly [created and open sourced 5 training runs of GPT-2 Small and GPT-2 Medium](https://huggingface.co/stanford-crfm), with 600 checkpoints taken during training. These can be loaded in via the same interface as above\n",[m
     "\n",[m
     "These are called `stanford-gpt2-small-A`, (with `small` or `medium` and `A`, `B`, `C`, `D`, `E` as the possible options)\n",[m
[36m@@ -3064,25 +1922,9 @@[m
     "You can see the available checkpoints [here](https://huggingface.co/stanford-crfm/alias-gpt2-small-x21/tree/main) (each checkpoint has a separate Git branch)"[m
    ][m
   },[m
[31m-  {[m
[31m-   "cell_type": "markdown",[m
[31m-   "metadata": {},[m
[31m-   "source": [[m
[31m-    "Model loading:"[m
[31m-   ][m
[31m-  },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[31m-   "metadata": {},[m
[31m-   "outputs": [],[m
[31m-   "source": [[m
[31m-    "checkpointed_model = EasyTransformer('stanford-gpt2-small-A', checkpoint=20000)"[m
[31m-   ][m
[31m-  },[m
[31m-  {[m
[31m-   "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 36,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
[36m@@ -3094,30 +1936,79 @@[m
     }[m
    ],[m
    "source": [[m
[31m-    "from easy_transformer.EasyTransformer import STANFORD_CRFM_CHECKPOINTS\n",[m
[31m-    "print(\"Available checkpoints - it's long, so toggle the flag to print\")\n",[m
[32m+[m[32m    "\n",[m
     "print_checkpoints = False #@param [False, True]\n",[m
     "if print_checkpoints:\n",[m
[31m-    "    print(STANFORD_CRFM_CHECKPOINTS)"[m
[32m+[m[32m    "    print(EasyTransformer.STANFORD_CRFM_CHECKPOINTS)\n",[m
[32m+[m[32m    "else:\n",[m
[32m+[m[32m    "    print(\"Available checkpoints - it's long, so toggle the flag to print\")"[m
    ][m
   },[m
   {[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "###Looking for induction heads\n",[m
[32m+[m[32m    "### Looking for induction heads\n",[m
     "We can use this to analyse whether models contain induction heads during training (by checking whether they can predict repeated sequences of random tokens), and can see something of a phase change early in training"[m
    ][m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": null,[m
[32m+[m[32m   "execution_count": 37,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "data": {[m
[32m+[m[32m      "application/json": {[m
[32m+[m[32m       "ascii": false,[m
[32m+[m[32m       "bar_format": null,[m
[32m+[m[32m       "colour": null,[m
[32m+[m[32m       "elapsed": 0.017441511154174805,[m
[32m+[m[32m       "initial": 0,[m
[32m+[m[32m       "n": 0,[m
[32m+[m[32m       "ncols": null,[m
[32m+[m[32m       "nrows": 6,[m
[32m+[m[32m       "postfix": null,[m
[32m+[m[32m       "prefix": "Downloading config.json",[m
[32m+[m[32m       "rate": null,[m
[32m+[m[32m       "total": 946,[m
[32m+[m[32m       "unit": "B",[m
[32m+[m[32m       "unit_divisor": 1024,[m
[32m+[m[32m       "unit_scale": true[m
[32m+[m[32m      },[m
       "application/vnd.jupyter.widget-view+json": {[m
[31m-       "model_id": "5f75b31d29ac453cada4e195afbae5c6",[m
[32m+[m[32m       "model_id": "8358628154cd4e99a74d2bd8560713db",[m
[32m+[m[32m       "version_major": 2,[m
[32m+[m[32m       "version_minor": 0[m
[32m+[m[32m      },[m
[32m+[m[32m      "text/plain": [[m
[32m+[m[32m       "Downloading config.json:   0%|          | 0.00/946 [00:00<?, ?B/s]"[m
[32m+[m[32m      ][m
[32m+[m[32m     },[m
[32m+[m[32m     "metadata": {},[m
[32m+[m[32m     "output_type": "display_data"[m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "data": {[m
[32m+[m[32m      "application/json": {[m
[32m+[m[32m       "ascii": false,[m
[32m+[m[32m       "bar_format": null,[m
[32m+[m[32m       "colour": null,[m
[32m+[m[32m       "elapsed": 0.024048566818237305,[m
[32m+[m[32m       "initial": 0,[m
[32m+[m[32m       "n": 0,[m
[32m+[m[32m       "ncols": null,[m
[32m+[m[32m       "nrows": 6,[m
[32m+[m[32m       "postfix": null,[m
[32m+[m[32m       "prefix": "Downloading pytorch_model.bin",[m
[32m+[m[32m       "rate": null,[m
[32m+[m[32m       "total": 261498618,[m
[32m+[m[32m       "unit": "B",[m
[32m+[m[32m       "unit_divisor": 1024,[m
[32m+[m[32m       "unit_scale": true[m
[32m+[m[32m      },[m
[32m+[m[32m      "application/vnd.jupyter.widget-view+json": {[m
[32m+[m[32m       "model_id": "9fa1f0bb46374a368ea6a666b3d76bca",[m
        "version_major": 2,[m
        "version_minor": 0[m
       },[m
[36m@@ -3130,8 +2021,56 @@[m
     },[m
     {[m
      "data": {[m
[32m+[m[32m      "application/json": {[m
[32m+[m[32m       "ascii": false,[m
[32m+[m[32m       "bar_format": null,[m
[32m+[m[32m       "colour": null,[m
[32m+[m[32m       "elapsed": 0.02236199378967285,[m
[32m+[m[32m       "initial": 0,[m
[32m+[m[32m       "n": 0,[m
[32m+[m[32m       "ncols": null,[m
[32m+[m[32m       "nrows": 6,[m
[32m+[m[32m       "postfix": null,[m
[32m+[m[32m       "prefix": "Downloading tokenizer_config.json",[m
[32m+[m[32m       "rate": null,[m
[32m+[m[32m       "total": 200,[m
[32m+[m[32m       "unit": "B",[m
[32m+[m[32m       "unit_divisor": 1024,[m
[32m+[m[32m       "unit_scale": true[m
[32m+[m[32m      },[m
[32m+[m[32m      "application/vnd.jupyter.widget-view+json": {[m
[32m+[m[32m       "model_id": "52fb71b8f50749d3932eb898a1410093",[m
[32m+[m[32m       "version_major": 2,[m
[32m+[m[32m       "version_minor": 0[m
[32m+[m[32m      },[m
[32m+[m[32m      "text/plain": [[m
[32m+[m[32m       "Downloading tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"[m
[32m+[m[32m      ][m
[32m+[m[32m     },[m
[32m+[m[32m     "metadata": {},[m
[32m+[m[32m     "output_type": "display_data"[m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "data": {[m
[32m+[m[32m      "application/json": {[m
[32m+[m[32m       "ascii": false,[m
[32m+[m[32m       "bar_format": null,[m
[32m+[m[32m       "colour": null,[m
[32m+[m[32m       "elapsed": 0.021581172943115234,[m
[32m+[m[32m       "initial": 0,[m
[32m+[m[32m       "n": 0,[m
[32m+[m[32m       "ncols": null,[m
[32m+[m[32m       "nrows": 6,[m
[32m+[m[32m       "postfix": null,[m
[32m+[m[32m       "prefix": "Downloading config.json",[m
[32m+[m[32m       "rate": null,[m
[32m+[m[32m       "total": 946,[m
[32m+[m[32m       "unit": "B",[m
[32m+[m[32m       "unit_divisor": 1024,[m
[32m+[m[32m       "unit_scale": true[m
[32m+[m[32m      },[m
       "application/vnd.jupyter.widget-view+json": {[m
[31m-       "model_id": "c353aba7b5744f08893fc1b7c54783ce",[m
[32m+[m[32m       "model_id": "f491d0aaea1946c891d66d52b8fd4112",[m
        "version_major": 2,[m
        "version_minor": 0[m
       },[m
[36m@@ -3144,13 +2083,30 @@[m
     },[m
     {[m
      "data": {[m
[32m+[m[32m      "application/json": {[m
[32m+[m[32m       "ascii": false,[m
[32m+[m[32m       "bar_format": null,[m
[32m+[m[32m       "colour": null,[m
[32m+[m[32m       "elapsed": 0.024343013763427734,[m
[32m+[m[32m       "initial": 0,[m
[32m+[m[32m       "n": 0,[m
[32m+[m[32m       "ncols": null,[m
[32m+[m[32m       "nrows": 6,[m
[32m+[m[32m       "postfix": null,[m
[32m+[m[32m       "prefix": "Downloading vocab.json",[m
[32m+[m[32m       "rate": null,[m
[32m+[m[32m       "total": 798156,[m
[32m+[m[32m       "unit": "B",[m
[32m+[m[32m       "unit_divisor": 1024,[m
[32m+[m[32m       "unit_scale": true[m
[32m+[m[32m      },[m
       "application/vnd.jupyter.widget-view+json": {[m
[31m-       "model_id": "34746c8ab06044758fb5343e7a8bc612",[m
[32m+[m[32m       "model_id": "895261938f394fd0a8d2e31ffeddd74b",[m
        "version_major": 2,[m
        "version_minor": 0[m
       },[m
       "text/plain": [[m
[31m-       "Downloading pytorch_model.bin:   0%|          | 0.00/249M [00:00<?, ?B/s]"[m
[32m+[m[32m       "Downloading vocab.json:   0%|          | 0.00/779k [00:00<?, ?B/s]"[m
       ][m
      },[m
      "metadata": {},[m
[36m@@ -3158,38 +2114,123 @@[m
     },[m
     {[m
      "data": {[m
[31m-      "text/html": [[m
[31m-       "<html>\n",[m
[31m-       "<head><meta charset=\"utf-8\" /></head>\n",[m
[31m-       "<body>\n",[m
[31m-       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",[m
[31m-       "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"c13cc773-ea8a-42e9-8514-71d697e44896\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c13cc773-ea8a-42e9-8514-71d697e44896\")) {                    Plotly.newPlot(                        \"c13cc773-ea8a-42e9-8514-71d697e44896\",                        [{\"hovertemplate\":\"variable=1000<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"1000\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"1000\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198],\"xaxis\":\"x\",\"y\":[-10.169904708862305,-16.1785831451416,-14.099587440490723,-13.20871639251709,-14.628175735473633,-14.246983528137207,-9.18644905090332,-15.06261920928955,-17.163822174072266,-8.058032989501953,-15.221464157104492,-12.245715141296387,-14.352300643920898,-10.539056777954102,-14.511606216430664,-8.296613693237305,-16.00469207763672,-14.492189407348633,-14.540521621704102,-18.111289978027344,-12.624839782714844,-14.809344291687012,-16.339262008666992,-14.948577880859375,-12.754712104797363,-14.404067039489746,-12.170370101928711,-14.484060287475586,-10.179152488708496,-14.773835182189941,-12.300344467163086,-10.523398399353027,-13.396316528320312,-10.623184204101562,-12.729358673095703,-10.35715103149414,-14.088462829589844,-10.757392883300781,-18.27972412109375,-16.803150177001953,-13.367546081542969,-16.161758422851562,-7.493396759033203,-15.991850852966309,-17.046348571777344,-15.379288673400879,-15.09642505645752,-15.46117115020752,-13.96975040435791,-15.727477073669434,-16.744237899780273,-23.3209171295166,-13.514623641967773,-12.219088554382324,-10.440898895263672,-11.065746307373047,-13.092704772949219,-15.27070140838623,-11.03502082824707,-12.52275276184082,-16.570125579833984,-15.385499954223633,-11.346116065979004,-16.49335289001465,-12.59421443939209,-10.577981948852539,-12.907443046569824,-13.337678909301758,-9.914481163024902,-14.713581085205078,-14.139934539794922,-7.4013214111328125,-12.283175468444824,-14.20503044128418,-12.339071273803711,-12.232239723205566,-12.69049072265625,-12.721364974975586,-8.611448287963867,-11.78878402709961,-14.31511402130127,-14.63360595703125,-11.829376220703125,-11.713081359863281,-10.560722351074219,-11.920974731445312,-8.49022102355957,-13.082365036010742,-14.460464477539062,-10.023245811462402,-10.177715301513672,-14.305697441101074,-12.150595664978027,-15.692195892333984,-8.79820728302002,-15.64896297454834,-13.892986297607422,-12.269469261169434,-10.841519355773926,-13.241598129272461,-9.736634254455566,-15.073979377746582,-10.820175170898438,-15.87829303741455,-4.9490203857421875,-12.229179382324219,-9.042577743530273,-4.94785213470459,-12.263386726379395,-4.897083759307861,-11.380074501037598,-7.5622758865356445,-8.748558044433594,-5.91819953918457,-11.783220291137695,-5.929254531860352,-14.533390045166016,-14.21351432800293,-13.989625930786133,-14.157017707824707,-11.019740104675293,-8.112157821655273,-11.740650177001953,-13.027027130126953,-12.258605003356934,-14.604574203491211,-8.39748764038086,-11.160348892211914,-7.918515205383301,-14.03647232055664,-4.68821907043457,-2.7120954990386963,-7.881185054779053,-3.3152830600738525,-11.386970520019531,-8.77828598022461,-9.326740264892578,-10.717475891113281,-16.44165802001953,-5.632707118988037,-11.123697280883789,-17.38658905029297,-3.9513368606567383,-9.075191497802734,-6.937821388244629,-8.143844604492188,-13.971152305603027,-16.11068344116211,-10.687265396118164,-13.901954650878906,-16.535491943359375,-14.067197799682617,-7.9508957862854,-12.260391235351562,-8.573264122009277,-10.644271850585938,-10.92931079864502,-13.093692779541016,-11.243764877319336,-12.796541213989258,-11.89771842956543,-7.146137714385986,-12.656353950500488,-10.497384071350098,-10.711078643798828,-12.56532096862793,-10.081533432006836,-14.056264877319336,-10.625682830810547,-12.78189754486084,-13.806941032409668,-7.3354339599609375,-10.890257835388184,-16.651012420654297,-11.281824111938477,-11.45136833190918,-11.216583251953125,-12.478287696838379,-9.761029243469238,-12.02978515625,-2.0264012813568115,-15.329422950744629,-9.932892799377441,-14.624382972717285,-6.908990383148193,-3.4736223220825195,-10.446271896362305,-12.089855194091797,-13.946113586425781,-10.919170379638672,-9.422340393066406,-13.026346206665039,-3.0118610858917236,-14.200709342956543,-9.73527717590332,-15.585583686828613,-14.78373908996582,-11.6781005859375,-9.13481330871582],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=2500<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"2500\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"2500\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198],\"xaxis\":\"x\",\"y\":[-9.112822532653809,-16.754610061645508,-17.516925811767578,-11.075376510620117,-14.772049903869629,-11.79538345336914,-10.034048080444336,-13.282712936401367,-20.441564559936523,-10.61941146850586,-14.849550247192383,-10.168761253356934,-14.680848121643066,-10.2703218460083,-14.282574653625488,-11.923603057861328,-16.81844139099121,-13.324760437011719,-12.983424186706543,-18.7503604888916,-7.5751800537109375,-15.980886459350586,-16.61226463317871,-16.64805793762207,-13.949365615844727,-14.607405662536621,-10.409916877746582,-13.178434371948242,-9.76733112335205,-14.387273788452148,-12.888025283813477,-11.242671012878418,-12.8472900390625,-12.773101806640625,-16.496078491210938,-6.366546630859375,-10.426459312438965,-13.731095314025879,-16.469345092773438,-14.641003608703613,-17.66354751586914,-17.664308547973633,-10.803947448730469,-14.132743835449219,-15.204919815063477,-16.644731521606445,-15.68589973449707,-13.553889274597168,-10.779217720031738,-13.298555374145508,-17.24054718017578,-20.954179763793945,-10.705493927001953,-8.161603927612305,-9.545528411865234,-9.366584777832031,-12.283487319946289,-17.730022430419922,-14.237386703491211,-15.485630989074707,-15.970723152160645,-17.958477020263672,-12.445796966552734,-15.617264747619629,-11.887188911437988,-12.999537467956543,-11.407029151916504,-10.433537483215332,-11.924795150756836,-16.285898208618164,-11.143503189086914,-7.647704601287842,-13.324067115783691,-14.625905990600586,-13.512029647827148,-12.369979858398438,-12.933974266052246,-16.444421768188477,-13.267087936401367,-15.51398754119873,-13.641966819763184,-15.292753219604492,-10.110518455505371,-10.196027755737305,-13.441812515258789,-12.652013778686523,-16.19346046447754,-14.039206504821777,-15.687639236450195,-16.401994705200195,-9.56981086730957,-12.306307792663574,-11.123722076416016,-15.672142028808594,-12.513130187988281,-18.3414249420166,-14.24693489074707,-15.175910949707031,-14.1544771194458,-15.70459270477295,-8.21438980102539,-14.921409606933594,-1.4503167867660522,-13.98151969909668,-0.2844240963459015,-3.7783150672912598,-0.63040691614151,-0.0403556153178215,-2.266632318496704,-0.010520121082663536,-14.777968406677246,-0.02840513549745083,-0.27471068501472473,-12.826835632324219,-0.14817865192890167,-0.6122336983680725,-2.622648239135742,-12.768826484680176,-13.318124771118164,-16.323482513427734,-14.969236373901367,-3.8344314098358154,-0.6578996777534485,-12.843513488769531,-15.62119197845459,-4.314388275146484,-0.26702451705932617,-9.26902961730957,-13.834228515625,-18.984020233154297,-10.509866714477539,-6.253365516662598,-0.6239904761314392,-0.0103273531422019,-0.0775076374411583,-0.017333919182419777,-9.639301300048828,-3.9470863342285156,-16.842357635498047,-0.48500487208366394,-0.007667870260775089,-14.686593055725098,-8.435789108276367,-0.03889451548457146,-0.15150468051433563,-13.769824981689453,-13.43677043914795,-1.8440922498703003,-3.687166690826416,-2.0477826595306396,-1.5154281854629517,-3.421665668487549,-11.89670467376709,-16.16617202758789,-0.1398765593767166,-0.011846075765788555,-14.1954345703125,-18.133106231689453,-17.439943313598633,-0.025321118533611298,-0.9358438849449158,-4.813921928405762,-9.054131507873535,-1.9164297580718994,-0.6372593641281128,-13.995111465454102,-12.596365928649902,-7.600948333740234,-0.009326815605163574,-12.07320499420166,-1.3755719661712646,-7.782767295837402,-18.28888511657715,-16.993234634399414,-1.1202791929244995,-0.703937292098999,-0.06549577414989471,-18.787778854370117,-16.416166305541992,-0.1405452936887741,-0.1294817328453064,-0.1721988469362259,-0.03835534676909447,-3.8327314853668213,-3.4784085750579834,-7.19723653793335,-15.280085563659668,-7.8608198165893555,-12.348773002624512,-14.913644790649414,-0.18854448199272156,-0.051465678960084915,-0.07223775237798691,-0.37428370118141174,-0.0034014484845101833,-0.8731757402420044,-0.4385683238506317,-0.633017897605896,-4.44157600402832],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=5000<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"5000\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"5000\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198],\"xaxis\":\"x\",\"y\":[-11.379352569580078,-17.09227180480957,-18.687030792236328,-13.003181457519531,-13.498170852661133,-13.406892776489258,-12.712095260620117,-14.460514068603516,-18.70784568786621,-10.927907943725586,-14.687738418579102,-14.534134864807129,-13.544949531555176,-8.009880065917969,-15.377270698547363,-12.520947456359863,-16.346210479736328,-10.755487442016602,-12.670844078063965,-17.757463455200195,-11.169025421142578,-14.30789852142334,-17.7606258392334,-18.790481567382812,-11.981375694274902,-14.759077072143555,-11.41507339477539,-13.814266204833984,-9.136680603027344,-11.271454811096191,-14.268754959106445,-11.468545913696289,-15.02254867553711,-13.681295394897461,-17.803770065307617,-6.786120414733887,-12.362807273864746,-12.411553382873535,-13.036033630371094,-14.141735076904297,-15.554170608520508,-15.363490104675293,-9.736677169799805,-14.687976837158203,-16.7592830657959,-14.754355430603027,-14.330594062805176,-10.008561134338379,-13.485123634338379,-18.299501419067383,-15.112451553344727,-18.418615341186523,-11.53049087524414,-12.945743560791016,-11.004692077636719,-9.781400680541992,-9.149373054504395,-14.346790313720703,-11.666770935058594,-13.954269409179688,-14.112874984741211,-14.943933486938477,-12.464090347290039,-15.362627983093262,-10.801703453063965,-14.362319946289062,-12.785075187683105,-11.326408386230469,-10.47839069366455,-16.739824295043945,-15.512471199035645,-8.924942016601562,-14.106633186340332,-14.573901176452637,-13.686656951904297,-11.234914779663086,-12.399641036987305,-12.298670768737793,-10.757689476013184,-15.499969482421875,-15.68054485321045,-15.481271743774414,-13.994462013244629,-14.174551010131836,-13.003730773925781,-15.509134292602539,-13.783599853515625,-15.769722938537598,-13.923798561096191,-13.541372299194336,-8.89613151550293,-11.899417877197266,-12.830875396728516,-11.59302806854248,-10.39329719543457,-12.626519203186035,-14.006410598754883,-12.882919311523438,-16.198123931884766,-15.996261596679688,-0.05285494402050972,-4.60919189453125,-0.1717948466539383,-4.893774032592773,-0.14565090835094452,-0.3803669214248657,-3.332987070083618,-0.03509395197033882,-3.197396993637085,-7.983342170715332,-0.2833082377910614,-1.3621450662612915,-14.666313171386719,-0.3626076579093933,-5.035462379455566,-16.498008728027344,-16.922470092773438,-1.2257404327392578,-0.3686278760433197,-0.7909314036369324,-0.117426298558712,-11.140294075012207,-0.4212837219238281,-0.8771460652351379,-0.8691726326942444,-9.572237968444824,-0.8881467580795288,-0.011413871310651302,-0.24067768454551697,-0.17679491639137268,-0.576930820941925,-5.662332534790039,-0.2501269280910492,-0.01126418262720108,-0.5591215491294861,-0.29591965675354004,-0.6177979111671448,-4.003918170928955,-5.597251892089844,-11.879411697387695,-0.010950824245810509,-1.5691665410995483,-0.7666605114936829,-13.379826545715332,-3.198817014694214,-0.399340957403183,-2.9370927810668945,-2.851276397705078,-0.4575403928756714,-3.5416204929351807,-0.008189549669623375,-3.6510229110717773,-0.04608233645558357,-0.044311344623565674,-0.35431650280952454,-0.20733997225761414,-0.043083954602479935,-0.3117120862007141,-0.013403546065092087,-1.3000613451004028,-0.19182121753692627,-18.326658248901367,-0.3023242950439453,-0.03923718258738518,-3.5663349628448486,-3.741107702255249,-2.8981916904449463,-1.6890279054641724,-0.1203811764717102,-22.208885192871094,-10.737905502319336,-0.02276885323226452,-14.22320556640625,-0.4617537260055542,-12.542912483215332,-0.2761439085006714,-16.689821243286133,-0.22895775735378265,-5.586780071258545,-0.9031574726104736,-0.95074063539505,-0.5981600880622864,-0.0372249111533165,-0.4058525562286377,-0.026585351675748825,-0.015513459220528603,-0.36629143357276917,-0.10123465955257416,-8.189126014709473,-0.07235729694366455,-0.08903150260448456,-0.06384509056806564,-0.013811847195029259,-13.749981880187988,-0.818477213382721,-0.022852515801787376,-1.6809457540512085,-0.6321879625320435,-0.004707207437604666],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",[m
[31m-       "                            \n",[m
[31m-       "var gd = document.getElementById('c13cc773-ea8a-42e9-8514-71d697e44896');\n",[m
[31m-       "var x = new MutationObserver(function (mutations, observer) {{\n",[m
[31m-       "        var display = window.getComputedStyle(gd).display;\n",[m
[31m-       "        if (!display || display === 'none') {{\n",[m
[31m-       "            console.log([gd, 'removed!']);\n",[m
[31m-       "            Plotly.purge(gd);\n",[m
[31m-       "            observer.disconnect();\n",[m
[31m-       "        }}\n",[m
[31m-       "}});\n",[m
[31m-       "\n",[m
[31m-       "// Listen for the removal of the full notebook cells\n",[m
[31m-       "var notebookContainer = gd.closest('#notebook-container');\n",[m
[31m-       "if (notebookContainer) {{\n",[m
[31m-       "    x.observe(notebookContainer, {childList: true});\n",[m
[31m-       "}}\n",[m
[31m-       "\n",[m
[31m-       "// Listen for the clearing of the current output cell\n",[m
[31m-       "var outputEl = gd.closest('.output');\n",[m
[31m-       "if (outputEl) {{\n",[m
[31m-       "    x.observe(outputEl, {childList: true});\n",[m
[31m-       "}}\n",[m
[31m-       "\n",[m
[31m-       "                        })                };                            </script>        </div>\n",[m
[31m-       "</body>\n",[m
[31m-       "</html>"[m
[32m+[m[32m      "application/json": {[m
[32m+[m[32m       "ascii": false,[m
[32m+[m[32m       "bar_format": null,[m
[32m+[m[32m       "colour": null,[m
[32m+[m[32m       "elapsed": 0.0165708065032959,[m
[32m+[m[32m       "initial": 0,[m
[32m+[m[32m       "n": 0,[m
[32m+[m[32m       "ncols": null,[m
[32m+[m[32m       "nrows": 6,[m
[32m+[m[32m       "postfix": null,[m
[32m+[m[32m       "prefix": "Downloading merges.txt",[m
[32m+[m[32m       "rate": null,[m
[32m+[m[32m       "total": 456356,[m
[32m+[m[32m       "unit": "B",[m
[32m+[m[32m       "unit_divisor": 1024,[m
[32m+[m[32m       "unit_scale": true[m
[32m+[m[32m      },[m
[32m+[m[32m      "application/vnd.jupyter.widget-view+json": {[m
[32m+[m[32m       "model_id": "36bb5113c2ee42c3ba2d8f4d5452bc87",[m
[32m+[m[32m       "version_major": 2,[m
[32m+[m[32m       "version_minor": 0[m
[32m+[m[32m      },[m
[32m+[m[32m      "text/plain": [[m
[32m+[m[32m       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"[m
[32m+[m[32m      ][m
[32m+[m[32m     },[m
[32m+[m[32m     "metadata": {},[m
[32m+[m[32m     "output_type": "display_data"[m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "data": {[m
[32m+[m[32m      "application/json": {[m
[32m+[m[32m       "ascii": false,[m
[32m+[m[32m       "bar_format": null,[m
[32m+[m[32m       "colour": null,[m
[32m+[m[32m       "elapsed": 0.022411584854125977,[m
[32m+[m[32m       "initial": 0,[m
[32m+[m[32m       "n": 0,[m
[32m+[m[32m       "ncols": null,[m
[32m+[m[32m       "nrows": 6,[m
[32m+[m[32m       "postfix": null,[m
[32m+[m[32m       "prefix": "Downloading special_tokens_map.json",[m
[32m+[m[32m       "rate": null,[m
[32m+[m[32m       "total": 90,[m
[32m+[m[32m       "unit": "B",[m
[32m+[m[32m       "unit_divisor": 1024,[m
[32m+[m[32m       "unit_scale": true[m
[32m+[m[32m      },[m
[32m+[m[32m      "application/vnd.jupyter.widget-view+json": {[m
[32m+[m[32m       "model_id": "a874912e806240d19e918cc73dbcea1e",[m
[32m+[m[32m       "version_major": 2,[m
[32m+[m[32m       "version_minor": 0[m
[32m+[m[32m      },[m
[32m+[m[32m      "text/plain": [[m
[32m+[m[32m       "Downloading special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"[m
[32m+[m[32m      ][m
[32m+[m[32m     },[m
[32m+[m[32m     "metadata": {},[m
[32m+[m[32m     "output_type": "display_data"[m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "data": {[m
[32m+[m[32m      "application/json": {[m
[32m+[m[32m       "ascii": false,[m
[32m+[m[32m       "bar_format": null,[m
[32m+[m[32m       "colour": null,[m
[32m+[m[32m       "elapsed": 0.01774907112121582,[m
[32m+[m[32m       "initial": 0,[m
[32m+[m[32m       "n": 0,[m
[32m+[m[32m       "ncols": null,[m
[32m+[m[32m       "nrows": 6,[m
[32m+[m[32m       "postfix": null,[m
[32m+[m[32m       "prefix": "Downloading config.json",[m
[32m+[m[32m       "rate": null,[m
[32m+[m[32m       "total": 946,[m
[32m+[m[32m       "unit": "B",[m
[32m+[m[32m       "unit_divisor": 1024,[m
[32m+[m[32m       "unit_scale": true[m
[32m+[m[32m      },[m
[32m+[m[32m      "application/vnd.jupyter.widget-view+json": {[m
[32m+[m[32m       "model_id": "0372316da77c47219b611523cb6d0d9a",[m
[32m+[m[32m       "version_major": 2,[m
[32m+[m[32m       "version_minor": 0[m
[32m+[m[32m      },[m
[32m+[m[32m      "text/plain": [[m
[32m+[m[32m       "Downloading config.json:   0%|          | 0.00/946 [00:00<?, ?B/s]"[m
[32m+[m[32m      ][m
[32m+[m[32m     },[m
[32m+[m[32m     "metadata": {},[m
[32m+[m[32m     "output_type": "display_data"[m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "data": {[m
[32m+[m[32m      "application/json": {[m
[32m+[m[32m       "ascii": false,[m
[32m+[m[32m       "bar_format": null,[m
[32m+[m[32m       "colour": null,[m
[32m+[m[32m       "elapsed": 0.023901939392089844,[m
[32m+[m[32m       "initial": 0,[m
[32m+[m[32m       "n": 0,[m
[32m+[m[32m       "ncols": null,[m
[32m+[m[32m       "nrows": 6,[m
[32m+[m[32m       "postfix": null,[m
[32m+[m[32m       "prefix": "Downloading pytorch_model.bin",[m
[32m+[m[32m       "rate": null,[m
[32m+[m[32m       "total": 261498618,[m
[32m+[m[32m       "unit": "B",[m
[32m+[m[32m       "unit_divisor": 1024,[m
[32m+[m[32m       "unit_scale": true[m
[32m+[m[32m      },[m
[32m+[m[32m      "application/vnd.jupyter.widget-view+json": {[m
[32m+[m[32m       "model_id": "829a37871e8b45b891a387054693b25d",[m
[32m+[m[32m       "version_major": 2,[m
[32m+[m[32m       "version_minor": 0[m
[32m+[m[32m      },[m
[32m+[m[32m      "text/plain": [[m
[32m+[m[32m       "Downloading pytorch_model.bin:   0%|          | 0.00/249M [00:00<?, ?B/s]"[m
       ][m
      },[m
      "metadata": {},[m
[36m@@ -3200,10 +2241,10 @@[m
     "\n",[m
     "plps = {}\n",[m
     "tokens = torch.randint(1000, 20000, (1, 100))\n",[m
[31m-    "tokens = torch.concat([tokens, tokens], axis=1)\n",[m
[32m+[m[32m    "tokens = torch.concat([tokens, tokens], axis=1).to(device)\n",[m
     "for check in [1000, 2500, 5000]:\n",[m
[31m-    "    model = EasyTransformer('stanford-gpt2-small-E', checkpoint=check)\n",[m
[31m-    "    logits = model(tokens)\n",[m
[32m+[m[32m    "    checkpointed_model = EasyTransformer.from_pretrained('stanford-gpt2-small-E', checkpoint=check)\n",[m
[32m+[m[32m    "    logits = checkpointed_model(tokens)\n",[m
     "    log_probs = F.log_softmax(logits, dim=-1)\n",[m
     "    plp = torch.gather(log_probs[:, :-1], -1, tokens[:, 1:, None])[0, :, 0]\n",[m
     "    plps[check] = plp.detach().cpu().numpy()\n",[m
[36m@@ -3214,7 +2255,7 @@[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "##Visualisations\n",[m
[32m+[m[32m    "## Visualisations\n",[m
     "\n",[m
     "Visualisations are an extremely important tool for doing good interpretability work - fundamentally, neural networks are complex, high-dimensional objects and we want to understand how to decompose them and understand them. \n",[m
     "\n",[m
[36m@@ -3227,7 +2268,7 @@[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "###Installation"[m
[32m+[m[32m    "### Installation"[m
    ][m
   },[m
   {[m
[36m@@ -3239,82 +2280,22 @@[m
      "name": "stdout",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "\n",[m
[31m-      "## Installing the NodeSource Node.js 16.x repo...\n",[m
[31m-      "\n",[m
[31m-      "\n",[m
[31m-      "## Populating apt-get cache...\n",[m
[31m-      "\n",[m
[31m-      "+ apt-get update\n",[m
[31m-      "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",[m
[31m-      "\r            \rHit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",[m
[31m-      "\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to cloud.r-pro\r0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\r                                                                               \rHit:3 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",[m
[31m-      "\r0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\r                                                                               \rHit:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",[m
[31m-      "\r0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\r                                                                               \rHit:5 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",[m
[31m-      "\r0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\r                                                                               \rHit:6 https://deb.nodesource.com/node_16.x bionic InRelease\n",[m
[31m-      "\r0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\r                                                                               \rHit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",[m
[31m-      "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",[m
[31m-      "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",[m
[31m-      "Hit:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",[m
[31m-      "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",[m
[31m-      "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",[m
[31m-      "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",[m
[31m-      "Reading package lists... Done\n",[m
[31m-      "\n",[m
[31m-      "## Confirming \"bionic\" is supported...\n",[m
[31m-      "\n",[m
[31m-      "+ curl -sLf -o /dev/null 'https://deb.nodesource.com/node_16.x/dists/bionic/Release'\n",[m
[31m-      "\n",[m
[31m-      "## Adding the NodeSource signing key to your keyring...\n",[m
[31m-      "\n",[m
[31m-      "+ curl -s https://deb.nodesource.com/gpgkey/nodesource.gpg.key | gpg --dearmor | tee /usr/share/keyrings/nodesource.gpg >/dev/null\n",[m
[31m-      "\n",[m
[31m-      "## Creating apt sources list file for the NodeSource Node.js 16.x repo...\n",[m
[31m-      "\n",[m
[31m-      "+ echo 'deb [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_16.x bionic main' > /etc/apt/sources.list.d/nodesource.list\n",[m
[31m-      "+ echo 'deb-src [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_16.x bionic main' >> /etc/apt/sources.list.d/nodesource.list\n",[m
[31m-      "\n",[m
[31m-      "## Running `apt-get update` for you...\n",[m
[31m-      "\n",[m
[31m-      "+ apt-get update\n",[m
[31m-      "Hit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",[m
[31m-      "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",[m
[31m-      "Hit:3 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",[m
[31m-      "Hit:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",[m
[31m-      "Hit:5 https://deb.nodesource.com/node_16.x bionic InRelease\n",[m
[31m-      "Hit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",[m
[31m-      "Hit:7 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",[m
[31m-      "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",[m
[31m-      "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",[m
[31m-      "Ign:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",[m
[31m-      "Hit:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",[m
[31m-      "Hit:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",[m
[31m-      "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",[m
[31m-      "Reading package lists... Done\n",[m
[31m-      "\n",[m
[31m-      "## Run `\u001b[1msudo apt-get install -y nodejs\u001b[m` to install Node.js 16.x and npm\n",[m
[31m-      "## You may also need development tools to build native addons:\n",[m
[31m-      "     sudo apt-get install gcc g++ make\n",[m
[31m-      "## To install the Yarn package manager, run:\n",[m
[31m-      "     curl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | gpg --dearmor | sudo tee /usr/share/keyrings/yarnkey.gpg >/dev/null\n",[m
[31m-      "     echo \"deb [signed-by=/usr/share/keyrings/yarnkey.gpg] https://dl.yarnpkg.com/debian stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list\n",[m
[31m-      "     sudo apt-get update && sudo apt-get install yarn\n",[m
[31m-      "\n",[m
[31m-      "\n",[m
[31m-      "Reading package lists... Done\n",[m
[31m-      "Building dependency tree       \n",[m
[31m-      "Reading state information... Done\n",[m
[31m-      "nodejs is already the newest version (16.17.0-1nodesource1).\n",[m
[31m-      "The following package was automatically installed and is no longer required:\n",[m
[31m-      "  libnvidia-common-460\n",[m
[31m-      "Use 'sudo apt autoremove' to remove it.\n",[m
[31m-      "0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\n",[m
[31m-      "Cloning into 'PySvelte'...\n",[m
[31m-      "remote: Enumerating objects: 87, done.\u001b[K\n",[m
[31m-      "remote: Counting objects: 100% (87/87), done.\u001b[K\n",[m
[31m-      "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",[m
[31m-      "remote: Total 87 (delta 37), reused 75 (delta 26), pack-reused 0\u001b[K\n",[m
[31m-      "Unpacking objects: 100% (87/87), done.\n"[m
[32m+[m[32m      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",[m
[32m+[m[32m      "To disable this warning, you can either:\n",[m
[32m+[m[32m      "\t- Avoid using `tokenizers` before the fork if possible\n",[m
[32m+[m[32m      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",[m
[32m+[m[32m      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",[m
[32m+[m[32m      "To disable this warning, you can either:\n",[m
[32m+[m[32m      "\t- Avoid using `tokenizers` before the fork if possible\n",[m
[32m+[m[32m      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",[m
[32m+[m[32m      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",[m
[32m+[m[32m      "To disable this warning, you can either:\n",[m
[32m+[m[32m      "\t- Avoid using `tokenizers` before the fork if possible\n",[m
[32m+[m[32m      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",[m
[32m+[m[32m      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",[m
[32m+[m[32m      "To disable this warning, you can either:\n",[m
[32m+[m[32m      "\t- Avoid using `tokenizers` before the fork if possible\n",[m
[32m+[m[32m      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"[m
      ][m
     }[m
    ],[m
[36m@@ -3351,8 +2332,34 @@[m
      "output_type": "stream",[m
      "text": [[m
       "pysvelte components appear to be unbuilt or stale\n",[m
[31m-      "Running npm install...\n",[m
[31m-      "Building pysvelte components with webpack...\n"[m
[32m+[m[32m      "Building pysvelte components with webpack...\n",[m
[32m+[m[32m      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",[m
[32m+[m[32m      "To disable this warning, you can either:\n",[m
[32m+[m[32m      "\t- Avoid using `tokenizers` before the fork if possible\n",[m
[32m+[m[32m      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",[m
[32m+[m[32m      "entry: {\"loader\":\"./src/loader.js\",\"Hello\":\"./src/Hello.svelte\"}\n",[m
[32m+[m[32m      "asset loader.js 44.1 KiB [emitted] [minimized] (name: loader) 1 related asset\n",[m
[32m+[m[32m      "asset Hello.js 6.82 KiB [emitted] [minimized] (name: Hello)\n",[m
[32m+[m[32m      "runtime modules 1.03 KiB 5 modules\n",[m
[32m+[m[32m      "orphan modules 53.6 KiB [orphan] 1 module\n",[m
[32m+[m[32m      "modules by path ./node_modules/ 116 KiB\n",[m
[32m+[m[32m      "  modules by path ./node_modules/pako/lib/ 102 KiB\n",[m
[32m+[m[32m      "    modules by path ./node_modules/pako/lib/zlib/*.js 84.7 KiB 9 modules\n",[m
[32m+[m[32m      "    modules by path ./node_modules/pako/lib/utils/*.js 5.62 KiB 2 modules\n",[m
[32m+[m[32m      "    ./node_modules/pako/lib/inflate.js 11.6 KiB [built] [code generated]\n",[m
[32m+[m[32m      "  ./node_modules/numpy-parser/dist/main.js 3.63 KiB [built] [code generated]\n",[m
[32m+[m[32m      "  ./node_modules/ndarray/ndarray.js 9.62 KiB [built] [code generated]\n",[m
[32m+[m[32m      "  ./node_modules/iota-array/iota.js 150 bytes [built] [code generated]\n",[m
[32m+[m[32m      "  ./node_modules/is-buffer/index.js 698 bytes [built] [code generated]\n",[m
[32m+[m[32m      "modules by path ./src/ 57.3 KiB\n",[m
[32m+[m[32m      "  ./src/loader.js 2.73 KiB [built] [code generated]\n",[m
[32m+[m[32m      "  ./src/Hello.svelte + 1 modules 54.6 KiB [built] [code generated]\n",[m
[32m+[m[32m      "\n",[m
[32m+[m[32m      "WARNING in configuration\n",[m
[32m+[m[32m      "The 'mode' option has not been set, webpack will fallback to 'production' for this value. Set 'mode' option to 'development' or 'production' to enable defaults for each environment.\n",[m
[32m+[m[32m      "You can also set it to 'none' to disable any default behavior. Learn more: https://webpack.js.org/configuration/mode/\n",[m
[32m+[m[32m      "\n",[m
[32m+[m[32m      "webpack 5.16.0 compiled with 1 warning in 1014 ms\n"[m
      ][m
     },[m
     {[m
[36m@@ -3363,7 +2370,7 @@[m
        "var loader;loader=(()=>{var __webpack_modules__={907:module=>{\"use strict\";module.exports=function iota(n){for(var result=new Array(n),i=0;i<n;++i)result[i]=i;return result}},738:module=>{function isBuffer(obj){return!!obj.constructor&&\"function\"==typeof obj.constructor.isBuffer&&obj.constructor.isBuffer(obj)}module.exports=function(obj){return null!=obj&&(isBuffer(obj)||function isSlowBuffer(obj){return\"function\"==typeof obj.readFloatLE&&\"function\"==typeof obj.slice&&isBuffer(obj.slice(0,0))}(obj)||!!obj._isBuffer)}},861:(module,__unused_webpack_exports,__webpack_require__)=>{var iota=__webpack_require__(907),isBuffer=__webpack_require__(738),hasTypedArrays=\"undefined\"!=typeof Float64Array;function compare1st(a,b){return a[0]-b[0]}function order(){var i,stride=this.stride,terms=new Array(stride.length);for(i=0;i<terms.length;++i)terms[i]=[Math.abs(stride[i]),i];terms.sort(compare1st);var result=new Array(terms.length);for(i=0;i<result.length;++i)result[i]=terms[i][1];return result}function compileConstructor(dtype,dimension){var className=[\"View\",dimension,\"d\",dtype].join(\"\");dimension<0&&(className=\"View_Nil\"+dtype);var useGetters=\"generic\"===dtype;if(-1===dimension){var code=\"function \"+className+\"(a){this.data=a;};var proto=\"+className+\".prototype;proto.dtype='\"+dtype+\"';proto.index=function(){return -1};proto.size=0;proto.dimension=-1;proto.shape=proto.stride=proto.order=[];proto.lo=proto.hi=proto.transpose=proto.step=function(){return new \"+className+\"(this.data);};proto.get=proto.set=function(){};proto.pick=function(){return null};return function construct_\"+className+\"(a){return new \"+className+\"(a);}\";return new Function(code)()}if(0===dimension){code=\"function \"+className+\"(a,d) {this.data = a;this.offset = d};var proto=\"+className+\".prototype;proto.dtype='\"+dtype+\"';proto.index=function(){return this.offset};proto.dimension=0;proto.size=1;proto.shape=proto.stride=proto.order=[];proto.lo=proto.hi=proto.transpose=proto.step=function \"+className+\"_copy() {return new \"+className+\"(this.data,this.offset)};proto.pick=function \"+className+\"_pick(){return TrivialArray(this.data);};proto.valueOf=proto.get=function \"+className+\"_get(){return \"+(useGetters?\"this.data.get(this.offset)\":\"this.data[this.offset]\")+\"};proto.set=function \"+className+\"_set(v){return \"+(useGetters?\"this.data.set(this.offset,v)\":\"this.data[this.offset]=v\")+\"};return function construct_\"+className+\"(a,b,c,d){return new \"+className+\"(a,d)}\";return new Function(\"TrivialArray\",code)(CACHED_CONSTRUCTORS[dtype][0])}code=[\"'use strict'\"];var indices=iota(dimension),args=indices.map((function(i){return\"i\"+i})),index_str=\"this.offset+\"+indices.map((function(i){return\"this.stride[\"+i+\"]*i\"+i})).join(\"+\"),shapeArg=indices.map((function(i){return\"b\"+i})).join(\",\"),strideArg=indices.map((function(i){return\"c\"+i})).join(\",\");code.push(\"function \"+className+\"(a,\"+shapeArg+\",\"+strideArg+\",d){this.data=a\",\"this.shape=[\"+shapeArg+\"]\",\"this.stride=[\"+strideArg+\"]\",\"this.offset=d|0}\",\"var proto=\"+className+\".prototype\",\"proto.dtype='\"+dtype+\"'\",\"proto.dimension=\"+dimension),code.push(\"Object.defineProperty(proto,'size',{get:function \"+className+\"_size(){return \"+indices.map((function(i){return\"this.shape[\"+i+\"]\"})).join(\"*\"),\"}})\"),1===dimension?code.push(\"proto.order=[0]\"):(code.push(\"Object.defineProperty(proto,'order',{get:\"),dimension<4?(code.push(\"function \"+className+\"_order(){\"),2===dimension?code.push(\"return (Math.abs(this.stride[0])>Math.abs(this.stride[1]))?[1,0]:[0,1]}})\"):3===dimension&&code.push(\"var s0=Math.abs(this.stride[0]),s1=Math.abs(this.stride[1]),s2=Math.abs(this.stride[2]);if(s0>s1){if(s1>s2){return [2,1,0];}else if(s0>s2){return [1,2,0];}else{return [1,0,2];}}else if(s0>s2){return [2,0,1];}else if(s2>s1){return [0,1,2];}else{return [0,2,1];}}})\")):code.push(\"ORDER})\")),code.push(\"proto.set=function \"+className+\"_set(\"+args.join(\",\")+\",v){\"),useGetters?code.push(\"return this.data.set(\"+index_str+\",v)}\"):code.push(\"return this.data[\"+index_str+\"]=v}\"),code.push(\"proto.get=function \"+className+\"_get(\"+args.join(\",\")+\"){\"),useGetters?code.push(\"return this.data.get(\"+index_str+\")}\"):code.push(\"return this.data[\"+index_str+\"]}\"),code.push(\"proto.index=function \"+className+\"_index(\",args.join(),\"){return \"+index_str+\"}\"),code.push(\"proto.hi=function \"+className+\"_hi(\"+args.join(\",\")+\"){return new \"+className+\"(this.data,\"+indices.map((function(i){return[\"(typeof i\",i,\"!=='number'||i\",i,\"<0)?this.shape[\",i,\"]:i\",i,\"|0\"].join(\"\")})).join(\",\")+\",\"+indices.map((function(i){return\"this.stride[\"+i+\"]\"})).join(\",\")+\",this.offset)}\");var a_vars=indices.map((function(i){return\"a\"+i+\"=this.shape[\"+i+\"]\"})),c_vars=indices.map((function(i){return\"c\"+i+\"=this.stride[\"+i+\"]\"}));code.push(\"proto.lo=function \"+className+\"_lo(\"+args.join(\",\")+\"){var b=this.offset,d=0,\"+a_vars.join(\",\")+\",\"+c_vars.join(\",\"));for(var i=0;i<dimension;++i)code.push(\"if(typeof i\"+i+\"==='number'&&i\"+i+\">=0){d=i\"+i+\"|0;b+=c\"+i+\"*d;a\"+i+\"-=d}\");code.push(\"return new \"+className+\"(this.data,\"+indices.map((function(i){return\"a\"+i})).join(\",\")+\",\"+indices.map((function(i){return\"c\"+i})).join(\",\")+\",b)}\"),code.push(\"proto.step=function \"+className+\"_step(\"+args.join(\",\")+\"){var \"+indices.map((function(i){return\"a\"+i+\"=this.shape[\"+i+\"]\"})).join(\",\")+\",\"+indices.map((function(i){return\"b\"+i+\"=this.stride[\"+i+\"]\"})).join(\",\")+\",c=this.offset,d=0,ceil=Math.ceil\");for(i=0;i<dimension;++i)code.push(\"if(typeof i\"+i+\"==='number'){d=i\"+i+\"|0;if(d<0){c+=b\"+i+\"*(a\"+i+\"-1);a\"+i+\"=ceil(-a\"+i+\"/d)}else{a\"+i+\"=ceil(a\"+i+\"/d)}b\"+i+\"*=d}\");code.push(\"return new \"+className+\"(this.data,\"+indices.map((function(i){return\"a\"+i})).join(\",\")+\",\"+indices.map((function(i){return\"b\"+i})).join(\",\")+\",c)}\");var tShape=new Array(dimension),tStride=new Array(dimension);for(i=0;i<dimension;++i)tShape[i]=\"a[i\"+i+\"]\",tStride[i]=\"b[i\"+i+\"]\";code.push(\"proto.transpose=function \"+className+\"_transpose(\"+args+\"){\"+args.map((function(n,idx){return n+\"=(\"+n+\"===undefined?\"+idx+\":\"+n+\"|0)\"})).join(\";\"),\"var a=this.shape,b=this.stride;return new \"+className+\"(this.data,\"+tShape.join(\",\")+\",\"+tStride.join(\",\")+\",this.offset)}\"),code.push(\"proto.pick=function \"+className+\"_pick(\"+args+\"){var a=[],b=[],c=this.offset\");for(i=0;i<dimension;++i)code.push(\"if(typeof i\"+i+\"==='number'&&i\"+i+\">=0){c=(c+this.stride[\"+i+\"]*i\"+i+\")|0}else{a.push(this.shape[\"+i+\"]);b.push(this.stride[\"+i+\"])}\");return code.push(\"var ctor=CTOR_LIST[a.length+1];return ctor(this.data,a,b,c)}\"),code.push(\"return function construct_\"+className+\"(data,shape,stride,offset){return new \"+className+\"(data,\"+indices.map((function(i){return\"shape[\"+i+\"]\"})).join(\",\")+\",\"+indices.map((function(i){return\"stride[\"+i+\"]\"})).join(\",\")+\",offset)}\"),new Function(\"CTOR_LIST\",\"ORDER\",code.join(\"\\n\"))(CACHED_CONSTRUCTORS[dtype],order)}var CACHED_CONSTRUCTORS={float32:[],float64:[],int8:[],int16:[],int32:[],uint8:[],uint16:[],uint32:[],array:[],uint8_clamped:[],bigint64:[],biguint64:[],buffer:[],generic:[]};module.exports=function wrappedNDArrayCtor(data,shape,stride,offset){if(void 0===data)return(0,CACHED_CONSTRUCTORS.array[0])([]);\"number\"==typeof data&&(data=[data]),void 0===shape&&(shape=[data.length]);var d=shape.length;if(void 0===stride){stride=new Array(d);for(var i=d-1,sz=1;i>=0;--i)stride[i]=sz,sz*=shape[i]}if(void 0===offset){offset=0;for(i=0;i<d;++i)stride[i]<0&&(offset-=(shape[i]-1)*stride[i])}for(var dtype=function arrayDType(data){if(isBuffer(data))return\"buffer\";if(hasTypedArrays)switch(Object.prototype.toString.call(data)){case\"[object Float64Array]\":return\"float64\";case\"[object Float32Array]\":return\"float32\";case\"[object Int8Array]\":return\"int8\";case\"[object Int16Array]\":return\"int16\";case\"[object Int32Array]\":return\"int32\";case\"[object Uint8Array]\":return\"uint8\";case\"[object Uint16Array]\":return\"uint16\";case\"[object Uint32Array]\":return\"uint32\";case\"[object Uint8ClampedArray]\":return\"uint8_clamped\";case\"[object BigInt64Array]\":return\"bigint64\";case\"[object BigUint64Array]\":return\"biguint64\"}return Array.isArray(data)?\"array\":\"generic\"}(data),ctor_list=CACHED_CONSTRUCTORS[dtype];ctor_list.length<=d+1;)ctor_list.push(compileConstructor(dtype,ctor_list.length-1));return(0,ctor_list[d+1])(data,shape,stride,offset)}},829:(__unused_webpack_module,exports)=>{\"use strict\";function _defineProperties(a,b){for(var c,d=0;d<b.length;d++)(c=b[d]).enumerable=c.enumerable||!1,c.configurable=!0,\"value\"in c&&(c.writable=!0),Object.defineProperty(a,c.key,c)}exports.g=function fromArrayBuffer(a){if(!a instanceof ArrayBuffer)throw new Error(\"Argument must be an ArrayBuffer.\");var b=new DataViewReader(a),c=b.readUint8(),d=b.readAndASCIIDecodeBytes(5);if(147!=c||\"NUMPY\"!=d)throw new Error('unknown file type: \"'.concat(c).concat(d,'\"'));var e,f=b.readUint8(),h=(b.readUint8(),10+(e=1>=f?b.readUint16(!0):b.readUint32(!0)));0!=h%16&&console.warn(\"NPY file header is incorrectly padded. (\".concat(h,\" is not evenly divisible by 16.)\"));var j=function parseHeaderStr(a){var b=a.toLowerCase().replace(\"(\",\"[\").replace(\"),\",\"]\").replace(\"[,\",\"[1,]\").replace(\",]\",\",1]\").replace(/'/g,'\"');return JSON.parse(b)}(b.readAndASCIIDecodeBytes(e));if(j.fortran_order)throw new Error(\"NPY file is written in Fortran byte order, support for this byte order is not yet implemented.\");return{data:new(typedArrayConstructorForDescription(j.descr))(a,b.offset),shape:j.shape}};var DataViewReader=function(){function a(b){(function _classCallCheck(a,b){if(!(a instanceof b))throw new TypeError(\"Cannot call a class as a function\")})(this,a),b instanceof DataView?this.dataView=b:b instanceof ArrayBuffer&&(this.dataView=new DataView(b)),this.offset=0}return function _createClass(a,b,c){return b&&_defineProperties(a.prototype,b),c&&_defineProperties(a,c),a}(a,[{key:\"readBytes\",value:function c(a){var b=new DataView(this.dataView.buffer,this.offset,a);return this.offset+=a,b}},{key:\"readAndASCIIDecodeBytes\",value:function c(a){var b=new Uint8Array(this.dataView.buffer,this.offset,a);return this.offset+=a,this._decodeASCIIByteArray(b)}},{key:\"readUint8\",value:function c(){var a=!!(0<arguments.length&&void 0!==arguments[0])&&arguments[0],b=this.dataView.getUint8(this.offset,a);return this.offset+=Uint8Array.BYTES_PER_ELEMENT,b}},{key:\"readUint16\",value:function c(){var a=!!(0<arguments.length&&void 0!==arguments[0])&&arguments[0],b=this.dataView.getUint16(this.offset,a);return this.offset+=Uint16Array.BYTES_PER_ELEMENT,b}},{key:\"readUint32\",value:function c(){var a=!!(0<arguments.length&&void 0!==arguments[0])&&arguments[0],b=this.dataView.getUint32(this.offset,a);return this.offset+=Uint32Array.BYTES_PER_ELEMENT,b}},{key:\"_decodeASCIIByteArray\",value:function k(a){var b=String.fromCharCode,c=[],d=!0,e=!1,f=void 0;try{for(var g,h=a[Symbol.iterator]();!(d=(g=h.next()).done);d=!0){var j=b(g.value);c.push(j)}}catch(a){e=!0,f=a}finally{try{d||null==h.return||h.return()}finally{if(e)throw f}}return c.join(\"\")}}]),a}();function typedArrayConstructorForDescription(a){switch(a){case\"|u1\":return Uint8Array;case\"<u2\":return Uint16Array;case\"<u4\":return Uint32Array;case\"<u8\":throw new Error(\"Because JavaScript doesn't currently include standard support for 64-bit unsigned integer values, support for this dtype is not yet implemented.\");case\"|i1\":return Int8Array;case\"<i2\":return Int16Array;case\"<i4\":return Int32Array;case\"<i8\":throw new Error(\"Because JavaScript doesn't currently include standard support for 64-bit integer values, support for this dtype is not yet implemented.\");case\"<f2\":throw new Error(\"Because JavaScript doesn't currently include standard support for 16-bit floating point values, support for this dtype is not yet implemented.\");case\"<f4\":return Float32Array;case\"<f8\":return Float64Array;default:throw new Error(\"Unknown or not yet implemented numpy dtype description: \"+dtype)}}},843:(module,__unused_webpack_exports,__webpack_require__)=>{\"use strict\";const zlib_inflate=__webpack_require__(948),utils=__webpack_require__(236),strings=__webpack_require__(373),msg=__webpack_require__(898),ZStream=__webpack_require__(292),GZheader=__webpack_require__(401),toString=Object.prototype.toString,{Z_NO_FLUSH,Z_FINISH,Z_OK,Z_STREAM_END,Z_NEED_DICT,Z_STREAM_ERROR,Z_DATA_ERROR,Z_MEM_ERROR}=__webpack_require__(619);function Inflate(options){this.options=utils.assign({chunkSize:65536,windowBits:15,to:\"\"},options||{});const opt=this.options;opt.raw&&opt.windowBits>=0&&opt.windowBits<16&&(opt.windowBits=-opt.windowBits,0===opt.windowBits&&(opt.windowBits=-15)),!(opt.windowBits>=0&&opt.windowBits<16)||options&&options.windowBits||(opt.windowBits+=32),opt.windowBits>15&&opt.windowBits<48&&0==(15&opt.windowBits)&&(opt.windowBits|=15),this.err=0,this.msg=\"\",this.ended=!1,this.chunks=[],this.strm=new ZStream,this.strm.avail_out=0;let status=zlib_inflate.inflateInit2(this.strm,opt.windowBits);if(status!==Z_OK)throw new Error(msg[status]);if(this.header=new GZheader,zlib_inflate.inflateGetHeader(this.strm,this.header),opt.dictionary&&(\"string\"==typeof opt.dictionary?opt.dictionary=strings.string2buf(opt.dictionary):\"[object ArrayBuffer]\"===toString.call(opt.dictionary)&&(opt.dictionary=new Uint8Array(opt.dictionary)),opt.raw&&(status=zlib_inflate.inflateSetDictionary(this.strm,opt.dictionary),status!==Z_OK)))throw new Error(msg[status])}function inflate(input,options){const inflator=new Inflate(options);if(inflator.push(input),inflator.err)throw inflator.msg||msg[inflator.err];return inflator.result}Inflate.prototype.push=function(data,flush_mode){const strm=this.strm,chunkSize=this.options.chunkSize,dictionary=this.options.dictionary;let status,_flush_mode,last_avail_out;if(this.ended)return!1;for(_flush_mode=flush_mode===~~flush_mode?flush_mode:!0===flush_mode?Z_FINISH:Z_NO_FLUSH,\"[object ArrayBuffer]\"===toString.call(data)?strm.input=new Uint8Array(data):strm.input=data,strm.next_in=0,strm.avail_in=strm.input.length;;){for(0===strm.avail_out&&(strm.output=new Uint8Array(chunkSize),strm.next_out=0,strm.avail_out=chunkSize),status=zlib_inflate.inflate(strm,_flush_mode),status===Z_NEED_DICT&&dictionary&&(status=zlib_inflate.inflateSetDictionary(strm,dictionary),status===Z_OK?status=zlib_inflate.inflate(strm,_flush_mode):status===Z_DATA_ERROR&&(status=Z_NEED_DICT));strm.avail_in>0&&status===Z_STREAM_END&&strm.state.wrap>0&&0!==data[strm.next_in];)zlib_inflate.inflateReset(strm),status=zlib_inflate.inflate(strm,_flush_mode);switch(status){case Z_STREAM_ERROR:case Z_DATA_ERROR:case Z_NEED_DICT:case Z_MEM_ERROR:return this.onEnd(status),this.ended=!0,!1}if(last_avail_out=strm.avail_out,strm.next_out&&(0===strm.avail_out||status===Z_STREAM_END))if(\"string\"===this.options.to){let next_out_utf8=strings.utf8border(strm.output,strm.next_out),tail=strm.next_out-next_out_utf8,utf8str=strings.buf2string(strm.output,next_out_utf8);strm.next_out=tail,strm.avail_out=chunkSize-tail,tail&&strm.output.set(strm.output.subarray(next_out_utf8,next_out_utf8+tail),0),this.onData(utf8str)}else this.onData(strm.output.length===strm.next_out?strm.output:strm.output.subarray(0,strm.next_out));if(status!==Z_OK||0!==last_avail_out){if(status===Z_STREAM_END)return status=zlib_inflate.inflateEnd(this.strm),this.onEnd(status),this.ended=!0,!0;if(0===strm.avail_in)break}}return!0},Inflate.prototype.onData=function(chunk){this.chunks.push(chunk)},Inflate.prototype.onEnd=function(status){status===Z_OK&&(\"string\"===this.options.to?this.result=this.chunks.join(\"\"):this.result=utils.flattenChunks(this.chunks)),this.chunks=[],this.err=status,this.msg=this.strm.msg},module.exports.rr=inflate,__webpack_require__(619)},236:module=>{\"use strict\";const _has=(obj,key)=>Object.prototype.hasOwnProperty.call(obj,key);module.exports.assign=function(obj){const sources=Array.prototype.slice.call(arguments,1);for(;sources.length;){const source=sources.shift();if(source){if(\"object\"!=typeof source)throw new TypeError(source+\"must be non-object\");for(const p in source)_has(source,p)&&(obj[p]=source[p])}}return obj},module.exports.flattenChunks=chunks=>{let len=0;for(let i=0,l=chunks.length;i<l;i++)len+=chunks[i].length;const result=new Uint8Array(len);for(let i=0,pos=0,l=chunks.length;i<l;i++){let chunk=chunks[i];result.set(chunk,pos),pos+=chunk.length}return result}},373:module=>{\"use strict\";let STR_APPLY_UIA_OK=!0;try{String.fromCharCode.apply(null,new Uint8Array(1))}catch(__){STR_APPLY_UIA_OK=!1}const _utf8len=new Uint8Array(256);for(let q=0;q<256;q++)_utf8len[q]=q>=252?6:q>=248?5:q>=240?4:q>=224?3:q>=192?2:1;_utf8len[254]=_utf8len[254]=1,module.exports.string2buf=str=>{let buf,c,c2,m_pos,i,str_len=str.length,buf_len=0;for(m_pos=0;m_pos<str_len;m_pos++)c=str.charCodeAt(m_pos),55296==(64512&c)&&m_pos+1<str_len&&(c2=str.charCodeAt(m_pos+1),56320==(64512&c2)&&(c=65536+(c-55296<<10)+(c2-56320),m_pos++)),buf_len+=c<128?1:c<2048?2:c<65536?3:4;for(buf=new Uint8Array(buf_len),i=0,m_pos=0;i<buf_len;m_pos++)c=str.charCodeAt(m_pos),55296==(64512&c)&&m_pos+1<str_len&&(c2=str.charCodeAt(m_pos+1),56320==(64512&c2)&&(c=65536+(c-55296<<10)+(c2-56320),m_pos++)),c<128?buf[i++]=c:c<2048?(buf[i++]=192|c>>>6,buf[i++]=128|63&c):c<65536?(buf[i++]=224|c>>>12,buf[i++]=128|c>>>6&63,buf[i++]=128|63&c):(buf[i++]=240|c>>>18,buf[i++]=128|c>>>12&63,buf[i++]=128|c>>>6&63,buf[i++]=128|63&c);return buf};module.exports.buf2string=(buf,max)=>{let i,out;const len=max||buf.length,utf16buf=new Array(2*len);for(out=0,i=0;i<len;){let c=buf[i++];if(c<128){utf16buf[out++]=c;continue}let c_len=_utf8len[c];if(c_len>4)utf16buf[out++]=65533,i+=c_len-1;else{for(c&=2===c_len?31:3===c_len?15:7;c_len>1&&i<len;)c=c<<6|63&buf[i++],c_len--;c_len>1?utf16buf[out++]=65533:c<65536?utf16buf[out++]=c:(c-=65536,utf16buf[out++]=55296|c>>10&1023,utf16buf[out++]=56320|1023&c)}}return((buf,len)=>{if(len<65534&&buf.subarray&&STR_APPLY_UIA_OK)return String.fromCharCode.apply(null,buf.length===len?buf:buf.subarray(0,len));let result=\"\";for(let i=0;i<len;i++)result+=String.fromCharCode(buf[i]);return result})(utf16buf,out)},module.exports.utf8border=(buf,max)=>{(max=max||buf.length)>buf.length&&(max=buf.length);let pos=max-1;for(;pos>=0&&128==(192&buf[pos]);)pos--;return pos<0||0===pos?max:pos+_utf8len[buf[pos]]>max?pos:max}},69:module=>{\"use strict\";module.exports=(adler,buf,len,pos)=>{let s1=65535&adler|0,s2=adler>>>16&65535|0,n=0;for(;0!==len;){n=len>2e3?2e3:len,len-=n;do{s1=s1+buf[pos++]|0,s2=s2+s1|0}while(--n);s1%=65521,s2%=65521}return s1|s2<<16|0}},619:module=>{\"use strict\";module.exports={Z_NO_FLUSH:0,Z_PARTIAL_FLUSH:1,Z_SYNC_FLUSH:2,Z_FULL_FLUSH:3,Z_FINISH:4,Z_BLOCK:5,Z_TREES:6,Z_OK:0,Z_STREAM_END:1,Z_NEED_DICT:2,Z_ERRNO:-1,Z_STREAM_ERROR:-2,Z_DATA_ERROR:-3,Z_MEM_ERROR:-4,Z_BUF_ERROR:-5,Z_NO_COMPRESSION:0,Z_BEST_SPEED:1,Z_BEST_COMPRESSION:9,Z_DEFAULT_COMPRESSION:-1,Z_FILTERED:1,Z_HUFFMAN_ONLY:2,Z_RLE:3,Z_FIXED:4,Z_DEFAULT_STRATEGY:0,Z_BINARY:0,Z_TEXT:1,Z_UNKNOWN:2,Z_DEFLATED:8}},869:module=>{\"use strict\";const crcTable=new Uint32Array((()=>{let c,table=[];for(var n=0;n<256;n++){c=n;for(var k=0;k<8;k++)c=1&c?3988292384^c>>>1:c>>>1;table[n]=c}return table})());module.exports=(crc,buf,len,pos)=>{const t=crcTable,end=pos+len;crc^=-1;for(let i=pos;i<end;i++)crc=crc>>>8^t[255&(crc^buf[i])];return-1^crc}},401:module=>{\"use strict\";module.exports=function GZheader(){this.text=0,this.time=0,this.xflags=0,this.os=0,this.extra=null,this.extra_len=0,this.name=\"\",this.comment=\"\",this.hcrc=0,this.done=!1}},264:module=>{\"use strict\";module.exports=function inflate_fast(strm,start){let _in,last,_out,beg,end,dmax,wsize,whave,wnext,s_window,hold,bits,lcode,dcode,lmask,dmask,here,op,len,dist,from,from_source,input,output;const state=strm.state;_in=strm.next_in,input=strm.input,last=_in+(strm.avail_in-5),_out=strm.next_out,output=strm.output,beg=_out-(start-strm.avail_out),end=_out+(strm.avail_out-257),dmax=state.dmax,wsize=state.wsize,whave=state.whave,wnext=state.wnext,s_window=state.window,hold=state.hold,bits=state.bits,lcode=state.lencode,dcode=state.distcode,lmask=(1<<state.lenbits)-1,dmask=(1<<state.distbits)-1;top:do{bits<15&&(hold+=input[_in++]<<bits,bits+=8,hold+=input[_in++]<<bits,bits+=8),here=lcode[hold&lmask];dolen:for(;;){if(op=here>>>24,hold>>>=op,bits-=op,op=here>>>16&255,0===op)output[_out++]=65535&here;else{if(!(16&op)){if(0==(64&op)){here=lcode[(65535&here)+(hold&(1<<op)-1)];continue dolen}if(32&op){state.mode=12;break top}strm.msg=\"invalid literal/length code\",state.mode=30;break top}len=65535&here,op&=15,op&&(bits<op&&(hold+=input[_in++]<<bits,bits+=8),len+=hold&(1<<op)-1,hold>>>=op,bits-=op),bits<15&&(hold+=input[_in++]<<bits,bits+=8,hold+=input[_in++]<<bits,bits+=8),here=dcode[hold&dmask];dodist:for(;;){if(op=here>>>24,hold>>>=op,bits-=op,op=here>>>16&255,!(16&op)){if(0==(64&op)){here=dcode[(65535&here)+(hold&(1<<op)-1)];continue dodist}strm.msg=\"invalid distance code\",state.mode=30;break top}if(dist=65535&here,op&=15,bits<op&&(hold+=input[_in++]<<bits,bits+=8,bits<op&&(hold+=input[_in++]<<bits,bits+=8)),dist+=hold&(1<<op)-1,dist>dmax){strm.msg=\"invalid distance too far back\",state.mode=30;break top}if(hold>>>=op,bits-=op,op=_out-beg,dist>op){if(op=dist-op,op>whave&&state.sane){strm.msg=\"invalid distance too far back\",state.mode=30;break top}if(from=0,from_source=s_window,0===wnext){if(from+=wsize-op,op<len){len-=op;do{output[_out++]=s_window[from++]}while(--op);from=_out-dist,from_source=output}}else if(wnext<op){if(from+=wsize+wnext-op,op-=wnext,op<len){len-=op;do{output[_out++]=s_window[from++]}while(--op);if(from=0,wnext<len){op=wnext,len-=op;do{output[_out++]=s_window[from++]}while(--op);from=_out-dist,from_source=output}}}else if(from+=wnext-op,op<len){len-=op;do{output[_out++]=s_window[from++]}while(--op);from=_out-dist,from_source=output}for(;len>2;)output[_out++]=from_source[from++],output[_out++]=from_source[from++],output[_out++]=from_source[from++],len-=3;len&&(output[_out++]=from_source[from++],len>1&&(output[_out++]=from_source[from++]))}else{from=_out-dist;do{output[_out++]=output[from++],output[_out++]=output[from++],output[_out++]=output[from++],len-=3}while(len>2);len&&(output[_out++]=output[from++],len>1&&(output[_out++]=output[from++]))}break}}break}}while(_in<last&&_out<end);len=bits>>3,_in-=len,bits-=len<<3,hold&=(1<<bits)-1,strm.next_in=_in,strm.next_out=_out,strm.avail_in=_in<last?last-_in+5:5-(_in-last),strm.avail_out=_out<end?end-_out+257:257-(_out-end),state.hold=hold,state.bits=bits}},948:(module,__unused_webpack_exports,__webpack_require__)=>{\"use strict\";const adler32=__webpack_require__(69),crc32=__webpack_require__(869),inflate_fast=__webpack_require__(264),inflate_table=__webpack_require__(241),{Z_FINISH,Z_BLOCK,Z_TREES,Z_OK,Z_STREAM_END,Z_NEED_DICT,Z_STREAM_ERROR,Z_DATA_ERROR,Z_MEM_ERROR,Z_BUF_ERROR,Z_DEFLATED}=__webpack_require__(619),zswap32=q=>(q>>>24&255)+(q>>>8&65280)+((65280&q)<<8)+((255&q)<<24);function InflateState(){this.mode=0,this.last=!1,this.wrap=0,this.havedict=!1,this.flags=0,this.dmax=0,this.check=0,this.total=0,this.head=null,this.wbits=0,this.wsize=0,this.whave=0,this.wnext=0,this.window=null,this.hold=0,this.bits=0,this.length=0,this.offset=0,this.extra=0,this.lencode=null,this.distcode=null,this.lenbits=0,this.distbits=0,this.ncode=0,this.nlen=0,this.ndist=0,this.have=0,this.next=null,this.lens=new Uint16Array(320),this.work=new Uint16Array(288),this.lendyn=null,this.distdyn=null,this.sane=0,this.back=0,this.was=0}const inflateResetKeep=strm=>{if(!strm||!strm.state)return Z_STREAM_ERROR;const state=strm.state;return strm.total_in=strm.total_out=state.total=0,strm.msg=\"\",state.wrap&&(strm.adler=1&state.wrap),state.mode=1,state.last=0,state.havedict=0,state.dmax=32768,state.head=null,state.hold=0,state.bits=0,state.lencode=state.lendyn=new Int32Array(852),state.distcode=state.distdyn=new Int32Array(592),state.sane=1,state.back=-1,Z_OK},inflateReset=strm=>{if(!strm||!strm.state)return Z_STREAM_ERROR;const state=strm.state;return state.wsize=0,state.whave=0,state.wnext=0,inflateResetKeep(strm)},inflateReset2=(strm,windowBits)=>{let wrap;if(!strm||!strm.state)return Z_STREAM_ERROR;const state=strm.state;return windowBits<0?(wrap=0,windowBits=-windowBits):(wrap=1+(windowBits>>4),windowBits<48&&(windowBits&=15)),windowBits&&(windowBits<8||windowBits>15)?Z_STREAM_ERROR:(null!==state.window&&state.wbits!==windowBits&&(state.window=null),state.wrap=wrap,state.wbits=windowBits,inflateReset(strm))},inflateInit2=(strm,windowBits)=>{if(!strm)return Z_STREAM_ERROR;const state=new InflateState;strm.state=state,state.window=null;const ret=inflateReset2(strm,windowBits);return ret!==Z_OK&&(strm.state=null),ret};let lenfix,distfix,virgin=!0;const fixedtables=state=>{if(virgin){lenfix=new Int32Array(512),distfix=new Int32Array(32);let sym=0;for(;sym<144;)state.lens[sym++]=8;for(;sym<256;)state.lens[sym++]=9;for(;sym<280;)state.lens[sym++]=7;for(;sym<288;)state.lens[sym++]=8;for(inflate_table(1,state.lens,0,288,lenfix,0,state.work,{bits:9}),sym=0;sym<32;)state.lens[sym++]=5;inflate_table(2,state.lens,0,32,distfix,0,state.work,{bits:5}),virgin=!1}state.lencode=lenfix,state.lenbits=9,state.distcode=distfix,state.distbits=5},updatewindow=(strm,src,end,copy)=>{let dist;const state=strm.state;return null===state.window&&(state.wsize=1<<state.wbits,state.wnext=0,state.whave=0,state.window=new Uint8Array(state.wsize)),copy>=state.wsize?(state.window.set(src.subarray(end-state.wsize,end),0),state.wnext=0,state.whave=state.wsize):(dist=state.wsize-state.wnext,dist>copy&&(dist=copy),state.window.set(src.subarray(end-copy,end-copy+dist),state.wnext),(copy-=dist)?(state.window.set(src.subarray(end-copy,end),0),state.wnext=copy,state.whave=state.wsize):(state.wnext+=dist,state.wnext===state.wsize&&(state.wnext=0),state.whave<state.wsize&&(state.whave+=dist))),0};module.exports.inflateReset=inflateReset,module.exports.inflateReset2=inflateReset2,module.exports.inflateResetKeep=inflateResetKeep,module.exports.inflateInit=strm=>inflateInit2(strm,15),module.exports.inflateInit2=inflateInit2,module.exports.inflate=(strm,flush)=>{let state,input,output,next,put,have,left,hold,bits,_in,_out,copy,from,from_source,here_bits,here_op,here_val,last_bits,last_op,last_val,len,ret,here=0;const hbuf=new Uint8Array(4);let opts,n;const order=new Uint8Array([16,17,18,0,8,7,9,6,10,5,11,4,12,3,13,2,14,1,15]);if(!strm||!strm.state||!strm.output||!strm.input&&0!==strm.avail_in)return Z_STREAM_ERROR;state=strm.state,12===state.mode&&(state.mode=13),put=strm.next_out,output=strm.output,left=strm.avail_out,next=strm.next_in,input=strm.input,have=strm.avail_in,hold=state.hold,bits=state.bits,_in=have,_out=left,ret=Z_OK;inf_leave:for(;;)switch(state.mode){case 1:if(0===state.wrap){state.mode=13;break}for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(2&state.wrap&&35615===hold){state.check=0,hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,state.check=crc32(state.check,hbuf,2,0),hold=0,bits=0,state.mode=2;break}if(state.flags=0,state.head&&(state.head.done=!1),!(1&state.wrap)||(((255&hold)<<8)+(hold>>8))%31){strm.msg=\"incorrect header check\",state.mode=30;break}if((15&hold)!==Z_DEFLATED){strm.msg=\"unknown compression method\",state.mode=30;break}if(hold>>>=4,bits-=4,len=8+(15&hold),0===state.wbits)state.wbits=len;else if(len>state.wbits){strm.msg=\"invalid window size\",state.mode=30;break}state.dmax=1<<state.wbits,strm.adler=state.check=1,state.mode=512&hold?10:12,hold=0,bits=0;break;case 2:for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(state.flags=hold,(255&state.flags)!==Z_DEFLATED){strm.msg=\"unknown compression method\",state.mode=30;break}if(57344&state.flags){strm.msg=\"unknown header flags set\",state.mode=30;break}state.head&&(state.head.text=hold>>8&1),512&state.flags&&(hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,state.check=crc32(state.check,hbuf,2,0)),hold=0,bits=0,state.mode=3;case 3:for(;bits<32;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.head&&(state.head.time=hold),512&state.flags&&(hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,hbuf[2]=hold>>>16&255,hbuf[3]=hold>>>24&255,state.check=crc32(state.check,hbuf,4,0)),hold=0,bits=0,state.mode=4;case 4:for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.head&&(state.head.xflags=255&hold,state.head.os=hold>>8),512&state.flags&&(hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,state.check=crc32(state.check,hbuf,2,0)),hold=0,bits=0,state.mode=5;case 5:if(1024&state.flags){for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.length=hold,state.head&&(state.head.extra_len=hold),512&state.flags&&(hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,state.check=crc32(state.check,hbuf,2,0)),hold=0,bits=0}else state.head&&(state.head.extra=null);state.mode=6;case 6:if(1024&state.flags&&(copy=state.length,copy>have&&(copy=have),copy&&(state.head&&(len=state.head.extra_len-state.length,state.head.extra||(state.head.extra=new Uint8Array(state.head.extra_len)),state.head.extra.set(input.subarray(next,next+copy),len)),512&state.flags&&(state.check=crc32(state.check,input,copy,next)),have-=copy,next+=copy,state.length-=copy),state.length))break inf_leave;state.length=0,state.mode=7;case 7:if(2048&state.flags){if(0===have)break inf_leave;copy=0;do{len=input[next+copy++],state.head&&len&&state.length<65536&&(state.head.name+=String.fromCharCode(len))}while(len&&copy<have);if(512&state.flags&&(state.check=crc32(state.check,input,copy,next)),have-=copy,next+=copy,len)break inf_leave}else state.head&&(state.head.name=null);state.length=0,state.mode=8;case 8:if(4096&state.flags){if(0===have)break inf_leave;copy=0;do{len=input[next+copy++],state.head&&len&&state.length<65536&&(state.head.comment+=String.fromCharCode(len))}while(len&&copy<have);if(512&state.flags&&(state.check=crc32(state.check,input,copy,next)),have-=copy,next+=copy,len)break inf_leave}else state.head&&(state.head.comment=null);state.mode=9;case 9:if(512&state.flags){for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(hold!==(65535&state.check)){strm.msg=\"header crc mismatch\",state.mode=30;break}hold=0,bits=0}state.head&&(state.head.hcrc=state.flags>>9&1,state.head.done=!0),strm.adler=state.check=0,state.mode=12;break;case 10:for(;bits<32;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}strm.adler=state.check=zswap32(hold),hold=0,bits=0,state.mode=11;case 11:if(0===state.havedict)return strm.next_out=put,strm.avail_out=left,strm.next_in=next,strm.avail_in=have,state.hold=hold,state.bits=bits,Z_NEED_DICT;strm.adler=state.check=1,state.mode=12;case 12:if(flush===Z_BLOCK||flush===Z_TREES)break inf_leave;case 13:if(state.last){hold>>>=7&bits,bits-=7&bits,state.mode=27;break}for(;bits<3;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}switch(state.last=1&hold,hold>>>=1,bits-=1,3&hold){case 0:state.mode=14;break;case 1:if(fixedtables(state),state.mode=20,flush===Z_TREES){hold>>>=2,bits-=2;break inf_leave}break;case 2:state.mode=17;break;case 3:strm.msg=\"invalid block type\",state.mode=30}hold>>>=2,bits-=2;break;case 14:for(hold>>>=7&bits,bits-=7&bits;bits<32;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if((65535&hold)!=(hold>>>16^65535)){strm.msg=\"invalid stored block lengths\",state.mode=30;break}if(state.length=65535&hold,hold=0,bits=0,state.mode=15,flush===Z_TREES)break inf_leave;case 15:state.mode=16;case 16:if(copy=state.length,copy){if(copy>have&&(copy=have),copy>left&&(copy=left),0===copy)break inf_leave;output.set(input.subarray(next,next+copy),put),have-=copy,next+=copy,left-=copy,put+=copy,state.length-=copy;break}state.mode=12;break;case 17:for(;bits<14;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(state.nlen=257+(31&hold),hold>>>=5,bits-=5,state.ndist=1+(31&hold),hold>>>=5,bits-=5,state.ncode=4+(15&hold),hold>>>=4,bits-=4,state.nlen>286||state.ndist>30){strm.msg=\"too many length or distance symbols\",state.mode=30;break}state.have=0,state.mode=18;case 18:for(;state.have<state.ncode;){for(;bits<3;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.lens[order[state.have++]]=7&hold,hold>>>=3,bits-=3}for(;state.have<19;)state.lens[order[state.have++]]=0;if(state.lencode=state.lendyn,state.lenbits=7,opts={bits:state.lenbits},ret=inflate_table(0,state.lens,0,19,state.lencode,0,state.work,opts),state.lenbits=opts.bits,ret){strm.msg=\"invalid code lengths set\",state.mode=30;break}state.have=0,state.mode=19;case 19:for(;state.have<state.nlen+state.ndist;){for(;here=state.lencode[hold&(1<<state.lenbits)-1],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(here_val<16)hold>>>=here_bits,bits-=here_bits,state.lens[state.have++]=here_val;else{if(16===here_val){for(n=here_bits+2;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(hold>>>=here_bits,bits-=here_bits,0===state.have){strm.msg=\"invalid bit length repeat\",state.mode=30;break}len=state.lens[state.have-1],copy=3+(3&hold),hold>>>=2,bits-=2}else if(17===here_val){for(n=here_bits+3;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}hold>>>=here_bits,bits-=here_bits,len=0,copy=3+(7&hold),hold>>>=3,bits-=3}else{for(n=here_bits+7;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}hold>>>=here_bits,bits-=here_bits,len=0,copy=11+(127&hold),hold>>>=7,bits-=7}if(state.have+copy>state.nlen+state.ndist){strm.msg=\"invalid bit length repeat\",state.mode=30;break}for(;copy--;)state.lens[state.have++]=len}}if(30===state.mode)break;if(0===state.lens[256]){strm.msg=\"invalid code -- missing end-of-block\",state.mode=30;break}if(state.lenbits=9,opts={bits:state.lenbits},ret=inflate_table(1,state.lens,0,state.nlen,state.lencode,0,state.work,opts),state.lenbits=opts.bits,ret){strm.msg=\"invalid literal/lengths set\",state.mode=30;break}if(state.distbits=6,state.distcode=state.distdyn,opts={bits:state.distbits},ret=inflate_table(2,state.lens,state.nlen,state.ndist,state.distcode,0,state.work,opts),state.distbits=opts.bits,ret){strm.msg=\"invalid distances set\",state.mode=30;break}if(state.mode=20,flush===Z_TREES)break inf_leave;case 20:state.mode=21;case 21:if(have>=6&&left>=258){strm.next_out=put,strm.avail_out=left,strm.next_in=next,strm.avail_in=have,state.hold=hold,state.bits=bits,inflate_fast(strm,_out),put=strm.next_out,output=strm.output,left=strm.avail_out,next=strm.next_in,input=strm.input,have=strm.avail_in,hold=state.hold,bits=state.bits,12===state.mode&&(state.back=-1);break}for(state.back=0;here=state.lencode[hold&(1<<state.lenbits)-1],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(here_op&&0==(240&here_op)){for(last_bits=here_bits,last_op=here_op,last_val=here_val;here=state.lencode[last_val+((hold&(1<<last_bits+last_op)-1)>>last_bits)],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(last_bits+here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}hold>>>=last_bits,bits-=last_bits,state.back+=last_bits}if(hold>>>=here_bits,bits-=here_bits,state.back+=here_bits,state.length=here_val,0===here_op){state.mode=26;break}if(32&here_op){state.back=-1,state.mode=12;break}if(64&here_op){strm.msg=\"invalid literal/length code\",state.mode=30;break}state.extra=15&here_op,state.mode=22;case 22:if(state.extra){for(n=state.extra;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.length+=hold&(1<<state.extra)-1,hold>>>=state.extra,bits-=state.extra,state.back+=state.extra}state.was=state.length,state.mode=23;case 23:for(;here=state.distcode[hold&(1<<state.distbits)-1],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(0==(240&here_op)){for(last_bits=here_bits,last_op=here_op,last_val=here_val;here=state.distcode[last_val+((hold&(1<<last_bits+last_op)-1)>>last_bits)],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(last_bits+here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}hold>>>=last_bits,bits-=last_bits,state.back+=last_bits}if(hold>>>=here_bits,bits-=here_bits,state.back+=here_bits,64&here_op){strm.msg=\"invalid distance code\",state.mode=30;break}state.offset=here_val,state.extra=15&here_op,state.mode=24;case 24:if(state.extra){for(n=state.extra;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.offset+=hold&(1<<state.extra)-1,hold>>>=state.extra,bits-=state.extra,state.back+=state.extra}if(state.offset>state.dmax){strm.msg=\"invalid distance too far back\",state.mode=30;break}state.mode=25;case 25:if(0===left)break inf_leave;if(copy=_out-left,state.offset>copy){if(copy=state.offset-copy,copy>state.whave&&state.sane){strm.msg=\"invalid distance too far back\",state.mode=30;break}copy>state.wnext?(copy-=state.wnext,from=state.wsize-copy):from=state.wnext-copy,copy>state.length&&(copy=state.length),from_source=state.window}else from_source=output,from=put-state.offset,copy=state.length;copy>left&&(copy=left),left-=copy,state.length-=copy;do{output[put++]=from_source[from++]}while(--copy);0===state.length&&(state.mode=21);break;case 26:if(0===left)break inf_leave;output[put++]=state.length,left--,state.mode=21;break;case 27:if(state.wrap){for(;bits<32;){if(0===have)break inf_leave;have--,hold|=input[next++]<<bits,bits+=8}if(_out-=left,strm.total_out+=_out,state.total+=_out,_out&&(strm.adler=state.check=state.flags?crc32(state.check,output,_out,put-_out):adler32(state.check,output,_out,put-_out)),_out=left,(state.flags?hold:zswap32(hold))!==state.check){strm.msg=\"incorrect data check\",state.mode=30;break}hold=0,bits=0}state.mode=28;case 28:if(state.wrap&&state.flags){for(;bits<32;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(hold!==(4294967295&state.total)){strm.msg=\"incorrect length check\",state.mode=30;break}hold=0,bits=0}state.mode=29;case 29:ret=Z_STREAM_END;break inf_leave;case 30:ret=Z_DATA_ERROR;break inf_leave;case 31:return Z_MEM_ERROR;case 32:default:return Z_STREAM_ERROR}return strm.next_out=put,strm.avail_out=left,strm.next_in=next,strm.avail_in=have,state.hold=hold,state.bits=bits,(state.wsize||_out!==strm.avail_out&&state.mode<30&&(state.mode<27||flush!==Z_FINISH))&&updatewindow(strm,strm.output,strm.next_out,_out-strm.avail_out)?(state.mode=31,Z_MEM_ERROR):(_in-=strm.avail_in,_out-=strm.avail_out,strm.total_in+=_in,strm.total_out+=_out,state.total+=_out,state.wrap&&_out&&(strm.adler=state.check=state.flags?crc32(state.check,output,_out,strm.next_out-_out):adler32(state.check,output,_out,strm.next_out-_out)),strm.data_type=state.bits+(state.last?64:0)+(12===state.mode?128:0)+(20===state.mode||15===state.mode?256:0),(0===_in&&0===_out||flush===Z_FINISH)&&ret===Z_OK&&(ret=Z_BUF_ERROR),ret)},module.exports.inflateEnd=strm=>{if(!strm||!strm.state)return Z_STREAM_ERROR;let state=strm.state;return state.window&&(state.window=null),strm.state=null,Z_OK},module.exports.inflateGetHeader=(strm,head)=>{if(!strm||!strm.state)return Z_STREAM_ERROR;const state=strm.state;return 0==(2&state.wrap)?Z_STREAM_ERROR:(state.head=head,head.done=!1,Z_OK)},module.exports.inflateSetDictionary=(strm,dictionary)=>{const dictLength=dictionary.length;let state,dictid,ret;return strm&&strm.state?(state=strm.state,0!==state.wrap&&11!==state.mode?Z_STREAM_ERROR:11===state.mode&&(dictid=1,dictid=adler32(dictid,dictionary,dictLength,0),dictid!==state.check)?Z_DATA_ERROR:(ret=updatewindow(strm,dictionary,dictLength,dictLength),ret?(state.mode=31,Z_MEM_ERROR):(state.havedict=1,Z_OK))):Z_STREAM_ERROR},module.exports.inflateInfo=\"pako inflate (from Nodeca project)\"},241:module=>{\"use strict\";const lbase=new Uint16Array([3,4,5,6,7,8,9,10,11,13,15,17,19,23,27,31,35,43,51,59,67,83,99,115,131,163,195,227,258,0,0]),lext=new Uint8Array([16,16,16,16,16,16,16,16,17,17,17,17,18,18,18,18,19,19,19,19,20,20,20,20,21,21,21,21,16,72,78]),dbase=new Uint16Array([1,2,3,4,5,7,9,13,17,25,33,49,65,97,129,193,257,385,513,769,1025,1537,2049,3073,4097,6145,8193,12289,16385,24577,0,0]),dext=new Uint8Array([16,16,16,16,17,17,18,18,19,19,20,20,21,21,22,22,23,23,24,24,25,25,26,26,27,27,28,28,29,29,64,64]);module.exports=(type,lens,lens_index,codes,table,table_index,work,opts)=>{const bits=opts.bits;let incr,fill,low,mask,next,end,len=0,sym=0,min=0,max=0,root=0,curr=0,drop=0,left=0,used=0,huff=0,base=null,base_index=0;const count=new Uint16Array(16),offs=new Uint16Array(16);let here_bits,here_op,here_val,extra=null,extra_index=0;for(len=0;len<=15;len++)count[len]=0;for(sym=0;sym<codes;sym++)count[lens[lens_index+sym]]++;for(root=bits,max=15;max>=1&&0===count[max];max--);if(root>max&&(root=max),0===max)return table[table_index++]=20971520,table[table_index++]=20971520,opts.bits=1,0;for(min=1;min<max&&0===count[min];min++);for(root<min&&(root=min),left=1,len=1;len<=15;len++)if(left<<=1,left-=count[len],left<0)return-1;if(left>0&&(0===type||1!==max))return-1;for(offs[1]=0,len=1;len<15;len++)offs[len+1]=offs[len]+count[len];for(sym=0;sym<codes;sym++)0!==lens[lens_index+sym]&&(work[offs[lens[lens_index+sym]]++]=sym);if(0===type?(base=extra=work,end=19):1===type?(base=lbase,base_index-=257,extra=lext,extra_index-=257,end=256):(base=dbase,extra=dext,end=-1),huff=0,sym=0,len=min,next=table_index,curr=root,drop=0,low=-1,used=1<<root,mask=used-1,1===type&&used>852||2===type&&used>592)return 1;for(;;){here_bits=len-drop,work[sym]<end?(here_op=0,here_val=work[sym]):work[sym]>end?(here_op=extra[extra_index+work[sym]],here_val=base[base_index+work[sym]]):(here_op=96,here_val=0),incr=1<<len-drop,fill=1<<curr,min=fill;do{fill-=incr,table[next+(huff>>drop)+fill]=here_bits<<24|here_op<<16|here_val|0}while(0!==fill);for(incr=1<<len-1;huff&incr;)incr>>=1;if(0!==incr?(huff&=incr-1,huff+=incr):huff=0,sym++,0==--count[len]){if(len===max)break;len=lens[lens_index+work[sym]]}if(len>root&&(huff&mask)!==low){for(0===drop&&(drop=root),next+=min,curr=len-drop,left=1<<curr;curr+drop<max&&(left-=count[curr+drop],!(left<=0));)curr++,left<<=1;if(used+=1<<curr,1===type&&used>852||2===type&&used>592)return 1;low=huff&mask,table[low]=root<<24|curr<<16|next-table_index|0}}return 0!==huff&&(table[next+huff]=len-drop<<24|64<<16|0),opts.bits=root,0}},898:module=>{\"use strict\";module.exports={2:\"need dictionary\",1:\"stream end\",0:\"\",\"-1\":\"file error\",\"-2\":\"stream error\",\"-3\":\"data error\",\"-4\":\"insufficient memory\",\"-5\":\"buffer error\",\"-6\":\"incompatible version\"}},292:module=>{\"use strict\";module.exports=function ZStream(){this.input=null,this.next_in=0,this.avail_in=0,this.total_in=0,this.output=null,this.next_out=0,this.avail_out=0,this.total_out=0,this.msg=\"\",this.state=null,this.data_type=2,this.adler=0}},330:(__unused_webpack_module,__webpack_exports__,__webpack_require__)=>{\"use strict\";__webpack_require__.d(__webpack_exports__,{default:()=>__WEBPACK_DEFAULT_EXPORT__});var numpy_parser__WEBPACK_IMPORTED_MODULE_0__=__webpack_require__(829),ndarray__WEBPACK_IMPORTED_MODULE_1__=__webpack_require__(861),pako_lib_inflate__WEBPACK_IMPORTED_MODULE_2__=__webpack_require__(843);const __WEBPACK_DEFAULT_EXPORT__=loader={unpack_obj:function unpack_obj(obj){if(Array.isArray(obj)){var ret=[];for(var v of obj)ret.push(unpack_obj(v));return ret}if(obj instanceof Object){if(obj.hasOwnProperty(\"__type__\"))return function unpack_custom_data(obj){if(\"npy\"==obj.__type__){var uint8arr;if(window.obj=obj,obj.hasOwnProperty(\"zdata\")){const compressed=Uint8Array.from(window.atob(obj.zdata),(c=>c.charCodeAt(0)));uint8arr=(0,pako_lib_inflate__WEBPACK_IMPORTED_MODULE_2__.rr)(compressed)}else uint8arr=Uint8Array.from(window.atob(obj.data),(c=>c.charCodeAt(0)));var arr=(0,numpy_parser__WEBPACK_IMPORTED_MODULE_0__.g)(uint8arr.buffer);if(arr=ndarray__WEBPACK_IMPORTED_MODULE_1__(arr.data,arr.shape),obj.hasOwnProperty(\"min\")){let scale=\"uint8\"===arr.dtype?255:65535;for(var size=1,i=0;i<arr.shape.length;i++)size*=arr.shape[i];var arr_=ndarray__WEBPACK_IMPORTED_MODULE_1__(new Float32Array(size),arr.shape);for(i=0;i<arr.data.length;i++)arr_.data[i]=obj.min+(obj.max-obj.min)*arr.data[i]/scale;return arr_}return arr}return{}}(obj);ret={};for(var k of Object.keys(obj))ret[k]=unpack_obj(obj[k]);return ret}return obj}}}},__webpack_module_cache__={};function __webpack_require__(moduleId){if(__webpack_module_cache__[moduleId])return __webpack_module_cache__[moduleId].exports;var module=__webpack_module_cache__[moduleId]={exports:{}};return __webpack_modules__[moduleId](module,module.exports,__webpack_require__),module.exports}return __webpack_require__.n=module=>{var getter=module&&module.__esModule?()=>module.default:()=>module;return __webpack_require__.d(getter,{a:getter}),getter},__webpack_require__.d=(exports,definition)=>{for(var key in definition)__webpack_require__.o(definition,key)&&!__webpack_require__.o(exports,key)&&Object.defineProperty(exports,key,{enumerable:!0,get:definition[key]})},__webpack_require__.o=(obj,prop)=>Object.prototype.hasOwnProperty.call(obj,prop),__webpack_require__(330)})().default;</script>\n",[m
        "<script>var Hello;Hello=(()=>{\"use strict\";var __webpack_modules__={906:(__unused_webpack_module,__webpack_exports__,__webpack_require__)=>{function noop(){}__webpack_require__.d(__webpack_exports__,{default:()=>Hello_svelte});function run(fn){return fn()}function blank_object(){return Object.create(null)}function run_all(fns){fns.forEach(run)}function is_function(thing){return\"function\"==typeof thing}function safe_not_equal(a,b){return a!=a?b==b:a!==b||a&&\"object\"==typeof a||\"function\"==typeof a}function is_empty(obj){return 0===Object.keys(obj).length}new Set;function append(target,node){target.appendChild(node)}function insert(target,node,anchor){target.insertBefore(node,anchor||null)}function detach(node){node.parentNode.removeChild(node)}function internal_element(name){return document.createElement(name)}function internal_text(data){return document.createTextNode(data)}new Set;let current_component;function set_current_component(component){current_component=component}const dirty_components=[],binding_callbacks=[],render_callbacks=[],flush_callbacks=[],resolved_promise=Promise.resolve();let update_scheduled=!1;function schedule_update(){update_scheduled||(update_scheduled=!0,resolved_promise.then(flush))}function add_render_callback(fn){render_callbacks.push(fn)}let flushing=!1;const seen_callbacks=new Set;function flush(){if(!flushing){flushing=!0;do{for(let i=0;i<dirty_components.length;i+=1){const component=dirty_components[i];set_current_component(component),update(component.$$)}for(set_current_component(null),dirty_components.length=0;binding_callbacks.length;)binding_callbacks.pop()();for(let i=0;i<render_callbacks.length;i+=1){const callback=render_callbacks[i];seen_callbacks.has(callback)||(seen_callbacks.add(callback),callback())}render_callbacks.length=0}while(dirty_components.length);for(;flush_callbacks.length;)flush_callbacks.pop()();update_scheduled=!1,flushing=!1,seen_callbacks.clear()}}function update($$){if(null!==$$.fragment){$$.update(),run_all($$.before_update);const dirty=$$.dirty;$$.dirty=[-1],$$.fragment&&$$.fragment.p($$.ctx,dirty),$$.after_update.forEach(add_render_callback)}}const outroing=new Set;function transition_in(block,local){block&&block.i&&(outroing.delete(block),block.i(local))}\"undefined\"!=typeof window?window:\"undefined\"!=typeof globalThis?globalThis:global;new Set([\"allowfullscreen\",\"allowpaymentrequest\",\"async\",\"autofocus\",\"autoplay\",\"checked\",\"controls\",\"default\",\"defer\",\"disabled\",\"formnovalidate\",\"hidden\",\"ismap\",\"loop\",\"multiple\",\"muted\",\"nomodule\",\"novalidate\",\"open\",\"playsinline\",\"readonly\",\"required\",\"reversed\",\"selected\"]);let SvelteElement;function destroy_component(component,detaching){const $$=component.$$;null!==$$.fragment&&(run_all($$.on_destroy),$$.fragment&&$$.fragment.d(detaching),$$.on_destroy=$$.fragment=null,$$.ctx=[])}function init(component,options,instance,create_fragment,not_equal,props,dirty=[-1]){const parent_component=current_component;set_current_component(component);const prop_values=options.props||{},$$=component.$$={fragment:null,ctx:null,props,update:noop,not_equal,bound:blank_object(),on_mount:[],on_destroy:[],before_update:[],after_update:[],context:new Map(parent_component?parent_component.$$.context:[]),callbacks:blank_object(),dirty,skip_bound:!1};let ready=!1;if($$.ctx=instance?instance(component,prop_values,((i,ret,...rest)=>{const value=rest.length?rest[0]:ret;return $$.ctx&&not_equal($$.ctx[i],$$.ctx[i]=value)&&(!$$.skip_bound&&$$.bound[i]&&$$.bound[i](value),ready&&function make_dirty(component,i){-1===component.$$.dirty[0]&&(dirty_components.push(component),schedule_update(),component.$$.dirty.fill(0)),component.$$.dirty[i/31|0]|=1<<i%31}(component,i)),ret})):[],$$.update(),ready=!0,run_all($$.before_update),$$.fragment=!!create_fragment&&create_fragment($$.ctx),options.target){if(options.hydrate){const nodes=function children(element){return Array.from(element.childNodes)}(options.target);$$.fragment&&$$.fragment.l(nodes),nodes.forEach(detach)}else $$.fragment&&$$.fragment.c();options.intro&&transition_in(component.$$.fragment),function mount_component(component,target,anchor){const{fragment,on_mount,on_destroy,after_update}=component.$$;fragment&&fragment.m(target,anchor),add_render_callback((()=>{const new_on_destroy=on_mount.map(run).filter(is_function);on_destroy?on_destroy.push(...new_on_destroy):run_all(new_on_destroy),component.$$.on_mount=[]})),after_update.forEach(add_render_callback)}(component,options.target,options.anchor),flush()}set_current_component(parent_component)}\"function\"==typeof HTMLElement&&(SvelteElement=class extends HTMLElement{constructor(){super(),this.attachShadow({mode:\"open\"})}connectedCallback(){for(const key in this.$$.slotted)this.appendChild(this.$$.slotted[key])}attributeChangedCallback(attr,_oldValue,newValue){this[attr]=newValue}$destroy(){destroy_component(this,1),this.$destroy=noop}$on(type,callback){const callbacks=this.$$.callbacks[type]||(this.$$.callbacks[type]=[]);return callbacks.push(callback),()=>{const index=callbacks.indexOf(callback);-1!==index&&callbacks.splice(index,1)}}$set($$props){this.$$set&&!is_empty($$props)&&(this.$$.skip_bound=!0,this.$$set($$props),this.$$.skip_bound=!1)}});class SvelteComponent{$destroy(){destroy_component(this,1),this.$destroy=noop}$on(type,callback){const callbacks=this.$$.callbacks[type]||(this.$$.callbacks[type]=[]);return callbacks.push(callback),()=>{const index=callbacks.indexOf(callback);-1!==index&&callbacks.splice(index,1)}}$set($$props){this.$$set&&!is_empty($$props)&&(this.$$.skip_bound=!0,this.$$set($$props),this.$$.skip_bound=!1)}}function create_fragment(ctx){let h2,t0,t1,t2;return{c(){h2=internal_element(\"h2\"),t0=internal_text(\"Hello \"),t1=internal_text(ctx[0]),t2=internal_text(\"!\")},m(target,anchor){insert(target,h2,anchor),append(h2,t0),append(h2,t1),append(h2,t2)},p(ctx,[dirty]){1&dirty&&function set_data(text,data){data=\"\"+data,text.wholeText!==data&&(text.data=data)}(t1,ctx[0])},i:noop,o:noop,d(detaching){detaching&&detach(h2)}}}function instance($$self,$$props,$$invalidate){let{name}=$$props;return $$self.$$set=$$props=>{\"name\"in $$props&&$$invalidate(0,name=$$props.name)},[name]}const Hello_svelte=class Hello extends SvelteComponent{constructor(options){super(),init(this,options,instance,create_fragment,safe_not_equal,{name:0})}}}},__webpack_module_cache__={};function __webpack_require__(moduleId){if(__webpack_module_cache__[moduleId])return __webpack_module_cache__[moduleId].exports;var module=__webpack_module_cache__[moduleId]={exports:{}};return __webpack_modules__[moduleId](module,module.exports,__webpack_require__),module.exports}return __webpack_require__.d=(exports,definition)=>{for(var key in definition)__webpack_require__.o(definition,key)&&!__webpack_require__.o(exports,key)&&Object.defineProperty(exports,key,{enumerable:!0,get:definition[key]})},__webpack_require__.o=(obj,prop)=>Object.prototype.hasOwnProperty.call(obj,prop),__webpack_require__(906)})().default;</script>\n",[m
        "        \n",[m
[31m-       "        <div id=\"Hello_407258a\"></div>\n",[m
[32m+[m[32m       "        <div id=\"Hello_2b6cd2b\"></div>\n",[m
        "        <script>\n",[m
        "        ( () => {\n",[m
        "            var data = {\n",[m
[36m@@ -3372,7 +2379,7 @@[m
        "            data = loader.unpack_obj(data);\n",[m
        "            window.Hello_data = data;\n",[m
        "            var Hello_inst = new Hello({\n",[m
[31m-       "                \"target\": document.getElementById(\"Hello_407258a\"),\n",[m
[32m+[m[32m       "                \"target\": document.getElementById(\"Hello_2b6cd2b\"),\n",[m
        "                \"props\": data\n",[m
        "                });\n",[m
        "        })();\n",[m
[36m@@ -3398,7 +2405,7 @@[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "###Visualising Attention Patterns\n",[m
[32m+[m[32m    "### Visualising Attention Patterns\n",[m
     "\n",[m
     "A component to visualise attention patterns over some text. This is a particularly hard problem, as attention patterns are rank 3 tensors - with a destination_pos, source_pos and num_heads dimension. \n",[m
     "\n",[m
[36m@@ -3411,41 +2418,15 @@[m
     "There's a toggle to flip the token view to show tokens attending TO the current token (ie, from later in the sequence attending back)"[m
    ][m
   },[m
[31m-  {[m
[31m-   "cell_type": "code",[m
[31m-   "execution_count": null,[m
[31m-   "metadata": {},[m
[31m-   "outputs": [[m
[31m-    {[m
[31m-     "data": {[m
[31m-      "text/plain": [[m
[31m-       "['123', '45', '678', '90']"[m
[31m-      ][m
[31m-     },[m
[31m-     "execution_count": null,[m
[31m-     "metadata": {},[m
[31m-     "output_type": "execute_result"[m
[31m-    }[m
[31m-   ],[m
[31m-   "source": [[m
[31m-    "def text_to_token_strings(text):\n",[m
[31m-    "    # Extremely hacky function to convert text into a list of each token (as text, not as a token index)\n",[m
[31m-    "    return model.tokenizer.batch_decode(model.tokenizer.encode(text), clean_up_tokenization_spaces=False)\n",[m
[31m-    "text_to_token_strings('1234567890')"[m
[31m-   ][m
[31m-  },[m
   {[m
    "cell_type": "code",[m
    "execution_count": null,[m
    "metadata": {},[m
    "outputs": [],[m
    "source": [[m
[31m-    "model = EasyTransformer('gpt2')\n",[m
[31m-    "model.reset_hooks()\n",[m
[31m-    "vis_cache = {}\n",[m
[31m-    "model.cache_all(vis_cache)\n",[m
[32m+[m[32m    "model = EasyTransformer.from_pretrained('gpt2')\n",[m
     "vis_text = \"Help, I live in three dimensions but need to interact with models with too many dimensions!! Help, I live in three dimensions but need to interact with models with too many dimensions!\"\n",[m
[31m-    "logits = model(vis_text)"[m
[32m+[m[32m    "logits, vis_cache = model.run_with_cache(vis_text)"[m
    ][m
   },[m
   {[m
[36m@@ -3458,7 +2439,34 @@[m
      "output_type": "stream",[m
      "text": [[m
       "pysvelte components appear to be unbuilt or stale\n",[m
[31m-      "Building pysvelte components with webpack...\n"[m
[32m+[m[32m      "Building pysvelte components with webpack...\n",[m
[32m+[m[32m      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",[m
[32m+[m[32m      "To disable this warning, you can either:\n",[m
[32m+[m[32m      "\t- Avoid using `tokenizers` before the fork if possible\n",[m
[32m+[m[32m      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",[m
[32m+[m[32m      "entry: {\"loader\":\"./src/loader.js\",\"AttentionMulti\":\"./src/AttentionMulti.svelte\"}\n",[m
[32m+[m[32m      "asset loader.js 44.1 KiB [emitted] [minimized] (name: loader) 1 related asset\n",[m
[32m+[m[32m      "asset AttentionMulti.js 41.1 KiB [emitted] [minimized] (name: AttentionMulti)\n",[m
[32m+[m[32m      "orphan modules 68.2 KiB [orphan] 6 modules\n",[m
[32m+[m[32m      "runtime modules 1.03 KiB 5 modules\n",[m
[32m+[m[32m      "modules by path ./node_modules/ 116 KiB\n",[m
[32m+[m[32m      "  modules by path ./node_modules/pako/lib/ 102 KiB\n",[m
[32m+[m[32m      "    modules by path ./node_modules/pako/lib/zlib/*.js 84.7 KiB 9 modules\n",[m
[32m+[m[32m      "    modules by path ./node_modules/pako/lib/utils/*.js 5.62 KiB 2 modules\n",[m
[32m+[m[32m      "    ./node_modules/pako/lib/inflate.js 11.6 KiB [built] [code generated]\n",[m
[32m+[m[32m      "  ./node_modules/numpy-parser/dist/main.js 3.63 KiB [built] [code generated]\n",[m
[32m+[m[32m      "  ./node_modules/ndarray/ndarray.js 9.62 KiB [built] [code generated]\n",[m
[32m+[m[32m      "  ./node_modules/iota-array/iota.js 150 bytes [built] [code generated]\n",[m
[32m+[m[32m      "  ./node_modules/is-buffer/index.js 698 bytes [built] [code generated]\n",[m
[32m+[m[32m      "modules by path ./src/ 107 KiB\n",[m
[32m+[m[32m      "  ./src/loader.js 2.73 KiB [built] [code generated]\n",[m
[32m+[m[32m      "  ./src/AttentionMulti.svelte + 6 modules 105 KiB [built] [code generated]\n",[m
[32m+[m[32m      "\n",[m
[32m+[m[32m      "WARNING in configuration\n",[m
[32m+[m[32m      "The 'mode' option has not been set, webpack will fallback to 'production' for this value. Set 'mode' option to 'development' or 'production' to enable defaults for each environment.\n",[m
[32m+[m[32m      "You can also set it to 'none' to disable any default behavior. Learn more: https://webpack.js.org/configuration/mode/\n",[m
[32m+[m[32m      "\n",[m
[32m+[m[32m      "webpack 5.16.0 compiled with 1 warning in 1462 ms\n"[m
      ][m
     },[m
     {[m
[36m@@ -3469,11 +2477,12 @@[m
        "var loader;loader=(()=>{var __webpack_modules__={907:module=>{\"use strict\";module.exports=function iota(n){for(var result=new Array(n),i=0;i<n;++i)result[i]=i;return result}},738:module=>{function isBuffer(obj){return!!obj.constructor&&\"function\"==typeof obj.constructor.isBuffer&&obj.constructor.isBuffer(obj)}module.exports=function(obj){return null!=obj&&(isBuffer(obj)||function isSlowBuffer(obj){return\"function\"==typeof obj.readFloatLE&&\"function\"==typeof obj.slice&&isBuffer(obj.slice(0,0))}(obj)||!!obj._isBuffer)}},861:(module,__unused_webpack_exports,__webpack_require__)=>{var iota=__webpack_require__(907),isBuffer=__webpack_require__(738),hasTypedArrays=\"undefined\"!=typeof Float64Array;function compare1st(a,b){return a[0]-b[0]}function order(){var i,stride=this.stride,terms=new Array(stride.length);for(i=0;i<terms.length;++i)terms[i]=[Math.abs(stride[i]),i];terms.sort(compare1st);var result=new Array(terms.length);for(i=0;i<result.length;++i)result[i]=terms[i][1];return result}function compileConstructor(dtype,dimension){var className=[\"View\",dimension,\"d\",dtype].join(\"\");dimension<0&&(className=\"View_Nil\"+dtype);var useGetters=\"generic\"===dtype;if(-1===dimension){var code=\"function \"+className+\"(a){this.data=a;};var proto=\"+className+\".prototype;proto.dtype='\"+dtype+\"';proto.index=function(){return -1};proto.size=0;proto.dimension=-1;proto.shape=proto.stride=proto.order=[];proto.lo=proto.hi=proto.transpose=proto.step=function(){return new \"+className+\"(this.data);};proto.get=proto.set=function(){};proto.pick=function(){return null};return function construct_\"+className+\"(a){return new \"+className+\"(a);}\";return new Function(code)()}if(0===dimension){code=\"function \"+className+\"(a,d) {this.data = a;this.offset = d};var proto=\"+className+\".prototype;proto.dtype='\"+dtype+\"';proto.index=function(){return this.offset};proto.dimension=0;proto.size=1;proto.shape=proto.stride=proto.order=[];proto.lo=proto.hi=proto.transpose=proto.step=function \"+className+\"_copy() {return new \"+className+\"(this.data,this.offset)};proto.pick=function \"+className+\"_pick(){return TrivialArray(this.data);};proto.valueOf=proto.get=function \"+className+\"_get(){return \"+(useGetters?\"this.data.get(this.offset)\":\"this.data[this.offset]\")+\"};proto.set=function \"+className+\"_set(v){return \"+(useGetters?\"this.data.set(this.offset,v)\":\"this.data[this.offset]=v\")+\"};return function construct_\"+className+\"(a,b,c,d){return new \"+className+\"(a,d)}\";return new Function(\"TrivialArray\",code)(CACHED_CONSTRUCTORS[dtype][0])}code=[\"'use strict'\"];var indices=iota(dimension),args=indices.map((function(i){return\"i\"+i})),index_str=\"this.offset+\"+indices.map((function(i){return\"this.stride[\"+i+\"]*i\"+i})).join(\"+\"),shapeArg=indices.map((function(i){return\"b\"+i})).join(\",\"),strideArg=indices.map((function(i){return\"c\"+i})).join(\",\");code.push(\"function \"+className+\"(a,\"+shapeArg+\",\"+strideArg+\",d){this.data=a\",\"this.shape=[\"+shapeArg+\"]\",\"this.stride=[\"+strideArg+\"]\",\"this.offset=d|0}\",\"var proto=\"+className+\".prototype\",\"proto.dtype='\"+dtype+\"'\",\"proto.dimension=\"+dimension),code.push(\"Object.defineProperty(proto,'size',{get:function \"+className+\"_size(){return \"+indices.map((function(i){return\"this.shape[\"+i+\"]\"})).join(\"*\"),\"}})\"),1===dimension?code.push(\"proto.order=[0]\"):(code.push(\"Object.defineProperty(proto,'order',{get:\"),dimension<4?(code.push(\"function \"+className+\"_order(){\"),2===dimension?code.push(\"return (Math.abs(this.stride[0])>Math.abs(this.stride[1]))?[1,0]:[0,1]}})\"):3===dimension&&code.push(\"var s0=Math.abs(this.stride[0]),s1=Math.abs(this.stride[1]),s2=Math.abs(this.stride[2]);if(s0>s1){if(s1>s2){return [2,1,0];}else if(s0>s2){return [1,2,0];}else{return [1,0,2];}}else if(s0>s2){return [2,0,1];}else if(s2>s1){return [0,1,2];}else{return [0,2,1];}}})\")):code.push(\"ORDER})\")),code.push(\"proto.set=function \"+className+\"_set(\"+args.join(\",\")+\",v){\"),useGetters?code.push(\"return this.data.set(\"+index_str+\",v)}\"):code.push(\"return this.data[\"+index_str+\"]=v}\"),code.push(\"proto.get=function \"+className+\"_get(\"+args.join(\",\")+\"){\"),useGetters?code.push(\"return this.data.get(\"+index_str+\")}\"):code.push(\"return this.data[\"+index_str+\"]}\"),code.push(\"proto.index=function \"+className+\"_index(\",args.join(),\"){return \"+index_str+\"}\"),code.push(\"proto.hi=function \"+className+\"_hi(\"+args.join(\",\")+\"){return new \"+className+\"(this.data,\"+indices.map((function(i){return[\"(typeof i\",i,\"!=='number'||i\",i,\"<0)?this.shape[\",i,\"]:i\",i,\"|0\"].join(\"\")})).join(\",\")+\",\"+indices.map((function(i){return\"this.stride[\"+i+\"]\"})).join(\",\")+\",this.offset)}\");var a_vars=indices.map((function(i){return\"a\"+i+\"=this.shape[\"+i+\"]\"})),c_vars=indices.map((function(i){return\"c\"+i+\"=this.stride[\"+i+\"]\"}));code.push(\"proto.lo=function \"+className+\"_lo(\"+args.join(\",\")+\"){var b=this.offset,d=0,\"+a_vars.join(\",\")+\",\"+c_vars.join(\",\"));for(var i=0;i<dimension;++i)code.push(\"if(typeof i\"+i+\"==='number'&&i\"+i+\">=0){d=i\"+i+\"|0;b+=c\"+i+\"*d;a\"+i+\"-=d}\");code.push(\"return new \"+className+\"(this.data,\"+indices.map((function(i){return\"a\"+i})).join(\",\")+\",\"+indices.map((function(i){return\"c\"+i})).join(\",\")+\",b)}\"),code.push(\"proto.step=function \"+className+\"_step(\"+args.join(\",\")+\"){var \"+indices.map((function(i){return\"a\"+i+\"=this.shape[\"+i+\"]\"})).join(\",\")+\",\"+indices.map((function(i){return\"b\"+i+\"=this.stride[\"+i+\"]\"})).join(\",\")+\",c=this.offset,d=0,ceil=Math.ceil\");for(i=0;i<dimension;++i)code.push(\"if(typeof i\"+i+\"==='number'){d=i\"+i+\"|0;if(d<0){c+=b\"+i+\"*(a\"+i+\"-1);a\"+i+\"=ceil(-a\"+i+\"/d)}else{a\"+i+\"=ceil(a\"+i+\"/d)}b\"+i+\"*=d}\");code.push(\"return new \"+className+\"(this.data,\"+indices.map((function(i){return\"a\"+i})).join(\",\")+\",\"+indices.map((function(i){return\"b\"+i})).join(\",\")+\",c)}\");var tShape=new Array(dimension),tStride=new Array(dimension);for(i=0;i<dimension;++i)tShape[i]=\"a[i\"+i+\"]\",tStride[i]=\"b[i\"+i+\"]\";code.push(\"proto.transpose=function \"+className+\"_transpose(\"+args+\"){\"+args.map((function(n,idx){return n+\"=(\"+n+\"===undefined?\"+idx+\":\"+n+\"|0)\"})).join(\";\"),\"var a=this.shape,b=this.stride;return new \"+className+\"(this.data,\"+tShape.join(\",\")+\",\"+tStride.join(\",\")+\",this.offset)}\"),code.push(\"proto.pick=function \"+className+\"_pick(\"+args+\"){var a=[],b=[],c=this.offset\");for(i=0;i<dimension;++i)code.push(\"if(typeof i\"+i+\"==='number'&&i\"+i+\">=0){c=(c+this.stride[\"+i+\"]*i\"+i+\")|0}else{a.push(this.shape[\"+i+\"]);b.push(this.stride[\"+i+\"])}\");return code.push(\"var ctor=CTOR_LIST[a.length+1];return ctor(this.data,a,b,c)}\"),code.push(\"return function construct_\"+className+\"(data,shape,stride,offset){return new \"+className+\"(data,\"+indices.map((function(i){return\"shape[\"+i+\"]\"})).join(\",\")+\",\"+indices.map((function(i){return\"stride[\"+i+\"]\"})).join(\",\")+\",offset)}\"),new Function(\"CTOR_LIST\",\"ORDER\",code.join(\"\\n\"))(CACHED_CONSTRUCTORS[dtype],order)}var CACHED_CONSTRUCTORS={float32:[],float64:[],int8:[],int16:[],int32:[],uint8:[],uint16:[],uint32:[],array:[],uint8_clamped:[],bigint64:[],biguint64:[],buffer:[],generic:[]};module.exports=function wrappedNDArrayCtor(data,shape,stride,offset){if(void 0===data)return(0,CACHED_CONSTRUCTORS.array[0])([]);\"number\"==typeof data&&(data=[data]),void 0===shape&&(shape=[data.length]);var d=shape.length;if(void 0===stride){stride=new Array(d);for(var i=d-1,sz=1;i>=0;--i)stride[i]=sz,sz*=shape[i]}if(void 0===offset){offset=0;for(i=0;i<d;++i)stride[i]<0&&(offset-=(shape[i]-1)*stride[i])}for(var dtype=function arrayDType(data){if(isBuffer(data))return\"buffer\";if(hasTypedArrays)switch(Object.prototype.toString.call(data)){case\"[object Float64Array]\":return\"float64\";case\"[object Float32Array]\":return\"float32\";case\"[object Int8Array]\":return\"int8\";case\"[object Int16Array]\":return\"int16\";case\"[object Int32Array]\":return\"int32\";case\"[object Uint8Array]\":return\"uint8\";case\"[object Uint16Array]\":return\"uint16\";case\"[object Uint32Array]\":return\"uint32\";case\"[object Uint8ClampedArray]\":return\"uint8_clamped\";case\"[object BigInt64Array]\":return\"bigint64\";case\"[object BigUint64Array]\":return\"biguint64\"}return Array.isArray(data)?\"array\":\"generic\"}(data),ctor_list=CACHED_CONSTRUCTORS[dtype];ctor_list.length<=d+1;)ctor_list.push(compileConstructor(dtype,ctor_list.length-1));return(0,ctor_list[d+1])(data,shape,stride,offset)}},829:(__unused_webpack_module,exports)=>{\"use strict\";function _defineProperties(a,b){for(var c,d=0;d<b.length;d++)(c=b[d]).enumerable=c.enumerable||!1,c.configurable=!0,\"value\"in c&&(c.writable=!0),Object.defineProperty(a,c.key,c)}exports.g=function fromArrayBuffer(a){if(!a instanceof ArrayBuffer)throw new Error(\"Argument must be an ArrayBuffer.\");var b=new DataViewReader(a),c=b.readUint8(),d=b.readAndASCIIDecodeBytes(5);if(147!=c||\"NUMPY\"!=d)throw new Error('unknown file type: \"'.concat(c).concat(d,'\"'));var e,f=b.readUint8(),h=(b.readUint8(),10+(e=1>=f?b.readUint16(!0):b.readUint32(!0)));0!=h%16&&console.warn(\"NPY file header is incorrectly padded. (\".concat(h,\" is not evenly divisible by 16.)\"));var j=function parseHeaderStr(a){var b=a.toLowerCase().replace(\"(\",\"[\").replace(\"),\",\"]\").replace(\"[,\",\"[1,]\").replace(\",]\",\",1]\").replace(/'/g,'\"');return JSON.parse(b)}(b.readAndASCIIDecodeBytes(e));if(j.fortran_order)throw new Error(\"NPY file is written in Fortran byte order, support for this byte order is not yet implemented.\");return{data:new(typedArrayConstructorForDescription(j.descr))(a,b.offset),shape:j.shape}};var DataViewReader=function(){function a(b){(function _classCallCheck(a,b){if(!(a instanceof b))throw new TypeError(\"Cannot call a class as a function\")})(this,a),b instanceof DataView?this.dataView=b:b instanceof ArrayBuffer&&(this.dataView=new DataView(b)),this.offset=0}return function _createClass(a,b,c){return b&&_defineProperties(a.prototype,b),c&&_defineProperties(a,c),a}(a,[{key:\"readBytes\",value:function c(a){var b=new DataView(this.dataView.buffer,this.offset,a);return this.offset+=a,b}},{key:\"readAndASCIIDecodeBytes\",value:function c(a){var b=new Uint8Array(this.dataView.buffer,this.offset,a);return this.offset+=a,this._decodeASCIIByteArray(b)}},{key:\"readUint8\",value:function c(){var a=!!(0<arguments.length&&void 0!==arguments[0])&&arguments[0],b=this.dataView.getUint8(this.offset,a);return this.offset+=Uint8Array.BYTES_PER_ELEMENT,b}},{key:\"readUint16\",value:function c(){var a=!!(0<arguments.length&&void 0!==arguments[0])&&arguments[0],b=this.dataView.getUint16(this.offset,a);return this.offset+=Uint16Array.BYTES_PER_ELEMENT,b}},{key:\"readUint32\",value:function c(){var a=!!(0<arguments.length&&void 0!==arguments[0])&&arguments[0],b=this.dataView.getUint32(this.offset,a);return this.offset+=Uint32Array.BYTES_PER_ELEMENT,b}},{key:\"_decodeASCIIByteArray\",value:function k(a){var b=String.fromCharCode,c=[],d=!0,e=!1,f=void 0;try{for(var g,h=a[Symbol.iterator]();!(d=(g=h.next()).done);d=!0){var j=b(g.value);c.push(j)}}catch(a){e=!0,f=a}finally{try{d||null==h.return||h.return()}finally{if(e)throw f}}return c.join(\"\")}}]),a}();function typedArrayConstructorForDescription(a){switch(a){case\"|u1\":return Uint8Array;case\"<u2\":return Uint16Array;case\"<u4\":return Uint32Array;case\"<u8\":throw new Error(\"Because JavaScript doesn't currently include standard support for 64-bit unsigned integer values, support for this dtype is not yet implemented.\");case\"|i1\":return Int8Array;case\"<i2\":return Int16Array;case\"<i4\":return Int32Array;case\"<i8\":throw new Error(\"Because JavaScript doesn't currently include standard support for 64-bit integer values, support for this dtype is not yet implemented.\");case\"<f2\":throw new Error(\"Because JavaScript doesn't currently include standard support for 16-bit floating point values, support for this dtype is not yet implemented.\");case\"<f4\":return Float32Array;case\"<f8\":return Float64Array;default:throw new Error(\"Unknown or not yet implemented numpy dtype description: \"+dtype)}}},843:(module,__unused_webpack_exports,__webpack_require__)=>{\"use strict\";const zlib_inflate=__webpack_require__(948),utils=__webpack_require__(236),strings=__webpack_require__(373),msg=__webpack_require__(898),ZStream=__webpack_require__(292),GZheader=__webpack_require__(401),toString=Object.prototype.toString,{Z_NO_FLUSH,Z_FINISH,Z_OK,Z_STREAM_END,Z_NEED_DICT,Z_STREAM_ERROR,Z_DATA_ERROR,Z_MEM_ERROR}=__webpack_require__(619);function Inflate(options){this.options=utils.assign({chunkSize:65536,windowBits:15,to:\"\"},options||{});const opt=this.options;opt.raw&&opt.windowBits>=0&&opt.windowBits<16&&(opt.windowBits=-opt.windowBits,0===opt.windowBits&&(opt.windowBits=-15)),!(opt.windowBits>=0&&opt.windowBits<16)||options&&options.windowBits||(opt.windowBits+=32),opt.windowBits>15&&opt.windowBits<48&&0==(15&opt.windowBits)&&(opt.windowBits|=15),this.err=0,this.msg=\"\",this.ended=!1,this.chunks=[],this.strm=new ZStream,this.strm.avail_out=0;let status=zlib_inflate.inflateInit2(this.strm,opt.windowBits);if(status!==Z_OK)throw new Error(msg[status]);if(this.header=new GZheader,zlib_inflate.inflateGetHeader(this.strm,this.header),opt.dictionary&&(\"string\"==typeof opt.dictionary?opt.dictionary=strings.string2buf(opt.dictionary):\"[object ArrayBuffer]\"===toString.call(opt.dictionary)&&(opt.dictionary=new Uint8Array(opt.dictionary)),opt.raw&&(status=zlib_inflate.inflateSetDictionary(this.strm,opt.dictionary),status!==Z_OK)))throw new Error(msg[status])}function inflate(input,options){const inflator=new Inflate(options);if(inflator.push(input),inflator.err)throw inflator.msg||msg[inflator.err];return inflator.result}Inflate.prototype.push=function(data,flush_mode){const strm=this.strm,chunkSize=this.options.chunkSize,dictionary=this.options.dictionary;let status,_flush_mode,last_avail_out;if(this.ended)return!1;for(_flush_mode=flush_mode===~~flush_mode?flush_mode:!0===flush_mode?Z_FINISH:Z_NO_FLUSH,\"[object ArrayBuffer]\"===toString.call(data)?strm.input=new Uint8Array(data):strm.input=data,strm.next_in=0,strm.avail_in=strm.input.length;;){for(0===strm.avail_out&&(strm.output=new Uint8Array(chunkSize),strm.next_out=0,strm.avail_out=chunkSize),status=zlib_inflate.inflate(strm,_flush_mode),status===Z_NEED_DICT&&dictionary&&(status=zlib_inflate.inflateSetDictionary(strm,dictionary),status===Z_OK?status=zlib_inflate.inflate(strm,_flush_mode):status===Z_DATA_ERROR&&(status=Z_NEED_DICT));strm.avail_in>0&&status===Z_STREAM_END&&strm.state.wrap>0&&0!==data[strm.next_in];)zlib_inflate.inflateReset(strm),status=zlib_inflate.inflate(strm,_flush_mode);switch(status){case Z_STREAM_ERROR:case Z_DATA_ERROR:case Z_NEED_DICT:case Z_MEM_ERROR:return this.onEnd(status),this.ended=!0,!1}if(last_avail_out=strm.avail_out,strm.next_out&&(0===strm.avail_out||status===Z_STREAM_END))if(\"string\"===this.options.to){let next_out_utf8=strings.utf8border(strm.output,strm.next_out),tail=strm.next_out-next_out_utf8,utf8str=strings.buf2string(strm.output,next_out_utf8);strm.next_out=tail,strm.avail_out=chunkSize-tail,tail&&strm.output.set(strm.output.subarray(next_out_utf8,next_out_utf8+tail),0),this.onData(utf8str)}else this.onData(strm.output.length===strm.next_out?strm.output:strm.output.subarray(0,strm.next_out));if(status!==Z_OK||0!==last_avail_out){if(status===Z_STREAM_END)return status=zlib_inflate.inflateEnd(this.strm),this.onEnd(status),this.ended=!0,!0;if(0===strm.avail_in)break}}return!0},Inflate.prototype.onData=function(chunk){this.chunks.push(chunk)},Inflate.prototype.onEnd=function(status){status===Z_OK&&(\"string\"===this.options.to?this.result=this.chunks.join(\"\"):this.result=utils.flattenChunks(this.chunks)),this.chunks=[],this.err=status,this.msg=this.strm.msg},module.exports.rr=inflate,__webpack_require__(619)},236:module=>{\"use strict\";const _has=(obj,key)=>Object.prototype.hasOwnProperty.call(obj,key);module.exports.assign=function(obj){const sources=Array.prototype.slice.call(arguments,1);for(;sources.length;){const source=sources.shift();if(source){if(\"object\"!=typeof source)throw new TypeError(source+\"must be non-object\");for(const p in source)_has(source,p)&&(obj[p]=source[p])}}return obj},module.exports.flattenChunks=chunks=>{let len=0;for(let i=0,l=chunks.length;i<l;i++)len+=chunks[i].length;const result=new Uint8Array(len);for(let i=0,pos=0,l=chunks.length;i<l;i++){let chunk=chunks[i];result.set(chunk,pos),pos+=chunk.length}return result}},373:module=>{\"use strict\";let STR_APPLY_UIA_OK=!0;try{String.fromCharCode.apply(null,new Uint8Array(1))}catch(__){STR_APPLY_UIA_OK=!1}const _utf8len=new Uint8Array(256);for(let q=0;q<256;q++)_utf8len[q]=q>=252?6:q>=248?5:q>=240?4:q>=224?3:q>=192?2:1;_utf8len[254]=_utf8len[254]=1,module.exports.string2buf=str=>{let buf,c,c2,m_pos,i,str_len=str.length,buf_len=0;for(m_pos=0;m_pos<str_len;m_pos++)c=str.charCodeAt(m_pos),55296==(64512&c)&&m_pos+1<str_len&&(c2=str.charCodeAt(m_pos+1),56320==(64512&c2)&&(c=65536+(c-55296<<10)+(c2-56320),m_pos++)),buf_len+=c<128?1:c<2048?2:c<65536?3:4;for(buf=new Uint8Array(buf_len),i=0,m_pos=0;i<buf_len;m_pos++)c=str.charCodeAt(m_pos),55296==(64512&c)&&m_pos+1<str_len&&(c2=str.charCodeAt(m_pos+1),56320==(64512&c2)&&(c=65536+(c-55296<<10)+(c2-56320),m_pos++)),c<128?buf[i++]=c:c<2048?(buf[i++]=192|c>>>6,buf[i++]=128|63&c):c<65536?(buf[i++]=224|c>>>12,buf[i++]=128|c>>>6&63,buf[i++]=128|63&c):(buf[i++]=240|c>>>18,buf[i++]=128|c>>>12&63,buf[i++]=128|c>>>6&63,buf[i++]=128|63&c);return buf};module.exports.buf2string=(buf,max)=>{let i,out;const len=max||buf.length,utf16buf=new Array(2*len);for(out=0,i=0;i<len;){let c=buf[i++];if(c<128){utf16buf[out++]=c;continue}let c_len=_utf8len[c];if(c_len>4)utf16buf[out++]=65533,i+=c_len-1;else{for(c&=2===c_len?31:3===c_len?15:7;c_len>1&&i<len;)c=c<<6|63&buf[i++],c_len--;c_len>1?utf16buf[out++]=65533:c<65536?utf16buf[out++]=c:(c-=65536,utf16buf[out++]=55296|c>>10&1023,utf16buf[out++]=56320|1023&c)}}return((buf,len)=>{if(len<65534&&buf.subarray&&STR_APPLY_UIA_OK)return String.fromCharCode.apply(null,buf.length===len?buf:buf.subarray(0,len));let result=\"\";for(let i=0;i<len;i++)result+=String.fromCharCode(buf[i]);return result})(utf16buf,out)},module.exports.utf8border=(buf,max)=>{(max=max||buf.length)>buf.length&&(max=buf.length);let pos=max-1;for(;pos>=0&&128==(192&buf[pos]);)pos--;return pos<0||0===pos?max:pos+_utf8len[buf[pos]]>max?pos:max}},69:module=>{\"use strict\";module.exports=(adler,buf,len,pos)=>{let s1=65535&adler|0,s2=adler>>>16&65535|0,n=0;for(;0!==len;){n=len>2e3?2e3:len,len-=n;do{s1=s1+buf[pos++]|0,s2=s2+s1|0}while(--n);s1%=65521,s2%=65521}return s1|s2<<16|0}},619:module=>{\"use strict\";module.exports={Z_NO_FLUSH:0,Z_PARTIAL_FLUSH:1,Z_SYNC_FLUSH:2,Z_FULL_FLUSH:3,Z_FINISH:4,Z_BLOCK:5,Z_TREES:6,Z_OK:0,Z_STREAM_END:1,Z_NEED_DICT:2,Z_ERRNO:-1,Z_STREAM_ERROR:-2,Z_DATA_ERROR:-3,Z_MEM_ERROR:-4,Z_BUF_ERROR:-5,Z_NO_COMPRESSION:0,Z_BEST_SPEED:1,Z_BEST_COMPRESSION:9,Z_DEFAULT_COMPRESSION:-1,Z_FILTERED:1,Z_HUFFMAN_ONLY:2,Z_RLE:3,Z_FIXED:4,Z_DEFAULT_STRATEGY:0,Z_BINARY:0,Z_TEXT:1,Z_UNKNOWN:2,Z_DEFLATED:8}},869:module=>{\"use strict\";const crcTable=new Uint32Array((()=>{let c,table=[];for(var n=0;n<256;n++){c=n;for(var k=0;k<8;k++)c=1&c?3988292384^c>>>1:c>>>1;table[n]=c}return table})());module.exports=(crc,buf,len,pos)=>{const t=crcTable,end=pos+len;crc^=-1;for(let i=pos;i<end;i++)crc=crc>>>8^t[255&(crc^buf[i])];return-1^crc}},401:module=>{\"use strict\";module.exports=function GZheader(){this.text=0,this.time=0,this.xflags=0,this.os=0,this.extra=null,this.extra_len=0,this.name=\"\",this.comment=\"\",this.hcrc=0,this.done=!1}},264:module=>{\"use strict\";module.exports=function inflate_fast(strm,start){let _in,last,_out,beg,end,dmax,wsize,whave,wnext,s_window,hold,bits,lcode,dcode,lmask,dmask,here,op,len,dist,from,from_source,input,output;const state=strm.state;_in=strm.next_in,input=strm.input,last=_in+(strm.avail_in-5),_out=strm.next_out,output=strm.output,beg=_out-(start-strm.avail_out),end=_out+(strm.avail_out-257),dmax=state.dmax,wsize=state.wsize,whave=state.whave,wnext=state.wnext,s_window=state.window,hold=state.hold,bits=state.bits,lcode=state.lencode,dcode=state.distcode,lmask=(1<<state.lenbits)-1,dmask=(1<<state.distbits)-1;top:do{bits<15&&(hold+=input[_in++]<<bits,bits+=8,hold+=input[_in++]<<bits,bits+=8),here=lcode[hold&lmask];dolen:for(;;){if(op=here>>>24,hold>>>=op,bits-=op,op=here>>>16&255,0===op)output[_out++]=65535&here;else{if(!(16&op)){if(0==(64&op)){here=lcode[(65535&here)+(hold&(1<<op)-1)];continue dolen}if(32&op){state.mode=12;break top}strm.msg=\"invalid literal/length code\",state.mode=30;break top}len=65535&here,op&=15,op&&(bits<op&&(hold+=input[_in++]<<bits,bits+=8),len+=hold&(1<<op)-1,hold>>>=op,bits-=op),bits<15&&(hold+=input[_in++]<<bits,bits+=8,hold+=input[_in++]<<bits,bits+=8),here=dcode[hold&dmask];dodist:for(;;){if(op=here>>>24,hold>>>=op,bits-=op,op=here>>>16&255,!(16&op)){if(0==(64&op)){here=dcode[(65535&here)+(hold&(1<<op)-1)];continue dodist}strm.msg=\"invalid distance code\",state.mode=30;break top}if(dist=65535&here,op&=15,bits<op&&(hold+=input[_in++]<<bits,bits+=8,bits<op&&(hold+=input[_in++]<<bits,bits+=8)),dist+=hold&(1<<op)-1,dist>dmax){strm.msg=\"invalid distance too far back\",state.mode=30;break top}if(hold>>>=op,bits-=op,op=_out-beg,dist>op){if(op=dist-op,op>whave&&state.sane){strm.msg=\"invalid distance too far back\",state.mode=30;break top}if(from=0,from_source=s_window,0===wnext){if(from+=wsize-op,op<len){len-=op;do{output[_out++]=s_window[from++]}while(--op);from=_out-dist,from_source=output}}else if(wnext<op){if(from+=wsize+wnext-op,op-=wnext,op<len){len-=op;do{output[_out++]=s_window[from++]}while(--op);if(from=0,wnext<len){op=wnext,len-=op;do{output[_out++]=s_window[from++]}while(--op);from=_out-dist,from_source=output}}}else if(from+=wnext-op,op<len){len-=op;do{output[_out++]=s_window[from++]}while(--op);from=_out-dist,from_source=output}for(;len>2;)output[_out++]=from_source[from++],output[_out++]=from_source[from++],output[_out++]=from_source[from++],len-=3;len&&(output[_out++]=from_source[from++],len>1&&(output[_out++]=from_source[from++]))}else{from=_out-dist;do{output[_out++]=output[from++],output[_out++]=output[from++],output[_out++]=output[from++],len-=3}while(len>2);len&&(output[_out++]=output[from++],len>1&&(output[_out++]=output[from++]))}break}}break}}while(_in<last&&_out<end);len=bits>>3,_in-=len,bits-=len<<3,hold&=(1<<bits)-1,strm.next_in=_in,strm.next_out=_out,strm.avail_in=_in<last?last-_in+5:5-(_in-last),strm.avail_out=_out<end?end-_out+257:257-(_out-end),state.hold=hold,state.bits=bits}},948:(module,__unused_webpack_exports,__webpack_require__)=>{\"use strict\";const adler32=__webpack_require__(69),crc32=__webpack_require__(869),inflate_fast=__webpack_require__(264),inflate_table=__webpack_require__(241),{Z_FINISH,Z_BLOCK,Z_TREES,Z_OK,Z_STREAM_END,Z_NEED_DICT,Z_STREAM_ERROR,Z_DATA_ERROR,Z_MEM_ERROR,Z_BUF_ERROR,Z_DEFLATED}=__webpack_require__(619),zswap32=q=>(q>>>24&255)+(q>>>8&65280)+((65280&q)<<8)+((255&q)<<24);function InflateState(){this.mode=0,this.last=!1,this.wrap=0,this.havedict=!1,this.flags=0,this.dmax=0,this.check=0,this.total=0,this.head=null,this.wbits=0,this.wsize=0,this.whave=0,this.wnext=0,this.window=null,this.hold=0,this.bits=0,this.length=0,this.offset=0,this.extra=0,this.lencode=null,this.distcode=null,this.lenbits=0,this.distbits=0,this.ncode=0,this.nlen=0,this.ndist=0,this.have=0,this.next=null,this.lens=new Uint16Array(320),this.work=new Uint16Array(288),this.lendyn=null,this.distdyn=null,this.sane=0,this.back=0,this.was=0}const inflateResetKeep=strm=>{if(!strm||!strm.state)return Z_STREAM_ERROR;const state=strm.state;return strm.total_in=strm.total_out=state.total=0,strm.msg=\"\",state.wrap&&(strm.adler=1&state.wrap),state.mode=1,state.last=0,state.havedict=0,state.dmax=32768,state.head=null,state.hold=0,state.bits=0,state.lencode=state.lendyn=new Int32Array(852),state.distcode=state.distdyn=new Int32Array(592),state.sane=1,state.back=-1,Z_OK},inflateReset=strm=>{if(!strm||!strm.state)return Z_STREAM_ERROR;const state=strm.state;return state.wsize=0,state.whave=0,state.wnext=0,inflateResetKeep(strm)},inflateReset2=(strm,windowBits)=>{let wrap;if(!strm||!strm.state)return Z_STREAM_ERROR;const state=strm.state;return windowBits<0?(wrap=0,windowBits=-windowBits):(wrap=1+(windowBits>>4),windowBits<48&&(windowBits&=15)),windowBits&&(windowBits<8||windowBits>15)?Z_STREAM_ERROR:(null!==state.window&&state.wbits!==windowBits&&(state.window=null),state.wrap=wrap,state.wbits=windowBits,inflateReset(strm))},inflateInit2=(strm,windowBits)=>{if(!strm)return Z_STREAM_ERROR;const state=new InflateState;strm.state=state,state.window=null;const ret=inflateReset2(strm,windowBits);return ret!==Z_OK&&(strm.state=null),ret};let lenfix,distfix,virgin=!0;const fixedtables=state=>{if(virgin){lenfix=new Int32Array(512),distfix=new Int32Array(32);let sym=0;for(;sym<144;)state.lens[sym++]=8;for(;sym<256;)state.lens[sym++]=9;for(;sym<280;)state.lens[sym++]=7;for(;sym<288;)state.lens[sym++]=8;for(inflate_table(1,state.lens,0,288,lenfix,0,state.work,{bits:9}),sym=0;sym<32;)state.lens[sym++]=5;inflate_table(2,state.lens,0,32,distfix,0,state.work,{bits:5}),virgin=!1}state.lencode=lenfix,state.lenbits=9,state.distcode=distfix,state.distbits=5},updatewindow=(strm,src,end,copy)=>{let dist;const state=strm.state;return null===state.window&&(state.wsize=1<<state.wbits,state.wnext=0,state.whave=0,state.window=new Uint8Array(state.wsize)),copy>=state.wsize?(state.window.set(src.subarray(end-state.wsize,end),0),state.wnext=0,state.whave=state.wsize):(dist=state.wsize-state.wnext,dist>copy&&(dist=copy),state.window.set(src.subarray(end-copy,end-copy+dist),state.wnext),(copy-=dist)?(state.window.set(src.subarray(end-copy,end),0),state.wnext=copy,state.whave=state.wsize):(state.wnext+=dist,state.wnext===state.wsize&&(state.wnext=0),state.whave<state.wsize&&(state.whave+=dist))),0};module.exports.inflateReset=inflateReset,module.exports.inflateReset2=inflateReset2,module.exports.inflateResetKeep=inflateResetKeep,module.exports.inflateInit=strm=>inflateInit2(strm,15),module.exports.inflateInit2=inflateInit2,module.exports.inflate=(strm,flush)=>{let state,input,output,next,put,have,left,hold,bits,_in,_out,copy,from,from_source,here_bits,here_op,here_val,last_bits,last_op,last_val,len,ret,here=0;const hbuf=new Uint8Array(4);let opts,n;const order=new Uint8Array([16,17,18,0,8,7,9,6,10,5,11,4,12,3,13,2,14,1,15]);if(!strm||!strm.state||!strm.output||!strm.input&&0!==strm.avail_in)return Z_STREAM_ERROR;state=strm.state,12===state.mode&&(state.mode=13),put=strm.next_out,output=strm.output,left=strm.avail_out,next=strm.next_in,input=strm.input,have=strm.avail_in,hold=state.hold,bits=state.bits,_in=have,_out=left,ret=Z_OK;inf_leave:for(;;)switch(state.mode){case 1:if(0===state.wrap){state.mode=13;break}for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(2&state.wrap&&35615===hold){state.check=0,hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,state.check=crc32(state.check,hbuf,2,0),hold=0,bits=0,state.mode=2;break}if(state.flags=0,state.head&&(state.head.done=!1),!(1&state.wrap)||(((255&hold)<<8)+(hold>>8))%31){strm.msg=\"incorrect header check\",state.mode=30;break}if((15&hold)!==Z_DEFLATED){strm.msg=\"unknown compression method\",state.mode=30;break}if(hold>>>=4,bits-=4,len=8+(15&hold),0===state.wbits)state.wbits=len;else if(len>state.wbits){strm.msg=\"invalid window size\",state.mode=30;break}state.dmax=1<<state.wbits,strm.adler=state.check=1,state.mode=512&hold?10:12,hold=0,bits=0;break;case 2:for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(state.flags=hold,(255&state.flags)!==Z_DEFLATED){strm.msg=\"unknown compression method\",state.mode=30;break}if(57344&state.flags){strm.msg=\"unknown header flags set\",state.mode=30;break}state.head&&(state.head.text=hold>>8&1),512&state.flags&&(hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,state.check=crc32(state.check,hbuf,2,0)),hold=0,bits=0,state.mode=3;case 3:for(;bits<32;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.head&&(state.head.time=hold),512&state.flags&&(hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,hbuf[2]=hold>>>16&255,hbuf[3]=hold>>>24&255,state.check=crc32(state.check,hbuf,4,0)),hold=0,bits=0,state.mode=4;case 4:for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.head&&(state.head.xflags=255&hold,state.head.os=hold>>8),512&state.flags&&(hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,state.check=crc32(state.check,hbuf,2,0)),hold=0,bits=0,state.mode=5;case 5:if(1024&state.flags){for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.length=hold,state.head&&(state.head.extra_len=hold),512&state.flags&&(hbuf[0]=255&hold,hbuf[1]=hold>>>8&255,state.check=crc32(state.check,hbuf,2,0)),hold=0,bits=0}else state.head&&(state.head.extra=null);state.mode=6;case 6:if(1024&state.flags&&(copy=state.length,copy>have&&(copy=have),copy&&(state.head&&(len=state.head.extra_len-state.length,state.head.extra||(state.head.extra=new Uint8Array(state.head.extra_len)),state.head.extra.set(input.subarray(next,next+copy),len)),512&state.flags&&(state.check=crc32(state.check,input,copy,next)),have-=copy,next+=copy,state.length-=copy),state.length))break inf_leave;state.length=0,state.mode=7;case 7:if(2048&state.flags){if(0===have)break inf_leave;copy=0;do{len=input[next+copy++],state.head&&len&&state.length<65536&&(state.head.name+=String.fromCharCode(len))}while(len&&copy<have);if(512&state.flags&&(state.check=crc32(state.check,input,copy,next)),have-=copy,next+=copy,len)break inf_leave}else state.head&&(state.head.name=null);state.length=0,state.mode=8;case 8:if(4096&state.flags){if(0===have)break inf_leave;copy=0;do{len=input[next+copy++],state.head&&len&&state.length<65536&&(state.head.comment+=String.fromCharCode(len))}while(len&&copy<have);if(512&state.flags&&(state.check=crc32(state.check,input,copy,next)),have-=copy,next+=copy,len)break inf_leave}else state.head&&(state.head.comment=null);state.mode=9;case 9:if(512&state.flags){for(;bits<16;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(hold!==(65535&state.check)){strm.msg=\"header crc mismatch\",state.mode=30;break}hold=0,bits=0}state.head&&(state.head.hcrc=state.flags>>9&1,state.head.done=!0),strm.adler=state.check=0,state.mode=12;break;case 10:for(;bits<32;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}strm.adler=state.check=zswap32(hold),hold=0,bits=0,state.mode=11;case 11:if(0===state.havedict)return strm.next_out=put,strm.avail_out=left,strm.next_in=next,strm.avail_in=have,state.hold=hold,state.bits=bits,Z_NEED_DICT;strm.adler=state.check=1,state.mode=12;case 12:if(flush===Z_BLOCK||flush===Z_TREES)break inf_leave;case 13:if(state.last){hold>>>=7&bits,bits-=7&bits,state.mode=27;break}for(;bits<3;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}switch(state.last=1&hold,hold>>>=1,bits-=1,3&hold){case 0:state.mode=14;break;case 1:if(fixedtables(state),state.mode=20,flush===Z_TREES){hold>>>=2,bits-=2;break inf_leave}break;case 2:state.mode=17;break;case 3:strm.msg=\"invalid block type\",state.mode=30}hold>>>=2,bits-=2;break;case 14:for(hold>>>=7&bits,bits-=7&bits;bits<32;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if((65535&hold)!=(hold>>>16^65535)){strm.msg=\"invalid stored block lengths\",state.mode=30;break}if(state.length=65535&hold,hold=0,bits=0,state.mode=15,flush===Z_TREES)break inf_leave;case 15:state.mode=16;case 16:if(copy=state.length,copy){if(copy>have&&(copy=have),copy>left&&(copy=left),0===copy)break inf_leave;output.set(input.subarray(next,next+copy),put),have-=copy,next+=copy,left-=copy,put+=copy,state.length-=copy;break}state.mode=12;break;case 17:for(;bits<14;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(state.nlen=257+(31&hold),hold>>>=5,bits-=5,state.ndist=1+(31&hold),hold>>>=5,bits-=5,state.ncode=4+(15&hold),hold>>>=4,bits-=4,state.nlen>286||state.ndist>30){strm.msg=\"too many length or distance symbols\",state.mode=30;break}state.have=0,state.mode=18;case 18:for(;state.have<state.ncode;){for(;bits<3;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.lens[order[state.have++]]=7&hold,hold>>>=3,bits-=3}for(;state.have<19;)state.lens[order[state.have++]]=0;if(state.lencode=state.lendyn,state.lenbits=7,opts={bits:state.lenbits},ret=inflate_table(0,state.lens,0,19,state.lencode,0,state.work,opts),state.lenbits=opts.bits,ret){strm.msg=\"invalid code lengths set\",state.mode=30;break}state.have=0,state.mode=19;case 19:for(;state.have<state.nlen+state.ndist;){for(;here=state.lencode[hold&(1<<state.lenbits)-1],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(here_val<16)hold>>>=here_bits,bits-=here_bits,state.lens[state.have++]=here_val;else{if(16===here_val){for(n=here_bits+2;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(hold>>>=here_bits,bits-=here_bits,0===state.have){strm.msg=\"invalid bit length repeat\",state.mode=30;break}len=state.lens[state.have-1],copy=3+(3&hold),hold>>>=2,bits-=2}else if(17===here_val){for(n=here_bits+3;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}hold>>>=here_bits,bits-=here_bits,len=0,copy=3+(7&hold),hold>>>=3,bits-=3}else{for(n=here_bits+7;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}hold>>>=here_bits,bits-=here_bits,len=0,copy=11+(127&hold),hold>>>=7,bits-=7}if(state.have+copy>state.nlen+state.ndist){strm.msg=\"invalid bit length repeat\",state.mode=30;break}for(;copy--;)state.lens[state.have++]=len}}if(30===state.mode)break;if(0===state.lens[256]){strm.msg=\"invalid code -- missing end-of-block\",state.mode=30;break}if(state.lenbits=9,opts={bits:state.lenbits},ret=inflate_table(1,state.lens,0,state.nlen,state.lencode,0,state.work,opts),state.lenbits=opts.bits,ret){strm.msg=\"invalid literal/lengths set\",state.mode=30;break}if(state.distbits=6,state.distcode=state.distdyn,opts={bits:state.distbits},ret=inflate_table(2,state.lens,state.nlen,state.ndist,state.distcode,0,state.work,opts),state.distbits=opts.bits,ret){strm.msg=\"invalid distances set\",state.mode=30;break}if(state.mode=20,flush===Z_TREES)break inf_leave;case 20:state.mode=21;case 21:if(have>=6&&left>=258){strm.next_out=put,strm.avail_out=left,strm.next_in=next,strm.avail_in=have,state.hold=hold,state.bits=bits,inflate_fast(strm,_out),put=strm.next_out,output=strm.output,left=strm.avail_out,next=strm.next_in,input=strm.input,have=strm.avail_in,hold=state.hold,bits=state.bits,12===state.mode&&(state.back=-1);break}for(state.back=0;here=state.lencode[hold&(1<<state.lenbits)-1],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(here_op&&0==(240&here_op)){for(last_bits=here_bits,last_op=here_op,last_val=here_val;here=state.lencode[last_val+((hold&(1<<last_bits+last_op)-1)>>last_bits)],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(last_bits+here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}hold>>>=last_bits,bits-=last_bits,state.back+=last_bits}if(hold>>>=here_bits,bits-=here_bits,state.back+=here_bits,state.length=here_val,0===here_op){state.mode=26;break}if(32&here_op){state.back=-1,state.mode=12;break}if(64&here_op){strm.msg=\"invalid literal/length code\",state.mode=30;break}state.extra=15&here_op,state.mode=22;case 22:if(state.extra){for(n=state.extra;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.length+=hold&(1<<state.extra)-1,hold>>>=state.extra,bits-=state.extra,state.back+=state.extra}state.was=state.length,state.mode=23;case 23:for(;here=state.distcode[hold&(1<<state.distbits)-1],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(0==(240&here_op)){for(last_bits=here_bits,last_op=here_op,last_val=here_val;here=state.distcode[last_val+((hold&(1<<last_bits+last_op)-1)>>last_bits)],here_bits=here>>>24,here_op=here>>>16&255,here_val=65535&here,!(last_bits+here_bits<=bits);){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}hold>>>=last_bits,bits-=last_bits,state.back+=last_bits}if(hold>>>=here_bits,bits-=here_bits,state.back+=here_bits,64&here_op){strm.msg=\"invalid distance code\",state.mode=30;break}state.offset=here_val,state.extra=15&here_op,state.mode=24;case 24:if(state.extra){for(n=state.extra;bits<n;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}state.offset+=hold&(1<<state.extra)-1,hold>>>=state.extra,bits-=state.extra,state.back+=state.extra}if(state.offset>state.dmax){strm.msg=\"invalid distance too far back\",state.mode=30;break}state.mode=25;case 25:if(0===left)break inf_leave;if(copy=_out-left,state.offset>copy){if(copy=state.offset-copy,copy>state.whave&&state.sane){strm.msg=\"invalid distance too far back\",state.mode=30;break}copy>state.wnext?(copy-=state.wnext,from=state.wsize-copy):from=state.wnext-copy,copy>state.length&&(copy=state.length),from_source=state.window}else from_source=output,from=put-state.offset,copy=state.length;copy>left&&(copy=left),left-=copy,state.length-=copy;do{output[put++]=from_source[from++]}while(--copy);0===state.length&&(state.mode=21);break;case 26:if(0===left)break inf_leave;output[put++]=state.length,left--,state.mode=21;break;case 27:if(state.wrap){for(;bits<32;){if(0===have)break inf_leave;have--,hold|=input[next++]<<bits,bits+=8}if(_out-=left,strm.total_out+=_out,state.total+=_out,_out&&(strm.adler=state.check=state.flags?crc32(state.check,output,_out,put-_out):adler32(state.check,output,_out,put-_out)),_out=left,(state.flags?hold:zswap32(hold))!==state.check){strm.msg=\"incorrect data check\",state.mode=30;break}hold=0,bits=0}state.mode=28;case 28:if(state.wrap&&state.flags){for(;bits<32;){if(0===have)break inf_leave;have--,hold+=input[next++]<<bits,bits+=8}if(hold!==(4294967295&state.total)){strm.msg=\"incorrect length check\",state.mode=30;break}hold=0,bits=0}state.mode=29;case 29:ret=Z_STREAM_END;break inf_leave;case 30:ret=Z_DATA_ERROR;break inf_leave;case 31:return Z_MEM_ERROR;case 32:default:return Z_STREAM_ERROR}return strm.next_out=put,strm.avail_out=left,strm.next_in=next,strm.avail_in=have,state.hold=hold,state.bits=bits,(state.wsize||_out!==strm.avail_out&&state.mode<30&&(state.mode<27||flush!==Z_FINISH))&&updatewindow(strm,strm.output,strm.next_out,_out-strm.avail_out)?(state.mode=31,Z_MEM_ERROR):(_in-=strm.avail_in,_out-=strm.avail_out,strm.total_in+=_in,strm.total_out+=_out,state.total+=_out,state.wrap&&_out&&(strm.adler=state.check=state.flags?crc32(state.check,output,_out,strm.next_out-_out):adler32(state.check,output,_out,strm.next_out-_out)),strm.data_type=state.bits+(state.last?64:0)+(12===state.mode?128:0)+(20===state.mode||15===state.mode?256:0),(0===_in&&0===_out||flush===Z_FINISH)&&ret===Z_OK&&(ret=Z_BUF_ERROR),ret)},module.exports.inflateEnd=strm=>{if(!strm||!strm.state)return Z_STREAM_ERROR;let state=strm.state;return state.window&&(state.window=null),strm.state=null,Z_OK},module.exports.inflateGetHeader=(strm,head)=>{if(!strm||!strm.state)return Z_STREAM_ERROR;const state=strm.state;return 0==(2&state.wrap)?Z_STREAM_ERROR:(state.head=head,head.done=!1,Z_OK)},module.exports.inflateSetDictionary=(strm,dictionary)=>{const dictLength=dictionary.length;let state,dictid,ret;return strm&&strm.state?(state=strm.state,0!==state.wrap&&11!==state.mode?Z_STREAM_ERROR:11===state.mode&&(dictid=1,dictid=adler32(dictid,dictionary,dictLength,0),dictid!==state.check)?Z_DATA_ERROR:(ret=updatewindow(strm,dictionary,dictLength,dictLength),ret?(state.mode=31,Z_MEM_ERROR):(state.havedict=1,Z_OK))):Z_STREAM_ERROR},module.exports.inflateInfo=\"pako inflate (from Nodeca project)\"},241:module=>{\"use strict\";const lbase=new Uint16Array([3,4,5,6,7,8,9,10,11,13,15,17,19,23,27,31,35,43,51,59,67,83,99,115,131,163,195,227,258,0,0]),lext=new Uint8Array([16,16,16,16,16,16,16,16,17,17,17,17,18,18,18,18,19,19,19,19,20,20,20,20,21,21,21,21,16,72,78]),dbase=new Uint16Array([1,2,3,4,5,7,9,13,17,25,33,49,65,97,129,193,257,385,513,769,1025,1537,2049,3073,4097,6145,8193,12289,16385,24577,0,0]),dext=new Uint8Array([16,16,16,16,17,17,18,18,19,19,20,20,21,21,22,22,23,23,24,24,25,25,26,26,27,27,28,28,29,29,64,64]);module.exports=(type,lens,lens_index,codes,table,table_index,work,opts)=>{const bits=opts.bits;let incr,fill,low,mask,next,end,len=0,sym=0,min=0,max=0,root=0,curr=0,drop=0,left=0,used=0,huff=0,base=null,base_index=0;const count=new Uint16Array(16),offs=new Uint16Array(16);let here_bits,here_op,here_val,extra=null,extra_index=0;for(len=0;len<=15;len++)count[len]=0;for(sym=0;sym<codes;sym++)count[lens[lens_index+sym]]++;for(root=bits,max=15;max>=1&&0===count[max];max--);if(root>max&&(root=max),0===max)return table[table_index++]=20971520,table[table_index++]=20971520,opts.bits=1,0;for(min=1;min<max&&0===count[min];min++);for(root<min&&(root=min),left=1,len=1;len<=15;len++)if(left<<=1,left-=count[len],left<0)return-1;if(left>0&&(0===type||1!==max))return-1;for(offs[1]=0,len=1;len<15;len++)offs[len+1]=offs[len]+count[len];for(sym=0;sym<codes;sym++)0!==lens[lens_index+sym]&&(work[offs[lens[lens_index+sym]]++]=sym);if(0===type?(base=extra=work,end=19):1===type?(base=lbase,base_index-=257,extra=lext,extra_index-=257,end=256):(base=dbase,extra=dext,end=-1),huff=0,sym=0,len=min,next=table_index,curr=root,drop=0,low=-1,used=1<<root,mask=used-1,1===type&&used>852||2===type&&used>592)return 1;for(;;){here_bits=len-drop,work[sym]<end?(here_op=0,here_val=work[sym]):work[sym]>end?(here_op=extra[extra_index+work[sym]],here_val=base[base_index+work[sym]]):(here_op=96,here_val=0),incr=1<<len-drop,fill=1<<curr,min=fill;do{fill-=incr,table[next+(huff>>drop)+fill]=here_bits<<24|here_op<<16|here_val|0}while(0!==fill);for(incr=1<<len-1;huff&incr;)incr>>=1;if(0!==incr?(huff&=incr-1,huff+=incr):huff=0,sym++,0==--count[len]){if(len===max)break;len=lens[lens_index+work[sym]]}if(len>root&&(huff&mask)!==low){for(0===drop&&(drop=root),next+=min,curr=len-drop,left=1<<curr;curr+drop<max&&(left-=count[curr+drop],!(left<=0));)curr++,left<<=1;if(used+=1<<curr,1===type&&used>852||2===type&&used>592)return 1;low=huff&mask,table[low]=root<<24|curr<<16|next-table_index|0}}return 0!==huff&&(table[next+huff]=len-drop<<24|64<<16|0),opts.bits=root,0}},898:module=>{\"use strict\";module.exports={2:\"need dictionary\",1:\"stream end\",0:\"\",\"-1\":\"file error\",\"-2\":\"stream error\",\"-3\":\"data error\",\"-4\":\"insufficient memory\",\"-5\":\"buffer error\",\"-6\":\"incompatible version\"}},292:module=>{\"use strict\";module.exports=function ZStream(){this.input=null,this.next_in=0,this.avail_in=0,this.total_in=0,this.output=null,this.next_out=0,this.avail_out=0,this.total_out=0,this.msg=\"\",this.state=null,this.data_type=2,this.adler=0}},330:(__unused_webpack_module,__webpack_exports__,__webpack_require__)=>{\"use strict\";__webpack_require__.d(__webpack_exports__,{default:()=>__WEBPACK_DEFAULT_EXPORT__});var numpy_parser__WEBPACK_IMPORTED_MODULE_0__=__webpack_require__(829),ndarray__WEBPACK_IMPORTED_MODULE_1__=__webpack_require__(861),pako_lib_inflate__WEBPACK_IMPORTED_MODULE_2__=__webpack_require__(843);const __WEBPACK_DEFAULT_EXPORT__=loader={unpack_obj:function unpack_obj(obj){if(Array.isArray(obj)){var ret=[];for(var v of obj)ret.push(unpack_obj(v));return ret}if(obj instanceof Object){if(obj.hasOwnProperty(\"__type__\"))return function unpack_custom_data(obj){if(\"npy\"==obj.__type__){var uint8arr;if(window.obj=obj,obj.hasOwnProperty(\"zdata\")){const compressed=Uint8Array.from(window.atob(obj.zdata),(c=>c.charCodeAt(0)));uint8arr=(0,pako_lib_inflate__WEBPACK_IMPORTED_MODULE_2__.rr)(compressed)}else uint8arr=Uint8Array.from(window.atob(obj.data),(c=>c.charCodeAt(0)));var arr=(0,numpy_parser__WEBPACK_IMPORTED_MODULE_0__.g)(uint8arr.buffer);if(arr=ndarray__WEBPACK_IMPORTED_MODULE_1__(arr.data,arr.shape),obj.hasOwnProperty(\"min\")){let scale=\"uint8\"===arr.dtype?255:65535;for(var size=1,i=0;i<arr.shape.length;i++)size*=arr.shape[i];var arr_=ndarray__WEBPACK_IMPORTED_MODULE_1__(new Float32Array(size),arr.shape);for(i=0;i<arr.data.length;i++)arr_.data[i]=obj.min+(obj.max-obj.min)*arr.data[i]/scale;return arr_}return arr}return{}}(obj);ret={};for(var k of Object.keys(obj))ret[k]=unpack_obj(obj[k]);return ret}return obj}}}},__webpack_module_cache__={};function __webpack_require__(moduleId){if(__webpack_module_cache__[moduleId])return __webpack_module_cache__[moduleId].exports;var module=__webpack_module_cache__[moduleId]={exports:{}};return __webpack_modules__[moduleId](module,module.exports,__webpack_require__),module.exports}return __webpack_require__.n=module=>{var getter=module&&module.__esModule?()=>module.default:()=>module;return __webpack_require__.d(getter,{a:getter}),getter},__webpack_require__.d=(exports,definition)=>{for(var key in definition)__webpack_require__.o(definition,key)&&!__webpack_require__.o(exports,key)&&Object.defineProperty(exports,key,{enumerable:!0,get:definition[key]})},__webpack_require__.o=(obj,prop)=>Object.prototype.hasOwnProperty.call(obj,prop),__webpack_require__(330)})().default;</script>\n",[m
        "<script>var AttentionMulti;AttentionMulti=(()=>{\"use strict\";var __webpack_modules__={143:(__unused_webpack_module,__webpack_exports__,__webpack_require__)=>{function noop(){}__webpack_require__.d(__webpack_exports__,{default:()=>AttentionMulti_svelte});function run(fn){return fn()}function blank_object(){return Object.create(null)}function run_all(fns){fns.forEach(run)}function is_function(thing){return\"function\"==typeof thing}function safe_not_equal(a,b){return a!=a?b==b:a!==b||a&&\"object\"==typeof a||\"function\"==typeof a}function is_empty(obj){return 0===Object.keys(obj).length}function get_slot_context(definition,ctx,$$scope,fn){return definition[1]&&fn?function internal_assign(tar,src){for(const k in src)tar[k]=src[k];return tar}($$scope.ctx.slice(),definition[1](fn(ctx))):$$scope.ctx}function get_slot_changes(definition,$$scope,dirty,fn){if(definition[2]&&fn){const lets=definition[2](fn(dirty));if(void 0===$$scope.dirty)return lets;if(\"object\"==typeof lets){const merged=[],len=Math.max($$scope.dirty.length,lets.length);for(let i=0;i<len;i+=1)merged[i]=$$scope.dirty[i]|lets[i];return merged}return $$scope.dirty|lets}return $$scope.dirty}new Set;function append(target,node){target.appendChild(node)}function insert(target,node,anchor){target.insertBefore(node,anchor||null)}function detach(node){node.parentNode.removeChild(node)}function destroy_each(iterations,detaching){for(let i=0;i<iterations.length;i+=1)iterations[i]&&iterations[i].d(detaching)}function internal_element(name){return document.createElement(name)}function internal_text(data){return document.createTextNode(data)}function space(){return internal_text(\" \")}function listen(node,event,handler,options){return node.addEventListener(event,handler,options),()=>node.removeEventListener(event,handler,options)}function attr(node,attribute,value){null==value?node.removeAttribute(attribute):node.getAttribute(attribute)!==value&&node.setAttribute(attribute,value)}function set_data(text,data){data=\"\"+data,text.wholeText!==data&&(text.data=data)}function set_style(node,key,value,important){node.style.setProperty(key,value,important?\"important\":\"\")}new Set;let current_component;function set_current_component(component){current_component=component}function get_current_component(){if(!current_component)throw new Error(\"Function called outside component initialization\");return current_component}const dirty_components=[],binding_callbacks=[],render_callbacks=[],flush_callbacks=[],resolved_promise=Promise.resolve();let update_scheduled=!1;function schedule_update(){update_scheduled||(update_scheduled=!0,resolved_promise.then(flush))}function add_render_callback(fn){render_callbacks.push(fn)}function add_flush_callback(fn){flush_callbacks.push(fn)}let flushing=!1;const seen_callbacks=new Set;function flush(){if(!flushing){flushing=!0;do{for(let i=0;i<dirty_components.length;i+=1){const component=dirty_components[i];set_current_component(component),update(component.$$)}for(set_current_component(null),dirty_components.length=0;binding_callbacks.length;)binding_callbacks.pop()();for(let i=0;i<render_callbacks.length;i+=1){const callback=render_callbacks[i];seen_callbacks.has(callback)||(seen_callbacks.add(callback),callback())}render_callbacks.length=0}while(dirty_components.length);for(;flush_callbacks.length;)flush_callbacks.pop()();update_scheduled=!1,flushing=!1,seen_callbacks.clear()}}function update($$){if(null!==$$.fragment){$$.update(),run_all($$.before_update);const dirty=$$.dirty;$$.dirty=[-1],$$.fragment&&$$.fragment.p($$.ctx,dirty),$$.after_update.forEach(add_render_callback)}}const outroing=new Set;let outros;function group_outros(){outros={r:0,c:[],p:outros}}function check_outros(){outros.r||run_all(outros.c),outros=outros.p}function transition_in(block,local){block&&block.i&&(outroing.delete(block),block.i(local))}function transition_out(block,local,detach,callback){if(block&&block.o){if(outroing.has(block))return;outroing.add(block),outros.c.push((()=>{outroing.delete(block),callback&&(detach&&block.d(1),callback())})),block.o(local)}}\"undefined\"!=typeof window?window:\"undefined\"!=typeof globalThis?globalThis:global;new Set([\"allowfullscreen\",\"allowpaymentrequest\",\"async\",\"autofocus\",\"autoplay\",\"checked\",\"controls\",\"default\",\"defer\",\"disabled\",\"formnovalidate\",\"hidden\",\"ismap\",\"loop\",\"multiple\",\"muted\",\"nomodule\",\"novalidate\",\"open\",\"playsinline\",\"readonly\",\"required\",\"reversed\",\"selected\"]);let SvelteElement;function bind(component,name,callback){const index=component.$$.props[name];void 0!==index&&(component.$$.bound[index]=callback,callback(component.$$.ctx[index]))}function create_component(block){block&&block.c()}function mount_component(component,target,anchor){const{fragment,on_mount,on_destroy,after_update}=component.$$;fragment&&fragment.m(target,anchor),add_render_callback((()=>{const new_on_destroy=on_mount.map(run).filter(is_function);on_destroy?on_destroy.push(...new_on_destroy):run_all(new_on_destroy),component.$$.on_mount=[]})),after_update.forEach(add_render_callback)}function destroy_component(component,detaching){const $$=component.$$;null!==$$.fragment&&(run_all($$.on_destroy),$$.fragment&&$$.fragment.d(detaching),$$.on_destroy=$$.fragment=null,$$.ctx=[])}function init(component,options,instance,create_fragment,not_equal,props,dirty=[-1]){const parent_component=current_component;set_current_component(component);const prop_values=options.props||{},$$=component.$$={fragment:null,ctx:null,props,update:noop,not_equal,bound:blank_object(),on_mount:[],on_destroy:[],before_update:[],after_update:[],context:new Map(parent_component?parent_component.$$.context:[]),callbacks:blank_object(),dirty,skip_bound:!1};let ready=!1;if($$.ctx=instance?instance(component,prop_values,((i,ret,...rest)=>{const value=rest.length?rest[0]:ret;return $$.ctx&&not_equal($$.ctx[i],$$.ctx[i]=value)&&(!$$.skip_bound&&$$.bound[i]&&$$.bound[i](value),ready&&function make_dirty(component,i){-1===component.$$.dirty[0]&&(dirty_components.push(component),schedule_update(),component.$$.dirty.fill(0)),component.$$.dirty[i/31|0]|=1<<i%31}(component,i)),ret})):[],$$.update(),ready=!0,run_all($$.before_update),$$.fragment=!!create_fragment&&create_fragment($$.ctx),options.target){if(options.hydrate){const nodes=function children(element){return Array.from(element.childNodes)}(options.target);$$.fragment&&$$.fragment.l(nodes),nodes.forEach(detach)}else $$.fragment&&$$.fragment.c();options.intro&&transition_in(component.$$.fragment),mount_component(component,options.target,options.anchor),flush()}set_current_component(parent_component)}\"function\"==typeof HTMLElement&&(SvelteElement=class extends HTMLElement{constructor(){super(),this.attachShadow({mode:\"open\"})}connectedCallback(){for(const key in this.$$.slotted)this.appendChild(this.$$.slotted[key])}attributeChangedCallback(attr,_oldValue,newValue){this[attr]=newValue}$destroy(){destroy_component(this,1),this.$destroy=noop}$on(type,callback){const callbacks=this.$$.callbacks[type]||(this.$$.callbacks[type]=[]);return callbacks.push(callback),()=>{const index=callbacks.indexOf(callback);-1!==index&&callbacks.splice(index,1)}}$set($$props){this.$$set&&!is_empty($$props)&&(this.$$.skip_bound=!0,this.$$set($$props),this.$$.skip_bound=!1)}});class SvelteComponent{$destroy(){destroy_component(this,1),this.$destroy=noop}$on(type,callback){const callbacks=this.$$.callbacks[type]||(this.$$.callbacks[type]=[]);return callbacks.push(callback),()=>{const index=callbacks.indexOf(callback);-1!==index&&callbacks.splice(index,1)}}$set($$props){this.$$set&&!is_empty($$props)&&(this.$$.skip_bound=!0,this.$$set($$props),this.$$.skip_bound=!1)}}function norm(x,ord=2){if(!(\"length\"in x))return function norm_nd(x,ord=2){for(var S=0,i=0;i<x.shape[0];i++)S+=Math.pow(Math.abs(x.get(i)),ord);return Math.pow(S,1/ord)}(x,ord);for(var S=0,i=0;i<x.length;i++)S+=Math.pow(Math.abs(x[i]),ord);return Math.pow(S,1/ord)}function normalize(v,ord=2){var v_norm=norm(v,ord);return v.map((x=>x/(1e-4+v_norm)))}function hue_to_rgb(theta){for(var colors=[[1,0,0],[1,1,0],[0,1,0],[0,1,1],[0,0,1],[1,0,1]].map((c=>normalize(c,1)));theta<0;)theta+=360;theta%=360;var d_theta=360/colors.length,n=Math.floor(theta/d_theta),mix_coef=(theta-n*d_theta)/d_theta,v=function interp(a,b,s){return a.map(((x,i)=>(1-s)*x+s*b[i]))}(colors[n],colors[(n+1)%colors.length],mix_coef);return v=normalize(v,1)}const __rgb_hue_vector_cache=[];function rgb_to_css(color){return`rgb(${255*color[0]}, ${255*color[1]}, ${255*color[2]})`}function sparse_color_map(v,zero_c=[.98,.98,.98],isolate_channel,hues){const v_len=\"length\"in v?v.length:v.shape[0],elem=\"length\"in v?i=>v[i]:i=>v.get(i);if(null==hues&&(hues=function rgb_hue_vector(n){if(n in __rgb_hue_vector_cache)return __rgb_hue_vector_cache[n];let out=[];for(let i=0;i<n;i++){const hue=360*i/n;out.push(hue_to_rgb(hue))}return __rgb_hue_vector_cache[n]=out,out}(v_len)),console.log(\"Hues\",hues),null==isolate_channel){for(var S=[0,0,0],i=0;i<v_len;i++){const ei=elem(i);if(0!=ei)for(var rgb=hues[i],j=0;j<3;j++)S[j]+=ei*rgb[j]}S=normalize(S,1);var mag=norm(v,2);for(mag=Math.max(0,Math.min(1,mag)),j=0;j<3;j++)S[j]=mag*S[j]+(1-mag)*zero_c[j];return S}var Ci=hues[i=isolate_channel];for(S=[0,0,0],j=0;j<3;j++)S[j]=elem(i)*Ci[j]+(1-elem(i))*zero_c[j];return S}function sparse_color_map_css(v,zero_c=[.98,.98,.98],isolate_channel,hues){return rgb_to_css(sparse_color_map(v,zero_c,isolate_channel,hues))}function create_if_block(ctx){let div0,t,div1;return{c(){div0=internal_element(\"div\"),t=space(),div1=internal_element(\"div\"),attr(div0,\"class\",\"focus-top svelte-1vzuzhc\"),set_style(div0,\"height\",ctx[2]*ctx[3]/ctx[0].shape[0]+\"px\"),attr(div1,\"class\",\"focus-bottom svelte-1vzuzhc\"),set_style(div1,\"height\",ctx[2]*(1-(ctx[3]+1)/ctx[0].shape[0])+\"px\")},m(target,anchor){insert(target,div0,anchor),insert(target,t,anchor),insert(target,div1,anchor)},p(ctx,dirty){13&dirty&&set_style(div0,\"height\",ctx[2]*ctx[3]/ctx[0].shape[0]+\"px\"),13&dirty&&set_style(div1,\"height\",ctx[2]*(1-(ctx[3]+1)/ctx[0].shape[0])+\"px\")},d(detaching){detaching&&detach(div0),detaching&&detach(t),detaching&&detach(div1)}}}function create_fragment(ctx){let div,canvas_1,t,if_block=null!=ctx[3]&&create_if_block(ctx);return{c(){div=internal_element(\"div\"),canvas_1=internal_element(\"canvas\"),t=space(),if_block&&if_block.c(),set_style(canvas_1,\"width\",ctx[1]+\"px\"),set_style(canvas_1,\"height\",ctx[2]+\"px\"),attr(canvas_1,\"class\",\"svelte-1vzuzhc\"),attr(div,\"class\",\"container svelte-1vzuzhc\"),set_style(div,\"width\",ctx[1]+\"px\"),set_style(div,\"height\",ctx[2]+\"px\")},m(target,anchor){insert(target,div,anchor),append(div,canvas_1),ctx[8](canvas_1),append(div,t),if_block&&if_block.m(div,null)},p(ctx,[dirty]){2&dirty&&set_style(canvas_1,\"width\",ctx[1]+\"px\"),4&dirty&&set_style(canvas_1,\"height\",ctx[2]+\"px\"),null!=ctx[3]?if_block?if_block.p(ctx,dirty):(if_block=create_if_block(ctx),if_block.c(),if_block.m(div,null)):if_block&&(if_block.d(1),if_block=null),2&dirty&&set_style(div,\"width\",ctx[1]+\"px\"),4&dirty&&set_style(div,\"height\",ctx[2]+\"px\")},i:noop,o:noop,d(detaching){detaching&&detach(div),ctx[8](null),if_block&&if_block.d()}}}function instance($$self,$$props,$$invalidate){let canvas,{array}=$$props,{width}=$$props,{height}=$$props,{hues}=$$props,{focus_token}=$$props,{isolate_channel}=$$props,{color_map=sparse_color_map}=$$props;function get_color(array,x,y,isolate_channel,hues){if(x<y)return[255,255,255];var v=array.pick(x,y,null);return color_map(v,void 0,isolate_channel,hues).map((c=>255*c))}function draw(canvas,array,isolate_channel,hues){if(null!=canvas&&null!=array){canvas.width=array.shape[0],canvas.height=array.shape[1];for(var ctx=canvas.getContext(\"2d\"),imgData=ctx.getImageData(0,0,canvas.width,canvas.height),x=0;x<canvas.width;x++)for(var y=0;y<canvas.height;y++){for(var ind=x*canvas.width+y,color=get_color(array,x,y,isolate_channel,hues=hues),channel=0;channel<3;channel++)imgData.data[4*ind+channel]=color[channel];imgData.data[4*ind+3]=255}ctx.putImageData(imgData,0,0)}}return function onMount(fn){get_current_component().$$.on_mount.push(fn)}((()=>draw(canvas,array,isolate_channel))),$$self.$$set=$$props=>{\"array\"in $$props&&$$invalidate(0,array=$$props.array),\"width\"in $$props&&$$invalidate(1,width=$$props.width),\"height\"in $$props&&$$invalidate(2,height=$$props.height),\"hues\"in $$props&&$$invalidate(5,hues=$$props.hues),\"focus_token\"in $$props&&$$invalidate(3,focus_token=$$props.focus_token),\"isolate_channel\"in $$props&&$$invalidate(6,isolate_channel=$$props.isolate_channel),\"color_map\"in $$props&&$$invalidate(7,color_map=$$props.color_map)},$$self.$$.update=()=>{113&$$self.$$.dirty&&draw(canvas,array,isolate_channel,hues)},[array,width,height,focus_token,canvas,hues,isolate_channel,color_map,function canvas_1_binding($$value){binding_callbacks[$$value?\"unshift\":\"push\"]((()=>{canvas=$$value,$$invalidate(4,canvas)}))}]}const ArrayImage_svelte=class ArrayImage extends SvelteComponent{constructor(options){super(),document.getElementById(\"svelte-1vzuzhc-style\")||function add_css(){var style=internal_element(\"style\");style.id=\"svelte-1vzuzhc-style\",style.textContent=\".container.svelte-1vzuzhc.svelte-1vzuzhc{position:relative;border:1px solid #aaa}.container.svelte-1vzuzhc>.svelte-1vzuzhc{position:absolute;width:100%;left:0px}.container.svelte-1vzuzhc canvas.svelte-1vzuzhc{top:0px;height:100%;image-rendering:pixelated}.container.svelte-1vzuzhc .focus-top.svelte-1vzuzhc,.container.svelte-1vzuzhc .focus-bottom.svelte-1vzuzhc{background:#aaa;opacity:0.3}.container.svelte-1vzuzhc .focus-top.svelte-1vzuzhc{top:0px}.container.svelte-1vzuzhc .focus-bottom.svelte-1vzuzhc{bottom:0px}\",append(document.head,style)}(),init(this,options,instance,create_fragment,safe_not_equal,{array:0,width:1,height:2,hues:5,focus_token:3,isolate_channel:6,color_map:7})}};function soft_update(v,val){return\"soft\"==v.mode&&(v.value=val),v}function hard_toggle_update(v,val){if(\"soft\"==v.mode)v.value=val,v.mode=\"hard\";else{if(\"hard\"!=v.mode||v.value==val)return function unset(v){return v.value=void 0,v.mode=\"soft\",v}(v);v.value=val}return v}function LockableValueToggle_svelte_create_fragment(ctx){let div,current,mounted,dispose;const default_slot_template=ctx[4].default,default_slot=function create_slot(definition,ctx,$$scope,fn){if(definition){const slot_ctx=get_slot_context(definition,ctx,$$scope,fn);return definition[0](slot_ctx)}}(default_slot_template,ctx,ctx[3],null);return{c(){div=internal_element(\"div\"),default_slot&&default_slot.c(),attr(div,\"style\",ctx[2])},m(target,anchor){insert(target,div,anchor),default_slot&&default_slot.m(div,null),current=!0,mounted||(dispose=[listen(div,\"mouseover\",ctx[5]),listen(div,\"click\",ctx[6]),listen(div,\"mouseout\",ctx[7])],mounted=!0)},p(ctx,[dirty]){default_slot&&default_slot.p&&8&dirty&&function update_slot(slot,slot_definition,ctx,$$scope,dirty,get_slot_changes_fn,get_slot_context_fn){const slot_changes=get_slot_changes(slot_definition,$$scope,dirty,get_slot_changes_fn);if(slot_changes){const slot_context=get_slot_context(slot_definition,ctx,$$scope,get_slot_context_fn);slot.p(slot_context,slot_changes)}}(default_slot,default_slot_template,ctx,ctx[3],dirty,null,null),(!current||4&dirty)&&attr(div,\"style\",ctx[2])},i(local){current||(transition_in(default_slot,local),current=!0)},o(local){transition_out(default_slot,local),current=!1},d(detaching){detaching&&detach(div),default_slot&&default_slot.d(detaching),mounted=!1,run_all(dispose)}}}function LockableValueToggle_svelte_instance($$self,$$props,$$invalidate){let{$$slots:slots={},$$scope}=$$props,{lock}=$$props,{set_value}=$$props,{style=\"\"}=$$props;return $$self.$$set=$$props=>{\"lock\"in $$props&&$$invalidate(0,lock=$$props.lock),\"set_value\"in $$props&&$$invalidate(1,set_value=$$props.set_value),\"style\"in $$props&&$$invalidate(2,style=$$props.style),\"$$scope\"in $$props&&$$invalidate(3,$$scope=$$props.$$scope)},[lock,set_value,style,$$scope,slots,()=>{$$invalidate(0,lock=soft_update(lock,set_value))},()=>{$$invalidate(0,lock=hard_toggle_update(lock,set_value))},()=>{$$invalidate(0,lock=soft_update(lock,void 0))}]}const LockableValueToggle_svelte=class LockableValueToggle extends SvelteComponent{constructor(options){super(),init(this,options,LockableValueToggle_svelte_instance,LockableValueToggle_svelte_create_fragment,safe_not_equal,{lock:0,set_value:1,style:2})}};function get_each_context(ctx,list,i){const child_ctx=ctx.slice();return child_ctx[32]=list[i],child_ctx[34]=i,child_ctx}function get_each_context_1(ctx,list,i){const child_ctx=ctx.slice();return child_ctx[35]=list[i],child_ctx}function create_if_block_4(ctx){let t0,t1,t2,t1_value=ctx[15][ctx[10]]+\"\";return{c(){t0=internal_text(\"(\"),t1=internal_text(t1_value),t2=internal_text(\")\")},m(target,anchor){insert(target,t0,anchor),insert(target,t1,anchor),insert(target,t2,anchor)},p(ctx,dirty){33792&dirty[0]&&t1_value!==(t1_value=ctx[15][ctx[10]]+\"\")&&set_data(t1,t1_value)},d(detaching){detaching&&detach(t0),detaching&&detach(t1),detaching&&detach(t2)}}}function create_if_block_3(ctx){let div,arrayimage,div_style_value,current;return arrayimage=new ArrayImage_svelte({props:{array:ctx[7],width:\"200\",height:\"200\",focus_token:ctx[9],isolate_channel:ctx[10]}}),{c(){div=internal_element(\"div\"),create_component(arrayimage.$$.fragment),attr(div,\"style\",div_style_value=\"grid-column: big-attn; grid-row: main; \"+(ctx[11]?\"\":\"display:none;\"))},m(target,anchor){insert(target,div,anchor),mount_component(arrayimage,div,null),current=!0},p(ctx,dirty){const arrayimage_changes={};128&dirty[0]&&(arrayimage_changes.array=ctx[7]),512&dirty[0]&&(arrayimage_changes.focus_token=ctx[9]),1024&dirty[0]&&(arrayimage_changes.isolate_channel=ctx[10]),arrayimage.$set(arrayimage_changes),(!current||2048&dirty[0]&&div_style_value!==(div_style_value=\"grid-column: big-attn; grid-row: main; \"+(ctx[11]?\"\":\"display:none;\")))&&attr(div,\"style\",div_style_value)},i(local){current||(transition_in(arrayimage.$$.fragment,local),current=!0)},o(local){transition_out(arrayimage.$$.fragment,local),current=!1},d(detaching){detaching&&detach(div),destroy_component(arrayimage)}}}function create_if_block_2(ctx){let div0,t1,div1,current,each_value_1=range(ctx[12].shape[2]),each_blocks=[];for(let i=0;i<each_value_1.length;i+=1)each_blocks[i]=create_each_block_1(get_each_context_1(ctx,each_value_1,i));const out=i=>transition_out(each_blocks[i],1,1,(()=>{each_blocks[i]=null}));return{c(){div0=internal_element(\"div\"),div0.textContent=\"Attention Heads (hover to focus, click to lock)\",t1=space(),div1=internal_element(\"div\");for(let i=0;i<each_blocks.length;i+=1)each_blocks[i].c();attr(div0,\"class\",\"figcaption svelte-1rpu49t\"),set_style(div0,\"grid-column\",\"heads\"),attr(div1,\"class\",\"heads svelte-1rpu49t\")},m(target,anchor){insert(target,div0,anchor),insert(target,t1,anchor),insert(target,div1,anchor);for(let i=0;i<each_blocks.length;i+=1)each_blocks[i].m(div1,null);current=!0},p(ctx,dirty){if(122566&dirty[0]){let i;for(each_value_1=range(ctx[12].shape[2]),i=0;i<each_value_1.length;i+=1){const child_ctx=get_each_context_1(ctx,each_value_1,i);each_blocks[i]?(each_blocks[i].p(child_ctx,dirty),transition_in(each_blocks[i],1)):(each_blocks[i]=create_each_block_1(child_ctx),each_blocks[i].c(),transition_in(each_blocks[i],1),each_blocks[i].m(div1,null))}for(group_outros(),i=each_value_1.length;i<each_blocks.length;i+=1)out(i);check_outros()}},i(local){if(!current){for(let i=0;i<each_value_1.length;i+=1)transition_in(each_blocks[i]);current=!0}},o(local){each_blocks=each_blocks.filter(Boolean);for(let i=0;i<each_blocks.length;i+=1)transition_out(each_blocks[i]);current=!1},d(detaching){detaching&&detach(div0),detaching&&detach(t1),detaching&&detach(div1),destroy_each(each_blocks,detaching)}}}function create_default_slot_1(ctx){let div1,arrayimage0,t0,div0,div1_style_value,t1,div3,arrayimage1,t2,div2,div3_style_value,t3,current,raw0_value=(null!=ctx[15][ctx[35]]?ctx[15][ctx[35]]:\"&nbsp\")+\"\",raw1_value=(null!=ctx[15][ctx[35]]?ctx[15][ctx[35]]:\"&nbsp\")+\"\";return arrayimage0=new ArrayImage_svelte({props:{array:ctx[7],width:\"60\",height:\"60\",isolate_channel:ctx[35]}}),arrayimage1=new ArrayImage_svelte({props:{array:ctx[6],width:\"60\",height:\"60\",isolate_channel:ctx[35]}}),{c(){div1=internal_element(\"div\"),create_component(arrayimage0.$$.fragment),t0=space(),div0=internal_element(\"div\"),t1=space(),div3=internal_element(\"div\"),create_component(arrayimage1.$$.fragment),t2=space(),div2=internal_element(\"div\"),t3=space(),attr(div0,\"class\",\"head-label svelte-1rpu49t\"),set_style(div0,\"background\",ctx[14][ctx[35]]),attr(div1,\"class\",\"head-icon svelte-1rpu49t\"),attr(div1,\"style\",div1_style_value=\"opacity: \"+(null!=ctx[10]&&ctx[10]!=ctx[35]?\"0.2\":ctx[16](ctx[35],ctx[9],ctx[2]))+\";\\n                        \"+(ctx[11]?\"\":\"display:none;\")),attr(div2,\"class\",\"head-label svelte-1rpu49t\"),set_style(div2,\"background\",ctx[14][ctx[35]]),attr(div3,\"class\",\"head-icon svelte-1rpu49t\"),attr(div3,\"style\",div3_style_value=\"opacity: \"+(null!=ctx[10]&&ctx[10]!=ctx[35]?\"0.2\":ctx[16](ctx[35],ctx[9],ctx[2]))+\";\\n                        \"+(ctx[11]?\"display:none;\":\"\"))},m(target,anchor){insert(target,div1,anchor),mount_component(arrayimage0,div1,null),append(div1,t0),append(div1,div0),div0.innerHTML=raw0_value,insert(target,t1,anchor),insert(target,div3,anchor),mount_component(arrayimage1,div3,null),append(div3,t2),append(div3,div2),div2.innerHTML=raw1_value,insert(target,t3,anchor),current=!0},p(ctx,dirty){const arrayimage0_changes={};128&dirty[0]&&(arrayimage0_changes.array=ctx[7]),4096&dirty[0]&&(arrayimage0_changes.isolate_channel=ctx[35]),arrayimage0.$set(arrayimage0_changes),(!current||36864&dirty[0])&&raw0_value!==(raw0_value=(null!=ctx[15][ctx[35]]?ctx[15][ctx[35]]:\"&nbsp\")+\"\")&&(div0.innerHTML=raw0_value),(!current||20480&dirty[0])&&set_style(div0,\"background\",ctx[14][ctx[35]]),(!current||7684&dirty[0]&&div1_style_value!==(div1_style_value=\"opacity: \"+(null!=ctx[10]&&ctx[10]!=ctx[35]?\"0.2\":ctx[16](ctx[35],ctx[9],ctx[2]))+\";\\n                        \"+(ctx[11]?\"\":\"display:none;\")))&&attr(div1,\"style\",div1_style_value);const arrayimage1_changes={};64&dirty[0]&&(arrayimage1_changes.array=ctx[6]),4096&dirty[0]&&(arrayimage1_changes.isolate_channel=ctx[35]),arrayimage1.$set(arrayimage1_changes),(!current||36864&dirty[0])&&raw1_value!==(raw1_value=(null!=ctx[15][ctx[35]]?ctx[15][ctx[35]]:\"&nbsp\")+\"\")&&(div2.innerHTML=raw1_value),(!current||20480&dirty[0])&&set_style(div2,\"background\",ctx[14][ctx[35]]),(!current||7684&dirty[0]&&div3_style_value!==(div3_style_value=\"opacity: \"+(null!=ctx[10]&&ctx[10]!=ctx[35]?\"0.2\":ctx[16](ctx[35],ctx[9],ctx[2]))+\";\\n                        \"+(ctx[11]?\"display:none;\":\"\")))&&attr(div3,\"style\",div3_style_value)},i(local){current||(transition_in(arrayimage0.$$.fragment,local),transition_in(arrayimage1.$$.fragment,local),current=!0)},o(local){transition_out(arrayimage0.$$.fragment,local),transition_out(arrayimage1.$$.fragment,local),current=!1},d(detaching){detaching&&detach(div1),destroy_component(arrayimage0),detaching&&detach(t1),detaching&&detach(div3),destroy_component(arrayimage1),detaching&&detach(t3)}}}function create_each_block_1(ctx){let lockablevaluetoggle,updating_lock,current;function lockablevaluetoggle_lock_binding(value){ctx[22].call(null,value)}let lockablevaluetoggle_props={set_value:ctx[35],$$slots:{default:[create_default_slot_1]},$$scope:{ctx}};return void 0!==ctx[1]&&(lockablevaluetoggle_props.lock=ctx[1]),lockablevaluetoggle=new LockableValueToggle_svelte({props:lockablevaluetoggle_props}),binding_callbacks.push((()=>bind(lockablevaluetoggle,\"lock\",lockablevaluetoggle_lock_binding))),{c(){create_component(lockablevaluetoggle.$$.fragment)},m(target,anchor){mount_component(lockablevaluetoggle,target,anchor),current=!0},p(ctx,dirty){const lockablevaluetoggle_changes={};4096&dirty[0]&&(lockablevaluetoggle_changes.set_value=ctx[35]),57028&dirty[0]|128&dirty[1]&&(lockablevaluetoggle_changes.$$scope={dirty,ctx}),!updating_lock&&2&dirty[0]&&(updating_lock=!0,lockablevaluetoggle_changes.lock=ctx[1],add_flush_callback((()=>updating_lock=!1))),lockablevaluetoggle.$set(lockablevaluetoggle_changes)},i(local){current||(transition_in(lockablevaluetoggle.$$.fragment,local),current=!0)},o(local){transition_out(lockablevaluetoggle.$$.fragment,local),current=!1},d(detaching){destroy_component(lockablevaluetoggle,detaching)}}}function AttentionMulti_svelte_create_if_block(ctx){let div3,div0,t1,div1,t2,div2,nobr,input,t3,span,t4,b,t5,t6,current,mounted,dispose,t5_value=ctx[2]?\"target\":\"source\",each_value=ctx[5],each_blocks=[];for(let i=0;i<each_value.length;i+=1)each_blocks[i]=create_each_block(get_each_context(ctx,each_value,i));const out=i=>transition_out(each_blocks[i],1,1,(()=>{each_blocks[i]=null}));let if_block=void 0!==ctx[7]&&create_if_block_1(ctx);return{c(){div3=internal_element(\"div\"),div0=internal_element(\"div\"),div0.textContent=\"Tokens (hover to focus, click to lock)\",t1=space(),div1=internal_element(\"div\");for(let i=0;i<each_blocks.length;i+=1)each_blocks[i].c();t2=space(),div2=internal_element(\"div\"),nobr=internal_element(\"nobr\"),input=internal_element(\"input\"),t3=space(),span=internal_element(\"span\"),t4=internal_text(\"Selected is\\n            \"),b=internal_element(\"b\"),t5=internal_text(t5_value),t6=space(),if_block&&if_block.c(),attr(div0,\"class\",\"figcaption svelte-1rpu49t\"),set_style(div0,\"grid-column\",\"left\"),attr(div1,\"class\",\"tokens svelte-1rpu49t\"),attr(input,\"class\",\"hover-mode svelte-1rpu49t\"),attr(input,\"type\",\"checkbox\"),attr(span,\"class\",\"hover-mode-text svelte-1rpu49t\"),set_style(span,\"white-space\",\"nowrap\"),attr(div2,\"class\",\"toggle\"),attr(div3,\"class\",\"tokens-container svelte-1rpu49t\")},m(target,anchor){insert(target,div3,anchor),append(div3,div0),append(div3,t1),append(div3,div1);for(let i=0;i<each_blocks.length;i+=1)each_blocks[i].m(div1,null);append(div3,t2),append(div3,div2),append(div2,nobr),append(nobr,input),input.checked=ctx[2],append(nobr,t3),append(nobr,span),append(span,t4),append(span,b),append(b,t5),append(nobr,t6),if_block&&if_block.m(nobr,null),current=!0,mounted||(dispose=[listen(input,\"change\",ctx[24]),listen(span,\"click\",ctx[25])],mounted=!0)},p(ctx,dirty){if(561&dirty[0]){let i;for(each_value=ctx[5],i=0;i<each_value.length;i+=1){const child_ctx=get_each_context(ctx,each_value,i);each_blocks[i]?(each_blocks[i].p(child_ctx,dirty),transition_in(each_blocks[i],1)):(each_blocks[i]=create_each_block(child_ctx),each_blocks[i].c(),transition_in(each_blocks[i],1),each_blocks[i].m(div1,null))}for(group_outros(),i=each_value.length;i<each_blocks.length;i+=1)out(i);check_outros()}4&dirty[0]&&(input.checked=ctx[2]),(!current||4&dirty[0])&&t5_value!==(t5_value=ctx[2]?\"target\":\"source\")&&set_data(t5,t5_value),void 0!==ctx[7]?if_block?if_block.p(ctx,dirty):(if_block=create_if_block_1(ctx),if_block.c(),if_block.m(nobr,null)):if_block&&(if_block.d(1),if_block=null)},i(local){if(!current){for(let i=0;i<each_value.length;i+=1)transition_in(each_blocks[i]);current=!0}},o(local){each_blocks=each_blocks.filter(Boolean);for(let i=0;i<each_blocks.length;i+=1)transition_out(each_blocks[i]);current=!1},d(detaching){detaching&&detach(div3),destroy_each(each_blocks,detaching),if_block&&if_block.d(),mounted=!1,run_all(dispose)}}}function create_default_slot(ctx){let span,t,span_class_value,t_value=ctx[32]+\"\";return{c(){span=internal_element(\"span\"),t=internal_text(t_value),attr(span,\"class\",span_class_value=\"token \"+(ctx[34]==ctx[9]?\"selected\":\"\")+\" svelte-1rpu49t\"),set_style(span,\"background\",ctx[4][ctx[34]])},m(target,anchor){insert(target,span,anchor),append(span,t)},p(ctx,dirty){32&dirty[0]&&t_value!==(t_value=ctx[32]+\"\")&&set_data(t,t_value),512&dirty[0]&&span_class_value!==(span_class_value=\"token \"+(ctx[34]==ctx[9]?\"selected\":\"\")+\" svelte-1rpu49t\")&&attr(span,\"class\",span_class_value),16&dirty[0]&&set_style(span,\"background\",ctx[4][ctx[34]])},d(detaching){detaching&&detach(span)}}}function create_each_block(ctx){let lockablevaluetoggle,updating_lock,current;function lockablevaluetoggle_lock_binding_1(value){ctx[23].call(null,value)}let lockablevaluetoggle_props={set_value:ctx[34],style:\"display: inline\",$$slots:{default:[create_default_slot]},$$scope:{ctx}};return void 0!==ctx[0]&&(lockablevaluetoggle_props.lock=ctx[0]),lockablevaluetoggle=new LockableValueToggle_svelte({props:lockablevaluetoggle_props}),binding_callbacks.push((()=>bind(lockablevaluetoggle,\"lock\",lockablevaluetoggle_lock_binding_1))),{c(){create_component(lockablevaluetoggle.$$.fragment)},m(target,anchor){mount_component(lockablevaluetoggle,target,anchor),current=!0},p(ctx,dirty){const lockablevaluetoggle_changes={};560&dirty[0]|128&dirty[1]&&(lockablevaluetoggle_changes.$$scope={dirty,ctx}),!updating_lock&&1&dirty[0]&&(updating_lock=!0,lockablevaluetoggle_changes.lock=ctx[0],add_flush_callback((()=>updating_lock=!1))),lockablevaluetoggle.$set(lockablevaluetoggle_changes)},i(local){current||(transition_in(lockablevaluetoggle.$$.fragment,local),current=!0)},o(local){transition_out(lockablevaluetoggle.$$.fragment,local),current=!1},d(detaching){destroy_component(lockablevaluetoggle,detaching)}}}function create_if_block_1(ctx){let input,t0,span,t1,b,t2,mounted,dispose,t2_value=ctx[3]?\"info-weighted\":\"unmodified\";return{c(){input=internal_element(\"input\"),t0=space(),span=internal_element(\"span\"),t1=internal_text(\"Attention is\\n            \"),b=internal_element(\"b\"),t2=internal_text(t2_value),attr(input,\"class\",\"info-mode svelte-1rpu49t\"),attr(input,\"type\",\"checkbox\"),attr(span,\"class\",\"info-mode-text svelte-1rpu49t\"),set_style(span,\"white-space\",\"nowrap\")},m(target,anchor){insert(target,input,anchor),input.checked=ctx[3],insert(target,t0,anchor),insert(target,span,anchor),append(span,t1),append(span,b),append(b,t2),mounted||(dispose=[listen(input,\"change\",ctx[26]),listen(span,\"click\",ctx[27])],mounted=!0)},p(ctx,dirty){8&dirty[0]&&(input.checked=ctx[3]),8&dirty[0]&&t2_value!==(t2_value=ctx[3]?\"info-weighted\":\"unmodified\")&&set_data(t2,t2_value)},d(detaching){detaching&&detach(input),detaching&&detach(t0),detaching&&detach(span),mounted=!1,run_all(dispose)}}}function AttentionMulti_svelte_create_fragment(ctx){let div2,div0,t0,t1,t2,div1,arrayimage,div1_style_value,t3,t4,if_block3_anchor,current,if_block0=null!=ctx[10]&&create_if_block_4(ctx),if_block1=void 0!==ctx[7]&&create_if_block_3(ctx);arrayimage=new ArrayImage_svelte({props:{array:ctx[6],width:\"200\",height:\"200\",focus_token:ctx[9],isolate_channel:ctx[10]}});let if_block2=ctx[13]>1&&create_if_block_2(ctx),if_block3=ctx[8]&&AttentionMulti_svelte_create_if_block(ctx);return{c(){div2=internal_element(\"div\"),div0=internal_element(\"div\"),t0=internal_text(\"Attention Pattern\\n        \"),if_block0&&if_block0.c(),t1=space(),if_block1&&if_block1.c(),t2=space(),div1=internal_element(\"div\"),create_component(arrayimage.$$.fragment),t3=space(),if_block2&&if_block2.c(),t4=space(),if_block3&&if_block3.c(),if_block3_anchor=function empty(){return internal_text(\"\")}(),attr(div0,\"class\",\"figcaption svelte-1rpu49t\"),set_style(div0,\"grid-column\",\"big-attn\"),attr(div1,\"style\",div1_style_value=\"grid-column: big-attn; grid-row: main; \"+(ctx[11]?\"display:none\":\"\")),attr(div2,\"class\",\"attn-container svelte-1rpu49t\")},m(target,anchor){insert(target,div2,anchor),append(div2,div0),append(div0,t0),if_block0&&if_block0.m(div0,null),append(div2,t1),if_block1&&if_block1.m(div2,null),append(div2,t2),append(div2,div1),mount_component(arrayimage,div1,null),append(div2,t3),if_block2&&if_block2.m(div2,null),insert(target,t4,anchor),if_block3&&if_block3.m(target,anchor),insert(target,if_block3_anchor,anchor),current=!0},p(ctx,dirty){null!=ctx[10]?if_block0?if_block0.p(ctx,dirty):(if_block0=create_if_block_4(ctx),if_block0.c(),if_block0.m(div0,null)):if_block0&&(if_block0.d(1),if_block0=null),void 0!==ctx[7]?if_block1?(if_block1.p(ctx,dirty),128&dirty[0]&&transition_in(if_block1,1)):(if_block1=create_if_block_3(ctx),if_block1.c(),transition_in(if_block1,1),if_block1.m(div2,t2)):if_block1&&(group_outros(),transition_out(if_block1,1,1,(()=>{if_block1=null})),check_outros());const arrayimage_changes={};64&dirty[0]&&(arrayimage_changes.array=ctx[6]),512&dirty[0]&&(arrayimage_changes.focus_token=ctx[9]),1024&dirty[0]&&(arrayimage_changes.isolate_channel=ctx[10]),arrayimage.$set(arrayimage_changes),(!current||2048&dirty[0]&&div1_style_value!==(div1_style_value=\"grid-column: big-attn; grid-row: main; \"+(ctx[11]?\"display:none\":\"\")))&&attr(div1,\"style\",div1_style_value),ctx[13]>1?if_block2?(if_block2.p(ctx,dirty),8192&dirty[0]&&transition_in(if_block2,1)):(if_block2=create_if_block_2(ctx),if_block2.c(),transition_in(if_block2,1),if_block2.m(div2,null)):if_block2&&(group_outros(),transition_out(if_block2,1,1,(()=>{if_block2=null})),check_outros()),ctx[8]?if_block3?(if_block3.p(ctx,dirty),256&dirty[0]&&transition_in(if_block3,1)):(if_block3=AttentionMulti_svelte_create_if_block(ctx),if_block3.c(),transition_in(if_block3,1),if_block3.m(if_block3_anchor.parentNode,if_block3_anchor)):if_block3&&(group_outros(),transition_out(if_block3,1,1,(()=>{if_block3=null})),check_outros())},i(local){current||(transition_in(if_block1),transition_in(arrayimage.$$.fragment,local),transition_in(if_block2),transition_in(if_block3),current=!0)},o(local){transition_out(if_block1),transition_out(arrayimage.$$.fragment,local),transition_out(if_block2),transition_out(if_block3),current=!1},d(detaching){detaching&&detach(div2),if_block0&&if_block0.d(),if_block1&&if_block1.d(),destroy_component(arrayimage),if_block2&&if_block2.d(),detaching&&detach(t4),if_block3&&if_block3.d(detaching),detaching&&detach(if_block3_anchor)}}}function range(n){return[...Array(n).keys()]}function reduce_Y(arr){if(void 0!==arr){for(var arr_=[],x=0;x<arr.shape[0];x++){arr_.push([]);for(var c=0;c<arr.shape[2];c++){for(var temp=0,y=0;y<arr.shape[1];y++)temp=Math.max(temp,arr.pick(x,y,c));arr_[x].push(temp)}}return arr_}}function reduce_X(arr){if(void 0!==arr){for(var arr_=[],y=0;y<arr.shape[0];y++){arr_.push([]);for(var c=0;c<arr.shape[2];c++){for(var temp=0,x=0;x<arr.shape[1];x++)temp=Math.max(temp,arr.pick(x,y,c));arr_[y].push(temp)}}return arr_}}function AttentionMulti_svelte_instance($$self,$$props,$$invalidate){let focus_token,focus_head,show_info_weighted,attention_show,attention_reduce_dst_unmodified,attention_reduce_src_unmodified,attention_reduce_dst_info_weighted,attention_reduce_src_info_weighted,attention_reduce_dst,attention_reduce_src,N_heads,colors,head_labels_,{tokens}=$$props,{attention}=$$props,{info_weighted}=$$props,{head_labels}=$$props,{show_tokens=!0}=$$props,{focus_token_lock={value:void 0,mode:\"soft\"}}=$$props,{focus_head_lock={value:void 0,mode:\"soft\"}}=$$props,{hover_token_is_target=!1}=$$props,{_show_info_weighted=!1}=$$props;function token_color(attention,focus_token_value,tok_i,isolate_channel,hover_token_is_target){if(null==focus_token_value)return sparse_color_map_css((hover_token_is_target?attention_reduce_src:attention_reduce_dst)[tok_i],void 0,isolate_channel);let tok_from,tok_to;return hover_token_is_target?(tok_from=tok_i,tok_to=focus_token_value):(tok_from=focus_token_value,tok_to=tok_i),function get_color(array,x,y,isolate_channel){return x<y?\"#FFF\":sparse_color_map_css(array.pick(x,y,null),void 0,isolate_channel)}(attention,tok_from,tok_to,isolate_channel)}let{all_token_colors}=$$props;return $$self.$$set=$$props=>{\"tokens\"in $$props&&$$invalidate(5,tokens=$$props.tokens),\"attention\"in $$props&&$$invalidate(6,attention=$$props.attention),\"info_weighted\"in $$props&&$$invalidate(7,info_weighted=$$props.info_weighted),\"head_labels\"in $$props&&$$invalidate(17,head_labels=$$props.head_labels),\"show_tokens\"in $$props&&$$invalidate(8,show_tokens=$$props.show_tokens),\"focus_token_lock\"in $$props&&$$invalidate(0,focus_token_lock=$$props.focus_token_lock),\"focus_head_lock\"in $$props&&$$invalidate(1,focus_head_lock=$$props.focus_head_lock),\"hover_token_is_target\"in $$props&&$$invalidate(2,hover_token_is_target=$$props.hover_token_is_target),\"_show_info_weighted\"in $$props&&$$invalidate(3,_show_info_weighted=$$props._show_info_weighted),\"all_token_colors\"in $$props&&$$invalidate(4,all_token_colors=$$props.all_token_colors)},$$self.$$.update=()=>{1&$$self.$$.dirty[0]&&$$invalidate(9,focus_token=focus_token_lock.value),2&$$self.$$.dirty[0]&&$$invalidate(10,focus_head=focus_head_lock.value),136&$$self.$$.dirty[0]&&$$invalidate(11,show_info_weighted=_show_info_weighted&&void 0!==info_weighted),2240&$$self.$$.dirty[0]&&$$invalidate(12,attention_show=show_info_weighted?info_weighted:attention),4096&$$self.$$.dirty[0]&&(window.attention_show=attention_show),64&$$self.$$.dirty[0]&&$$invalidate(18,attention_reduce_dst_unmodified=reduce_Y(attention)),64&$$self.$$.dirty[0]&&$$invalidate(19,attention_reduce_src_unmodified=reduce_X(attention)),128&$$self.$$.dirty[0]&&$$invalidate(20,attention_reduce_dst_info_weighted=reduce_Y(info_weighted)),128&$$self.$$.dirty[0]&&$$invalidate(21,attention_reduce_src_info_weighted=reduce_X(info_weighted)),1312768&$$self.$$.dirty[0]&&(attention_reduce_dst=show_info_weighted?attention_reduce_dst_info_weighted:attention_reduce_dst_unmodified),2623488&$$self.$$.dirty[0]&&(attention_reduce_src=show_info_weighted?attention_reduce_src_info_weighted:attention_reduce_src_unmodified),64&$$self.$$.dirty[0]&&$$invalidate(13,N_heads=attention.shape[2]),8192&$$self.$$.dirty[0]&&$$invalidate(14,colors=range(N_heads).map((i=>sparse_color_map_css(range(N_heads).map((x=>1)),void 0,i)))),139264&$$self.$$.dirty[0]&&$$invalidate(15,head_labels_=null!=head_labels?head_labels:range(N_heads)),5668&$$self.$$.dirty[0]&&$$invalidate(4,all_token_colors=range(tokens.length).map((i=>token_color(attention_show,focus_token,i,focus_head,hover_token_is_target))))},[focus_token_lock,focus_head_lock,hover_token_is_target,_show_info_weighted,all_token_colors,tokens,attention,info_weighted,show_tokens,focus_token,focus_head,show_info_weighted,attention_show,N_heads,colors,head_labels_,function head_intensity(head_i,focus_token_value,hover_token_is_target){if(null==focus_token_value)var v=1;else{var reduced_attn=hover_token_is_target?attention_reduce_src:attention_reduce_dst;v=Math.max(0,Math.min(1,reduced_attn[focus_token_value][head_i]))}return\"\"+v},head_labels,attention_reduce_dst_unmodified,attention_reduce_src_unmodified,attention_reduce_dst_info_weighted,attention_reduce_src_info_weighted,function lockablevaluetoggle_lock_binding(value){focus_head_lock=value,$$invalidate(1,focus_head_lock)},function lockablevaluetoggle_lock_binding_1(value){focus_token_lock=value,$$invalidate(0,focus_token_lock)},function input_change_handler(){hover_token_is_target=this.checked,$$invalidate(2,hover_token_is_target)},()=>$$invalidate(2,hover_token_is_target^=!0),function input_change_handler_1(){_show_info_weighted=this.checked,$$invalidate(3,_show_info_weighted)},()=>$$invalidate(3,_show_info_weighted^=!0)]}const AttentionMulti_svelte=class AttentionMulti extends SvelteComponent{constructor(options){super(),document.getElementById(\"svelte-1rpu49t-style\")||function AttentionMulti_svelte_add_css(){var style=internal_element(\"style\");style.id=\"svelte-1rpu49t-style\",style.textContent=\".attn-container.svelte-1rpu49t.svelte-1rpu49t.svelte-1rpu49t{display:grid;grid-template-rows:[title] min-content [main] min-content;grid-template-columns:[big-attn] min-content [heads] minmax(min-content, 624px);gap:12px}.figcaption.svelte-1rpu49t.svelte-1rpu49t.svelte-1rpu49t{color:#888;grid-row:title;white-space:nowrap}.tokens-container.svelte-1rpu49t.svelte-1rpu49t.svelte-1rpu49t{display:grid;grid-template-rows:[title] min-content [main] min-content;grid-template-columns:[left] min-content [right] minmax(min-content, 800px) [end];gap:12px;margin-top:24px}.tokens.svelte-1rpu49t.svelte-1rpu49t.svelte-1rpu49t{grid-row:main;grid-column-start:left;grid-column-end:end;cursor:pointer;height:min-content;line-height:110%}.tokens.svelte-1rpu49t .token.svelte-1rpu49t.svelte-1rpu49t{white-space:pre-wrap}.tokens.svelte-1rpu49t .selected.svelte-1rpu49t.svelte-1rpu49t{border:1px solid #999;z-index:10}.tokens.svelte-1rpu49t .token.svelte-1rpu49t.svelte-1rpu49t:not(.selected){z-index:0;padding:1px}.hover-mode.svelte-1rpu49t.svelte-1rpu49t.svelte-1rpu49t,.hover-mode-text.svelte-1rpu49t.svelte-1rpu49t.svelte-1rpu49t,.info-mode.svelte-1rpu49t.svelte-1rpu49t.svelte-1rpu49t,.info-mode-text.svelte-1rpu49t.svelte-1rpu49t.svelte-1rpu49t{color:#888;grid-row:title;grid-column:settings;cursor:pointer}.hover-mode-text.svelte-1rpu49t.svelte-1rpu49t.svelte-1rpu49t,.info-mode-text.svelte-1rpu49t.svelte-1rpu49t.svelte-1rpu49t{margin-right:8px}.heads.svelte-1rpu49t.svelte-1rpu49t.svelte-1rpu49t{grid-column:heads;grid-row:main;display:flex;flex-direction:row;flex-wrap:wrap;gap:6px;height:min-content}.heads.svelte-1rpu49t .head-icon.svelte-1rpu49t.svelte-1rpu49t{position:relative;width:62px;height:62px}.heads.svelte-1rpu49t .head-icon.svelte-1rpu49t>.svelte-1rpu49t{position:absolute;right:0px;top:0px}.heads.svelte-1rpu49t .head-icon .head-label.svelte-1rpu49t.svelte-1rpu49t{background:#333;color:#eee;font-size:65%;padding:1px;border-bottom-left-radius:2px;padding-left:4px;padding-right:2px;min-width:14px;opacity:0.75}\",append(document.head,style)}(),init(this,options,AttentionMulti_svelte_instance,AttentionMulti_svelte_create_fragment,safe_not_equal,{tokens:5,attention:6,info_weighted:7,head_labels:17,show_tokens:8,focus_token_lock:0,focus_head_lock:1,hover_token_is_target:2,_show_info_weighted:3,all_token_colors:4},[-1,-1])}}}},__webpack_module_cache__={};function __webpack_require__(moduleId){if(__webpack_module_cache__[moduleId])return __webpack_module_cache__[moduleId].exports;var module=__webpack_module_cache__[moduleId]={exports:{}};return __webpack_modules__[moduleId](module,module.exports,__webpack_require__),module.exports}return __webpack_require__.d=(exports,definition)=>{for(var key in definition)__webpack_require__.o(definition,key)&&!__webpack_require__.o(exports,key)&&Object.defineProperty(exports,key,{enumerable:!0,get:definition[key]})},__webpack_require__.o=(obj,prop)=>Object.prototype.hasOwnProperty.call(obj,prop),__webpack_require__(143)})().default;</script>\n",[m
        "        \n",[m
[31m-       "        <div id=\"AttentionMulti_f771f6\"></div>\n",[m
[32m+[m[32m       "        <div id=\"AttentionMulti_5a79874\"></div>\n",[m
        "        <script>\n",[m
        "        ( () => {\n",[m
        "            var data = {\n",[m
        "\"tokens\": [\n",[m
[32m+[m[32m       "\"<|endoftext|>\",\n",[m
        "\"Help\",\n",[m
        "\",\",\n",[m
        "\" I\",\n",[m
[36m@@ -3513,7 +2522,7 @@[m
        "],\n",[m
        "\"attention\": {\n",[m
        "\"__type__\": \"npy\",\n",[m
[31m-       "\"zdata\": \"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\",\n",[m
[32m+[m[32m       "\"zdata\": \"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\",\n",[m
        "\"min\": 0.0,\n",[m
        "\"max\": 1.0\n",[m
        "},\n",[m
[36m@@ -3522,7 +2531,7 @@[m
        "            data = loader.unpack_obj(data);\n",[m
        "            window.AttentionMulti_data = data;\n",[m
        "            var AttentionMulti_inst = new AttentionMulti({\n",[m
[31m-       "                \"target\": document.getElementById(\"AttentionMulti_f771f6\"),\n",[m
[32m+[m[32m       "                \"target\": document.getElementById(\"AttentionMulti_5a79874\"),\n",[m
        "                \"props\": data\n",[m
        "                });\n",[m
        "        })();\n",[m
[36m@@ -3543,7 +2552,7 @@[m
     "attn_pattern = einops.rearrange(vis_cache[f'blocks.{layer}.attn.hook_attn'][0], \n",[m
     "                                \"num_heads dest_pos src_pos -> dest_pos src_pos num_heads\") # Indexing into shape [batch, n_heads, dest_pos, src_pos]\n",[m
     "\n",[m
[31m-    "tokenized_text = text_to_token_strings(vis_text)\n",[m
[32m+[m[32m    "tokenized_text = model.to_str_tokens(vis_text)\n",[m
     "html_object = pysvelte.AttentionMulti(tokens=tokenized_text, attention=attn_pattern, head_labels=None)\n",[m
     "html_object.show()"[m
    ][m
[36m@@ -3554,14 +2563,17 @@[m
    "source": [[m
     "Earlier we saw that layer 5 contained some induction heads - can you figure out what they are from the above diagram? \n",[m
     "\n",[m
[31m-    "(Hint: What should the attention pattern visualised as a grid for each head look like?)"[m
[32m+[m[32m    "(Hint: What should the attention pattern visualised as a grid for each head look like?)\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "<details> <summary> Answer: </summary> Heads 1 and 5 strongly, head 0 weakly. Weirdly, head 8 seems somewhat induction-y here, but wasn't at all earlier - I'm not sure what's happening here, if you figure it out then please let me know! </details>"[m
    ][m
   },[m
   {[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "###Visualising Neuron Activations\n",[m
[32m+[m[32m    "### Visualising Neuron Activations\n",[m
     "\n",[m
     "We can also plot neuron activations over text - we input a list of the token in the text, and 1D array of activations, one per token. \n",[m
     "\n",[m
[36m@@ -3673,11 +2685,11 @@[m
    "cell_type": "markdown",[m
    "metadata": {},[m
    "source": [[m
[31m-    "##Training an Algorithmic Model\n",[m
[32m+[m[32m    "## Training an Algorithmic Model\n",[m
     "\n",[m
     "EasyTransformer also supports passing in custom config and initialising weights to create your own model. This isn't optimised for performance, so is likely best for training small LMs or small transformers for algorithmic tasks.\n",[m
     "\n",[m
[31m-    "We demonstrate training a small model to predict a string of constant tokens"[m
[32m+[m[32m    "We demonstrate training a (very!) small model to predict a string of consecutive numbers (with a random initial offset)"[m
    ][m
   },[m
   {[m
[36m@@ -3686,19 +2698,23 @@[m
    "metadata": {},[m
    "outputs": [],[m
    "source": [[m
[31m-    "training_cfg = EasyTransformerConfig(\n",[m
[32m+[m[32m    "tiny_cfg = EasyTransformerConfig(\n",[m
     "    d_model = 32,\n",[m
     "    d_head = 16,\n",[m
     "    n_heads = 2,\n",[m
     "    d_mlp = 128,\n",[m
     "    n_layers=1,\n",[m
[31m-    "    n_ctx = 20,\n",[m
[32m+[m[32m    "    n_ctx = 50,\n",[m
     "    act_fn='solu_ln',\n",[m
[31m-    "    d_vocab=100,\n",[m
[32m+[m[32m    "    d_vocab=150,\n",[m
     "    normalization_type='LN',\n",[m
[32m+[m[32m    "    seed=23, # Now we're training a custom model, it's good to set the seed to get reproducible results. It defaults to 42.\n",[m
     "    )\n",[m
[31m-    "tiny_model = EasyTransformer.from_config(training_cfg).to(device)\n",[m
[31m-    "tiny_optimizer = torch.optim.Adam(tiny_model.parameters(), lr=1e-3)"[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "tiny_model = EasyTransformer(tiny_cfg).to(device)\n",[m
[32m+[m[32m    "tiny_optimizer = torch.optim.Adam(tiny_model.parameters(), lr=1e-3)\n",[m
[32m+[m[32m    "batch_size = 20\n",[m
[32m+[m[32m    "num_epochs=301"[m
    ][m
   },[m
   {[m
[36m@@ -3709,7 +2725,7 @@[m
     {[m
      "data": {[m
       "application/vnd.jupyter.widget-view+json": {[m
[31m-       "model_id": "15e716928fe649c4abdaea25f0dcc58c",[m
[32m+[m[32m       "model_id": "8dbaa43142b64fdfa4b7fcf5e1526f14",[m
        "version_major": 2,[m
        "version_minor": 0[m
       },[m
[36m@@ -3724,17 +2740,19 @@[m
      "name": "stdout",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "Epoch: 0. Loss: 4.70389986038208\n",[m
[31m-      "Epoch: 100. Loss: 2.0002427101135254\n",[m
[31m-      "Epoch: 200. Loss: 0.6429383754730225\n",[m
[31m-      "Epoch: 300. Loss: 0.2464432418346405\n"[m
[32m+[m[32m      "Epoch: 0. Loss: 5.026259899139404\n",[m
[32m+[m[32m      "Epoch: 100. Loss: 1.738516092300415\n",[m
[32m+[m[32m      "Epoch: 200. Loss: 0.33298417925834656\n",[m
[32m+[m[32m      "Epoch: 300. Loss: 0.11569619923830032\n"[m
      ][m
     }[m
    ],[m
    "source": [[m
     "for epoch in tqdm.tqdm(range(301)):\n",[m
[31m-    "    random_int = torch.randint(0, 100, (20,))\n",[m
[31m-    "    batch = einops.repeat(random_int, 'batch -> batch pos', pos=tiny_model.cfg.n_ctx).to(device)\n",[m
[32m+[m[32m    "    batch_offset = torch.randint(0, 100, (20,))\n",[m
[32m+[m[32m    "    range_over_ctx = torch.arange(tiny_model.cfg.n_ctx)\n",[m
[32m+[m[32m    "    # Fancy indexing to get a batch of consecutive tokens, with each row starting with batch_offset\n",[m
[32m+[m[32m    "    batch = batch_offset[:, None] + range_over_ctx[None, :]\n",[m
     "    loss = tiny_model(batch, return_type='loss')\n",[m
     "    loss.backward()\n",[m
     "    tiny_optimizer.step()\n",[m
[36m@@ -3743,6 +2761,144 @@[m
     "        print(f\"Epoch: {epoch}. Loss: {loss}\")"[m
    ][m
   },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": null,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stdout",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "tensor(0.1146, device='cuda:0', grad_fn=<NegBackward0>)\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    }[m
[32m+[m[32m   ],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "loss, tiny_cache = tiny_model.run_with_cache(batch, return_type='loss')"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "markdown",[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "## Training a Language Model\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "Though EasyTransformer is not designed for high-performance model training, we provide some utilities for training small language models.\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "See train.py for an example training script, the following is how to use it for a simple training task:"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": null,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "micro_gpt_cfg = EasyTransformerConfig(\n",[m
[32m+[m[32m    "    d_model = 64,\n",[m
[32m+[m[32m    "    d_head = 32,\n",[m
[32m+[m[32m    "    n_heads = 2,\n",[m
[32m+[m[32m    "    d_mlp = 256,\n",[m
[32m+[m[32m    "    n_layers=3,\n",[m
[32m+[m[32m    "    n_ctx = 512,\n",[m
[32m+[m[32m    "    act_fn='gelu_new',\n",[m
[32m+[m[32m    "    normalization_type='LN',\n",[m
[32m+[m[32m    "    tokenizer_name='EleutherAI/gpt-neox-20b',\n",[m
[32m+[m[32m    "    )\n",[m
[32m+[m[32m    "micro_gpt = EasyTransformer(micro_gpt_cfg)"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "markdown",[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "We download 10K samples of the Pile (via a small utility dataset on HuggingFace), and use a utility to tokenize them, concatenate them (separated by EOS tokens), and reshape them into batches of size n_ctx"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": null,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stderr",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "Using custom data configuration NeelNanda--pile-10k-698b4c44102ba425\n",[m
[32m+[m[32m      "Reusing dataset parquet (/workspace/cache/NeelNanda___parquet/NeelNanda--pile-10k-698b4c44102ba425/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stdout",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      " "[m
[32m+[m[32m     ][m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stderr",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "Loading cached processed dataset at /workspace/cache/NeelNanda___parquet/NeelNanda--pile-10k-698b4c44102ba425/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-16d4ee1731535bf6.arrow\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stdout",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      " "[m
[32m+[m[32m     ][m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stderr",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "Loading cached processed dataset at /workspace/cache/NeelNanda___parquet/NeelNanda--pile-10k-698b4c44102ba425/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-9eff21dca5ec913c.arrow\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stdout",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      " "[m
[32m+[m[32m     ][m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stderr",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "Loading cached processed dataset at /workspace/cache/NeelNanda___parquet/NeelNanda--pile-10k-698b4c44102ba425/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-47632d7a5e7766f1.arrow\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stdout",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      " "[m
[32m+[m[32m     ][m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stderr",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "Loading cached processed dataset at /workspace/cache/NeelNanda___parquet/NeelNanda--pile-10k-698b4c44102ba425/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d1466a3d20801ca1.arrow\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    }[m
[32m+[m[32m   ],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",[m
[32m+[m[32m    "dataset = easy_transformer.utils.tokenize_and_concatenate(dataset, micro_gpt.tokenizer, max_length=micro_gpt.cfg.n_ctx, add_bos_token=False)"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "markdown",[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "As an example, we train our tiny model for 500 steps, of batch size 2, with AdamW"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
   {[m
    "cell_type": "code",[m
    "execution_count": null,[m
[36m@@ -3750,27 +2906,153 @@[m
    "outputs": [[m
     {[m
      "data": {[m
[32m+[m[32m      "application/vnd.jupyter.widget-view+json": {[m
[32m+[m[32m       "model_id": "68bbaab2680d4c2797ea2c89ba7ad4ea",[m
[32m+[m[32m       "version_major": 2,[m
[32m+[m[32m       "version_minor": 0[m
[32m+[m[32m      },[m
       "text/plain": [[m
[31m-       "Parameter containing:\n",[m
[31m-       "tensor([[-0.2011, -0.0070,  0.0844,  ...,  0.0310,  0.0827,  0.0282],\n",[m
[31m-       "        [-0.0823, -0.1351,  0.1912,  ...,  0.1648, -0.1778,  0.0144],\n",[m
[31m-       "        [-0.0794, -0.1046, -0.1132,  ...,  0.0497, -0.1960,  0.0225],\n",[m
[31m-       "        ...,\n",[m
[31m-       "        [-0.0221, -0.1382,  0.0742,  ...,  0.0045,  0.1767,  0.0143],\n",[m
[31m-       "        [-0.1435, -0.0913, -0.1271,  ...,  0.0784, -0.2603, -0.0963],\n",[m
[31m-       "        [-0.1051, -0.0659, -0.1182,  ..., -0.1384,  0.1496, -0.0220]],\n",[m
[31m-       "       device='cuda:0', requires_grad=True)"[m
[32m+[m[32m       "  0%|          | 0/1 [00:00<?, ?it/s]"[m
       ][m
      },[m
[31m-     "execution_count": null,[m
[32m+[m[32m     "metadata": {},[m
[32m+[m[32m     "output_type": "display_data"[m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "data": {[m
[32m+[m[32m      "application/vnd.jupyter.widget-view+json": {[m
[32m+[m[32m       "model_id": "c52ef3218c6c40e69691e8048b16b6c2",[m
[32m+[m[32m       "version_major": 2,[m
[32m+[m[32m       "version_minor": 0[m
[32m+[m[32m      },[m
[32m+[m[32m      "text/plain": [[m
[32m+[m[32m       "0it [00:00, ?it/s]"[m
[32m+[m[32m      ][m
[32m+[m[32m     },[m
[32m+[m[32m     "metadata": {},[m
[32m+[m[32m     "output_type": "display_data"[m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stdout",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "Epoch 1 Samples 2 Step 0 Loss 10.832801818847656\n",[m
[32m+[m[32m      "Epoch 1 Samples 102 Step 50 Loss 8.344871520996094\n",[m
[32m+[m[32m      "Epoch 1 Samples 202 Step 100 Loss 7.829599380493164\n",[m
[32m+[m[32m      "Epoch 1 Samples 302 Step 150 Loss 8.067302703857422\n",[m
[32m+[m[32m      "Epoch 1 Samples 402 Step 200 Loss 7.894046306610107\n",[m
[32m+[m[32m      "Epoch 1 Samples 502 Step 250 Loss 7.74192476272583\n",[m
[32m+[m[32m      "Epoch 1 Samples 602 Step 300 Loss 8.823657035827637\n",[m
[32m+[m[32m      "Epoch 1 Samples 702 Step 350 Loss 7.71836519241333\n",[m
[32m+[m[32m      "Epoch 1 Samples 802 Step 400 Loss 7.205349445343018\n",[m
[32m+[m[32m      "Epoch 1 Samples 902 Step 450 Loss 7.272680759429932\n",[m
[32m+[m[32m      "Epoch 1 Samples 1002 Step 500 Loss 6.653236389160156\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    }[m
[32m+[m[32m   ],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "training_cfg = easy_transformer.train.EasyTransformerTrainConfig(\n",[m
[32m+[m[32m    "    num_epochs = 1,\n",[m
[32m+[m[32m    "    batch_size = 2,\n",[m
[32m+[m[32m    "    weight_decay = 0.01,\n",[m
[32m+[m[32m    "    optimizer_name = 'AdamW',\n",[m
[32m+[m[32m    "    max_steps = 500,\n",[m
[32m+[m[32m    ")\n",[m
[32m+[m[32m    "micro_gpt = easy_transformer.train.train(micro_gpt, training_cfg, dataset)"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": null,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stdout",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "The autoreload extension is already loaded. To reload it, use:\n",[m
[32m+[m[32m      "  %reload_ext autoreload\n",[m
[32m+[m[32m      "In IPython\n",[m
[32m+[m[32m      "Set autoreload\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    }[m
[32m+[m[32m   ],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "from neel.imports import *"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "markdown",[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "## Generating Text\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "This isn't a core feature of the library, but is pretty useful to have! A key move in both ML and interpretability is to really get your hands dirty, play around with your models and your data, and try to understand what's going on. Generating a bunch of text and playing around is a good way to engage with that.\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "Thanks to Ansh Rahhakrishnan for adding this feature!"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": null,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "model = EasyTransformer.from_pretrained(\"gpt2-medium\")"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": null,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [[m
[32m+[m[32m    {[m
[32m+[m[32m     "data": {[m
[32m+[m[32m      "application/json": {[m
[32m+[m[32m       "ascii": false,[m
[32m+[m[32m       "bar_format": null,[m
[32m+[m[32m       "colour": null,[m
[32m+[m[32m       "elapsed": 0.016024351119995117,[m
[32m+[m[32m       "initial": 0,[m
[32m+[m[32m       "n": 0,[m
[32m+[m[32m       "ncols": null,[m
[32m+[m[32m       "nrows": 6,[m
[32m+[m[32m       "postfix": null,[m
[32m+[m[32m       "prefix": "",[m
[32m+[m[32m       "rate": null,[m
[32m+[m[32m       "total": 50,[m
[32m+[m[32m       "unit": "it",[m
[32m+[m[32m       "unit_divisor": 1000,[m
[32m+[m[32m       "unit_scale": false[m
[32m+[m[32m      },[m
[32m+[m[32m      "application/vnd.jupyter.widget-view+json": {[m
[32m+[m[32m       "model_id": "d63b5ff6e6104cc093bfe2c09d084712",[m
[32m+[m[32m       "version_major": 2,[m
[32m+[m[32m       "version_minor": 0[m
[32m+[m[32m      },[m
[32m+[m[32m      "text/plain": [[m
[32m+[m[32m       "  0%|          | 0/50 [00:00<?, ?it/s]"[m
[32m+[m[32m      ][m
[32m+[m[32m     },[m
[32m+[m[32m     "metadata": {},[m
[32m+[m[32m     "output_type": "display_data"[m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "data": {[m
[32m+[m[32m      "text/plain": [[m
[32m+[m[32m       "'<|endoftext|>The following work gives an insightful and original solution to the alignment problem:\\n\\nIn an effort to find a solution to the alignment problem, the authors propose a solution that is both elegant and simple to implement. The solution is based on the concept of \"boundedness\" and the concept of \"bounds\". The'"[m
[32m+[m[32m      ][m
[32m+[m[32m     },[m
[32m+[m[32m     "execution_count": 12,[m
      "metadata": {},[m
      "output_type": "execute_result"[m
     }[m
    ],[m
    "source": [[m
[31m-    "tiny_cache = {}\n",[m
[31m-    "tiny_model.cache_all(tiny_cache)\n",[m
[31m-    "loss = tiny_model(batch, return_type='loss')"[m
[32m+[m[32m    "prompt = \"The following work gives an insightful and original solution to the alignment problem:\"\n",[m
[32m+[m[32m    "model.generate(prompt, max_new_tokens=50, temperature=0.9, top_k=5)"[m
    ][m
   },[m
   {[m
[36m@@ -3783,17 +3065,25 @@[m
  ],[m
  "metadata": {[m
   "kernelspec": {[m
[31m-   "display_name": "Python 3.9.13 ('arthur_new_env')",[m
[32m+[m[32m   "display_name": "Python 3.9.12 ('base')",[m
    "language": "python",[m
    "name": "python3"[m
   },[m
   "language_info": {[m
[32m+[m[32m   "codemirror_mode": {[m
[32m+[m[32m    "name": "ipython",[m
[32m+[m[32m    "version": 3[m
[32m+[m[32m   },[m
[32m+[m[32m   "file_extension": ".py",[m
[32m+[m[32m   "mimetype": "text/x-python",[m
    "name": "python",[m
[31m-   "version": "3.9.13"[m
[32m+[m[32m   "nbconvert_exporter": "python",[m
[32m+[m[32m   "pygments_lexer": "ipython3",[m
[32m+[m[32m   "version": "3.9.12"[m
   },[m
   "vscode": {[m
    "interpreter": {[m
[31m-    "hash": "15e4f89301c6425a3984d8cc040212c6bd8cbe8b6825a953ec92c255245dc397"[m
[32m+[m[32m    "hash": "e0da427ee4e12dce923dd3966a5478522db96e71b54a9e09254c97953650efdb"[m
    }[m
   }[m
  },[m
[1mdiff --git a/Hacky-Interactive-Lexoscope.ipynb b/Hacky-Interactive-Lexoscope.ipynb[m
[1mnew file mode 100644[m
[1mindex 0000000..02fe2a0[m
[1m--- /dev/null[m
[1m+++ b/Hacky-Interactive-Lexoscope.ipynb[m
[36m@@ -0,0 +1,405 @@[m
[32m+[m[32m{[m
[32m+[m[32m "cells": [[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "markdown",[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "# Interactive Lexoscope\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "There's a surprisingly rich ecosystem of easy ways to create interactive graphics, especially for ML systems. If you're trying to do mechanistic interpretability, the ability to do web dev and to both visualize data and interact with it seems high value! \n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "This is a demo of how you can combine EasyTransformer and [Gradio](https://gradio.app/) to create an interactive Lexoscope - a visualization of a neuron's activations on text that will dynamically update as you edit the text. I don't particularly claim that this code is any *good*, but the goal is to illustrate what quickly hacking together a custom visualisation (while knowing fuck all about web dev, like me) can look like! (And as such, I try to explain the basic web dev concepts I use)\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "Note that you'll need to run the code yourself to get the interactive interface, so the cell at the bottom will be blank at first!"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "markdown",[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "## Setup"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": 1,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stdout",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "Running as a Jupyter notebook - intended for development only!\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    }[m
[32m+[m[32m   ],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "import os\n",[m
[32m+[m[32m    "try:\n",[m
[32m+[m[32m    "  import google.colab\n",[m
[32m+[m[32m    "  IN_COLAB = True\n",[m
[32m+[m[32m    "  print(\"Running as a Colab notebook\")\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "except:\n",[m
[32m+[m[32m    "  IN_COLAB = False\n",[m
[32m+[m[32m    "  print(\"Running as a Jupyter notebook - intended for development only!\")\n",[m
[32m+[m[32m    "  from IPython import get_ipython\n",[m
[32m+[m[32m    "  ipython = get_ipython()\n",[m
[32m+[m[32m    "  # Code to automatically update the EasyTransformer code as its edited without restarting the kernel\n",[m
[32m+[m[32m    "  ipython.magic(\"load_ext autoreload\")\n",[m
[32m+[m[32m    "  ipython.magic(\"autoreload 2\")\n",[m
[32m+[m[32m    "  "[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": 2,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "import os\n",[m
[32m+[m[32m    "if IN_COLAB:\n",[m
[32m+[m[32m    "    os.system('pip install git+https://github.com/neelnanda-io/Easy-Transformer.git')\n",[m
[32m+[m[32m    "    os.system('pip install gradio')"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": 3,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "import gradio as gr\n",[m
[32m+[m[32m    "from easy_transformer import EasyTransformer\n",[m
[32m+[m[32m    "from easy_transformer.utils import to_numpy\n",[m
[32m+[m[32m    "from IPython.display import HTML"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "markdown",[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "## Extracting Model Activations\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "We first write some code using EasyTransformer's cache to extract the neuron activations on a given layer and neuron, for a given text"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": 4,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "model_name = \"gpt2-small\"\n",[m
[32m+[m[32m    "model = EasyTransformer.from_pretrained(model_name)"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": 5,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "def get_neuron_acts(text, layer, neuron_index):\n",[m
[32m+[m[32m    "    # Hacky way to get out state from a single hook - we have a single element list and edit that list within the hook.\n",[m
[32m+[m[32m    "    cache = {}\n",[m
[32m+[m[32m    "    def caching_hook(act, hook):\n",[m
[32m+[m[32m    "        cache[\"activation\"] = act[0, :, neuron_index]\n",[m
[32m+[m[32m    "    model.run_with_hooks(text, fwd_hooks=[(f\"blocks.{layer}.mlp.hook_post\", caching_hook)])\n",[m
[32m+[m[32m    "    return to_numpy(cache[\"activation\"])"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "markdown",[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "We can run this function and verify that it gives vaguely sensible outputs"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": 6,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stdout",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "['<|endoftext|>', 'The', ' following', ' is', ' a', ' list', ' of', ' powers', ' of', ' 10', ':', ' 1', ',', ' 10', ',', ' 100', ',', ' 1000', ',', ' 10000', ',', ' 100', '000', ',', ' 100', '0000', ',', ' 100', '00000']\n",[m
[32m+[m[32m      "[-0.08643489 -0.14071977 -0.10398155 -0.12390741 -0.04058974 -0.11064898\n",[m
[32m+[m[32m      " -0.05189841 -0.1127612  -0.06905474 -0.1118938  -0.03059204 -0.10336912\n",[m
[32m+[m[32m      " -0.04322346  1.5935538  -0.14205772  2.5116613  -0.13316444  2.5196686\n",[m
[32m+[m[32m      " -0.11360876  3.076523   -0.11637457  0.5393893   2.349966   -0.14952165\n",[m
[32m+[m[32m      " -0.16476323  1.9449059  -0.13690168 -0.08802504  2.184884  ]\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    }[m
[32m+[m[32m   ],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "default_layer = 9\n",[m
[32m+[m[32m    "default_neuron_index = 652\n",[m
[32m+[m[32m    "default_text = \"The following is a list of powers of 10: 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000\"\n",[m
[32m+[m[32m    "print(model.to_str_tokens(default_text))\n",[m
[32m+[m[32m    "print(get_neuron_acts(default_text, default_layer, default_neuron_index))"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "markdown",[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "## Visualizing Model Activations\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "We now write some code to visualize the neuron activations on some text - we're going to hack something together which just does some string processing to make an HTML string, with each token element colored according to the intensity neuron activation. We normalize the neuron activations so they all lie in [0, 1]. You can do much better, but this is a useful proof of concept of what \"just hack stuff together\" can look like!\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "I'll be keeping neuron 562 in layer 9 as a running example, as it seems to activate strongly on powers of 10.\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "Note that this visualization is very sensitive to `max_val` and `min_val`! You can tune those to whatever seems reasonable for the distribution of neuron activations you care about - I generally default to `min_val=0` and `max_val` as the max activation across the dataset."[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": 7,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "# This is some CSS (tells us what style )to give each token a thin gray border, to make it easy to see token separation\n",[m
[32m+[m[32m    "style_string = \"\"\"<style> \n",[m
[32m+[m[32m    "    span.token {\n",[m
[32m+[m[32m    "        border: 1px solid rgb(123, 123, 123)\n",[m
[32m+[m[32m    "        } \n",[m
[32m+[m[32m    "    </style>\"\"\"\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "def calculate_color(val, max_val, min_val):\n",[m
[32m+[m[32m    "    # Hacky code that takes in a value val in range [min_val, max_val], normalizes it to [0, 1] and returns a color which interpolates between slightly off-white and red (0 = white, 1 = red)\n",[m
[32m+[m[32m    "    # We return a string of the form \"rgb(240, 240, 240)\" which is a color CSS knows \n",[m
[32m+[m[32m    "    normalized_val = (val - min_val)/max_val\n",[m
[32m+[m[32m    "    return f\"rgb(240, {240*(1-normalized_val)}, {240*(1-normalized_val)})\"\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "def shitty_neuron_vis(text, layer, neuron_index, max_val=None, min_val=None):\n",[m
[32m+[m[32m    "    \"\"\" \n",[m
[32m+[m[32m    "    text: The text to visualize\n",[m
[32m+[m[32m    "    layer: The layer index\n",[m
[32m+[m[32m    "    neuron_index: The neuron index\n",[m
[32m+[m[32m    "    max_val: The top end of our activation range, defaults to the maximum activation\n",[m
[32m+[m[32m    "    min_val: The top end of our activation range, defaults to the minimum activation\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "    Returns a string of HTML that displays the text with each token colored according to its activation\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "    Note: It's useful to be able to input a fixed max_val and min_val, because otherwise the colors will change as you edit the text, which is annoying.\n",[m
[32m+[m[32m    "    \"\"\"\n",[m
[32m+[m[32m    "    if layer is None:\n",[m
[32m+[m[32m    "        return \"Please select a Layer\"\n",[m
[32m+[m[32m    "    if neuron_index is None:\n",[m
[32m+[m[32m    "        return \"Please select a Neuron\"\n",[m
[32m+[m[32m    "    acts = get_neuron_acts(text, layer, neuron_index)\n",[m
[32m+[m[32m    "    act_max = acts.max()\n",[m
[32m+[m[32m    "    act_min = acts.min()\n",[m
[32m+[m[32m    "    # Defaults to the max and min of the activations\n",[m
[32m+[m[32m    "    if max_val is None:\n",[m
[32m+[m[32m    "        max_val = act_max\n",[m
[32m+[m[32m    "    if min_val is None:\n",[m
[32m+[m[32m    "        min_val = act_min\n",[m
[32m+[m[32m    "    # We want to make a list of HTML strings to concatenate into our final HTML string\n",[m
[32m+[m[32m    "    # We first add the style to make each token element have a nice border\n",[m
[32m+[m[32m    "    htmls = [style_string]\n",[m
[32m+[m[32m    "    # We then add some text to tell us what layer and neuron we're looking at - we're just dealing with strings and can use f-strings as normal\n",[m
[32m+[m[32m    "    # h4 means \"small heading\"\n",[m
[32m+[m[32m    "    htmls.append(f\"<h4>Layer: <b>{layer}</b>. Neuron Index: <b>{neuron_index}</b></h4>\")\n",[m
[32m+[m[32m    "    # We then add a line telling us the limits of our range\n",[m
[32m+[m[32m    "    htmls.append(f\"<h4>Max Range: <b>{max_val:.4f}</b>. Min Range: <b>{min_val:.4f}</b></h4>\")\n",[m
[32m+[m[32m    "    # If we added a custom range, print a line telling us the range of our activations too.\n",[m
[32m+[m[32m    "    if act_max!=max_val or act_min!=min_val:\n",[m
[32m+[m[32m    "        htmls.append(f\"<h4>Custom Range Set. Max Act: <b>{act_max:.4f}</b>. Min Act: <b>{act_min:.4f}</b></h4>\")\n",[m
[32m+[m[32m    "    # Convert the text to a list of tokens\n",[m
[32m+[m[32m    "    str_tokens = model.to_str_tokens(text)\n",[m
[32m+[m[32m    "    for tok, act in zip(str_tokens, acts):\n",[m
[32m+[m[32m    "        # A span is an HTML element that lets us style a part of a string (and remains on the same line by default)\n",[m
[32m+[m[32m    "        # We set the background color of the span to be the color we calculated from the activation\n",[m
[32m+[m[32m    "        # We set the contents of the span to be the token\n",[m
[32m+[m[32m    "        htmls.append(f\"<span class='token' style='background-color:{calculate_color(act, max_val, min_val)}' >{tok}</span>\")\n",[m
[32m+[m[32m    "    \n",[m
[32m+[m[32m    "    return \"\".join(htmls)"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": 8,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stdout",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "Displayed HTML\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "data": {[m
[32m+[m[32m      "text/html": [[m
[32m+[m[32m       "<style> \n",[m
[32m+[m[32m       "    span.token {\n",[m
[32m+[m[32m       "        border: 1px solid rgb(123, 123, 123)\n",[m
[32m+[m[32m       "        } \n",[m
[32m+[m[32m       "    </style><h4>Layer: <b>9</b>. Neuron Index: <b>652</b></h4><h4>Max Range: <b>4.0000</b>. Min Range: <b>0.0000</b></h4><h4>Custom Range Set. Max Act: <b>3.0765</b>. Min Act: <b>-0.1648</b></h4><span class='token' style='background-color:rgb(240, 245.1860935986042, 245.1860935986042)' ><|endoftext|></span><span class='token' style='background-color:rgb(240, 248.44318628311157, 248.44318628311157)' >The</span><span class='token' style='background-color:rgb(240, 246.23889327049255, 246.23889327049255)' > following</span><span class='token' style='background-color:rgb(240, 247.43444457650185, 247.43444457650185)' > is</span><span class='token' style='background-color:rgb(240, 242.43538431823254, 242.43538431823254)' > a</span><span class='token' style='background-color:rgb(240, 246.63893893361092, 246.63893893361092)' > list</span><span class='token' style='background-color:rgb(240, 243.11390474438667, 243.11390474438667)' > of</span><span class='token' style='background-color:rgb(240, 246.76567196846008, 246.76567196846008)' > powers</span><span class='token' style='background-color:rgb(240, 244.14328426122665, 244.14328426122665)' > of</span><span class='token' style='background-color:rgb(240, 246.71362817287445, 246.71362817287445)' > 10</span><span class='token' style='background-color:rgb(240, 241.83552224189043, 241.83552224189043)' >:</span><span class='token' style='background-color:rgb(240, 246.20214700698853, 246.20214700698853)' > 1</span><span class='token' style='background-color:rgb(240, 242.59340777993202, 242.59340777993202)' >,</span><span class='token' style='background-color:rgb(240, 144.38677310943604, 144.38677310943604)' > 10</span><span class='token' style='background-color:rgb(240, 248.52346301078796, 248.52346301078796)' >,</span><span class='token' style='background-color:rgb(240, 89.30032253265381, 89.30032253265381)' > 100</span><span class='token' style='background-color:rgb(240, 247.98986613750458, 247.98986613750458)' >,</span><span class='token' style='background-color:rgb(240, 88.81988525390625, 88.81988525390625)' > 1000</span><span class='token' style='background-color:rgb(240, 246.81652531027794, 246.81652531027794)' >,</span><span class='token' style='background-color:rgb(240, 55.408616065979004, 55.408616065979004)' > 10000</span><span class='token' style='background-color:rgb(240, 246.98247447609901, 246.98247447609901)' >,</span><span class='token' style='background-color:rgb(240, 207.6366412639618, 207.6366412639618)' > 100</span><span class='token' style='background-color:rgb(240, 99.00203704833984, 99.00203704833984)' >000</span><span class='token' style='background-color:rgb(240, 248.97129893302917, 248.97129893302917)' >,</span><span class='token' style='background-color:rgb(240, 249.88579362630844, 249.88579362630844)' > 100</span><span class='token' style='background-color:rgb(240, 123.30564737319946, 123.30564737319946)' >0000</span><span class='token' style='background-color:rgb(240, 248.21410059928894, 248.21410059928894)' >,</span><span class='token' style='background-color:rgb(240, 245.28150245547295, 245.28150245547295)' > 100</span><span class='token' style='background-color:rgb(240, 108.90695571899414, 108.90695571899414)' >00000</span>"[m
[32m+[m[32m      ],[m
[32m+[m[32m      "text/plain": [[m
[32m+[m[32m       "<IPython.core.display.HTML object>"[m
[32m+[m[32m      ][m
[32m+[m[32m     },[m
[32m+[m[32m     "metadata": {},[m
[32m+[m[32m     "output_type": "display_data"[m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stdout",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "HTML String - it's just raw HTML code!\n",[m
[32m+[m[32m      "<style> \n",[m
[32m+[m[32m      "    span.token {\n",[m
[32m+[m[32m      "        border: 1px solid rgb(123, 123, 123)\n",[m
[32m+[m[32m      "        } \n",[m
[32m+[m[32m      "    </style><h4>Layer: <b>9</b>. Neuron Index: <b>652</b></h4><h4>Max Range: <b>4.0000</b>. Min Range: <b>0.0000</b></h4><h4>Custom Range Set. Max Act: <b>3.0765</b>. Min Act: <b>-0.1648</b></h4><span class='token' style='background-color:rgb(240, 245.1860935986042, 245.1860935986042)' ><|endoftext|></span><span class='token' style='background-color:rgb(240, 248.44318628311157, 248.44318628311157)' >The</span><span class='token' style='background-color:rgb(240, 246.23889327049255, 246.23889327049255)' > following</span><span class='token' style='background-color:rgb(240, 247.43444457650185, 247.43444457650185)' > is</span><span class='token' style='background-color:rgb(240, 242.43538431823254, 242.43538431823254)' > a</span><span class='token' style='background-color:rgb(240, 246.63893893361092, 246.63893893361092)' > list</span><span class='token' style='background-color:rgb(240, 243.11390474438667, 243.11390474438667)' > of</span><span class='token' style='background-color:rgb(240, 246.76567196846008, 246.76567196846008)' > powers</span><span class='token' style='background-color:rgb(240, 244.14328426122665, 244.14328426122665)' > of</span><span class='token' style='background-color:rgb(240, 246.71362817287445, 246.71362817287445)' > 10</span><span class='token' style='background-color:rgb(240, 241.83552224189043, 241.83552224189043)' >:</span><span class='token' style='background-color:rgb(240, 246.20214700698853, 246.20214700698853)' > 1</span><span class='token' style='background-color:rgb(240, 242.59340777993202, 242.59340777993202)' >,</span><span class='token' style='background-color:rgb(240, 144.38677310943604, 144.38677310943604)' > 10</span><span class='token' style='background-color:rgb(240, 248.52346301078796, 248.52346301078796)' >,</span><span class='token' style='background-color:rgb(240, 89.30032253265381, 89.30032253265381)' > 100</span><span class='token' style='background-color:rgb(240, 247.98986613750458, 247.98986613750458)' >,</span><span class='token' style='background-color:rgb(240, 88.81988525390625, 88.81988525390625)' > 1000</span><span class='token' style='background-color:rgb(240, 246.81652531027794, 246.81652531027794)' >,</span><span class='token' style='background-color:rgb(240, 55.408616065979004, 55.408616065979004)' > 10000</span><span class='token' style='background-color:rgb(240, 246.98247447609901, 246.98247447609901)' >,</span><span class='token' style='background-color:rgb(240, 207.6366412639618, 207.6366412639618)' > 100</span><span class='token' style='background-color:rgb(240, 99.00203704833984, 99.00203704833984)' >000</span><span class='token' style='background-color:rgb(240, 248.97129893302917, 248.97129893302917)' >,</span><span class='token' style='background-color:rgb(240, 249.88579362630844, 249.88579362630844)' > 100</span><span class='token' style='background-color:rgb(240, 123.30564737319946, 123.30564737319946)' >0000</span><span class='token' style='background-color:rgb(240, 248.21410059928894, 248.21410059928894)' >,</span><span class='token' style='background-color:rgb(240, 245.28150245547295, 245.28150245547295)' > 100</span><span class='token' style='background-color:rgb(240, 108.90695571899414, 108.90695571899414)' >00000</span>\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    }[m
[32m+[m[32m   ],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "# The function outputs a string of HTML\n",[m
[32m+[m[32m    "default_max_val = 4.0\n",[m
[32m+[m[32m    "default_min_val = 0.0\n",[m
[32m+[m[32m    "default_html_string = shitty_neuron_vis(default_text, default_layer, default_neuron_index, max_val=default_max_val, min_val=default_min_val)\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "# IPython lets us display HTML\n",[m
[32m+[m[32m    "print(\"Displayed HTML\")\n",[m
[32m+[m[32m    "display(HTML(default_html_string))\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "# We can also print the string directly\n",[m
[32m+[m[32m    "print(\"HTML String - it's just raw HTML code!\")\n",[m
[32m+[m[32m    "print(default_html_string)"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "markdown",[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "## Create Interactive UI\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "We now put all these together to create an interactive visualization in Gradio! \n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "The internal format is that there's a bunch of elements - Textboxes, Numbers, etc which the user can interact with and which return strings and numbers. And we can also define output elements that just display things - in this case, one which takes in an arbitrary HTML string. We call `input.change(update_function, inputs, output)` - this says \"if that input element changes, run the update function on the value of each of the elements in `inputs` and set the value of `output` to the output of the function\". As a bonus, this gives us live interactivity!\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "This is also more complex than a typical Gradio intro example - I wanted to use custom HTML to display the nice colours, which made things much messier! Normally you could just make `out` into another Textbox and pass it a string."[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": 9,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "# The `with gr.Blocks() as demo:` syntax just creates a variable called demo containing all these components\n",[m
[32m+[m[32m    "with gr.Blocks() as demo:\n",[m
[32m+[m[32m    "    gr.HTML(value=f\"Hacky Interactive Lexoscope for {model_name}\")\n",[m
[32m+[m[32m    "    # The input elements\n",[m
[32m+[m[32m    "    with gr.Row():\n",[m
[32m+[m[32m    "        with gr.Column():\n",[m
[32m+[m[32m    "            text = gr.Textbox(label=\"Text\", value=default_text)\n",[m
[32m+[m[32m    "            # Precision=0 makes it an int, otherwise it's a float\n",[m
[32m+[m[32m    "            # Value sets the initial default value\n",[m
[32m+[m[32m    "            layer = gr.Number(label=\"Layer\", value=default_layer, precision=0)\n",[m
[32m+[m[32m    "            neuron_index = gr.Number(label=\"Neuron Index\", value=default_neuron_index, precision=0)\n",[m
[32m+[m[32m    "            # If empty, these two map to None\n",[m
[32m+[m[32m    "            max_val = gr.Number(label=\"Max Value\", value=default_max_val)\n",[m
[32m+[m[32m    "            min_val = gr.Number(label=\"Min Value\", value=default_min_val)\n",[m
[32m+[m[32m    "            inputs = [text, layer, neuron_index, max_val, min_val]\n",[m
[32m+[m[32m    "        with gr.Column():\n",[m
[32m+[m[32m    "            # The output element\n",[m
[32m+[m[32m    "            out = gr.HTML(\n",[m
[32m+[m[32m    "                label=\"Neuron Acts\", \n",[m
[32m+[m[32m    "                value=default_html_string)\n",[m
[32m+[m[32m    "    for inp in inputs:\n",[m
[32m+[m[32m    "        inp.change(shitty_neuron_vis, inputs, out)"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "markdown",[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "We can now launch our demo element, and we're done! The setting share=True even gives you a public link to the demo (though it just redirects to the backend run by this notebook, and will go away once you turn the notebook off!) Sharing makes it much slower, and can be turned off if you aren't in a colab.\n",[m
[32m+[m[32m    "\n",[m
[32m+[m[32m    "**Exercise:** Explore where this neuron does and does not activate. Is it just powers of ten? Just comma separated numbers? Numbers in any particular sequence?"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": 14,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [[m
[32m+[m[32m    {[m
[32m+[m[32m     "name": "stdout",[m
[32m+[m[32m     "output_type": "stream",[m
[32m+[m[32m     "text": [[m
[32m+[m[32m      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",[m
[32m+[m[32m      "----\n",[m
[32m+[m[32m      "Running on local URL:  http://127.0.0.1:7860\n",[m
[32m+[m[32m      "Running on public URL: https://c77f5882d648162d.gradio.app\n",[m
[32m+[m[32m      "\n",[m
[32m+[m[32m      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "data": {[m
[32m+[m[32m      "text/html": [[m
[32m+[m[32m       "<div><iframe src=\"https://c77f5882d648162d.gradio.app\" width=\"900\" height=\"1000\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"[m
[32m+[m[32m      ],[m
[32m+[m[32m      "text/plain": [[m
[32m+[m[32m       "<IPython.core.display.HTML object>"[m
[32m+[m[32m      ][m
[32m+[m[32m     },[m
[32m+[m[32m     "metadata": {},[m
[32m+[m[32m     "output_type": "display_data"[m
[32m+[m[32m    },[m
[32m+[m[32m    {[m
[32m+[m[32m     "data": {[m
[32m+[m[32m      "text/plain": [[m
[32m+[m[32m       "(<gradio.routes.App at 0x7f9820050c50>,\n",[m
[32m+[m[32m       " 'http://127.0.0.1:7860/',\n",[m
[32m+[m[32m       " 'https://c77f5882d648162d.gradio.app')"[m
[32m+[m[32m      ][m
[32m+[m[32m     },[m
[32m+[m[32m     "execution_count": 14,[m
[32m+[m[32m     "metadata": {},[m
[32m+[m[32m     "output_type": "execute_result"[m
[32m+[m[32m    }[m
[32m+[m[32m   ],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "demo.launch(share=True, height=1000)"[m
[32m+[m[32m   ][m
[32m+[m[32m  }[m
[32m+[m[32m ],[m
[32m+[m[32m "metadata": {[m
[32m+[m[32m  "kernelspec": {[m
[32m+[m[32m   "display_name": "Python 3.7.13 ('base')",[m
[32m+[m[32m   "language": "python",[m
[32m+[m[32m   "name": "python3"[m
[32m+[m[32m  },[m
[32m+[m[32m  "language_info": {[m
[32m+[m[32m   "codemirror_mode": {[m
[32m+[m[32m    "name": "ipython",[m
[32m+[m[32m    "version": 3[m
[32m+[m[32m   },[m
[32m+[m[32m   "file_extension": ".py",[m
[32m+[m[32m   "mimetype": "text/x-python",[m
[32m+[m[32m   "name": "python",[m
[32m+[m[32m   "nbconvert_exporter": "python",[m
[32m+[m[32m   "pygments_lexer": "ipython3",[m
[32m+[m[32m   "version": "3.7.13"[m
[32m+[m[32m  },[m
[32m+[m[32m  "orig_nbformat": 4,[m
[32m+[m[32m  "vscode": {[m
[32m+[m[32m   "interpreter": {[m
[32m+[m[32m    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"[m
[32m+[m[32m   }[m
[32m+[m[32m  }[m
[32m+[m[32m },[m
[32m+[m[32m "nbformat": 4,[m
[32m+[m[32m "nbformat_minor": 2[m
[32m+[m[32m}[m
[1mdiff --git a/README.md b/README.md[m
[1mindex 08ad544..34ec555 100644[m
[1m--- a/README.md[m
[1m+++ b/README.md[m
[36m@@ -1,14 +1,13 @@[m
[32m+[m[32mThis repository contains the code for all experiments in the paper "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small" (Wang et al, 2022).[m
[32m+[m
 # Easy Transformer[m
 [m
 ## An implementation of transformers tailored for mechanistic interpretability.[m
 [m
[31m-[m
[31m-[m
[31m-[m
 It supports the importation of open sources models, a convenient handling of hooks [m
 to get access to intermediate activations and features to perform simple emperiments such as ablations and patching.[m
 [m
[31m-A demo notebook can be found [here](https://colab.research.google.com/drive/1MLwJ7P94cizVs2LD8Qwi-vLGSoH-cHxq) and a more comprehensive description of the library can be found [here](https://colab.research.google.com/drive/1_tH4PfRSPYuKGnJbhC1NqFesOYuXrir_#scrollTo=zs8juArnyuyB)[m
[32m+[m[32mA demo notebook can be found [here](https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/EasyTransformer_Demo.ipynb) and a more comprehensive description of the library can be found [here](https://colab.research.google.com/drive/1_tH4PfRSPYuKGnJbhC1NqFesOYuXrir_#scrollTo=zs8juArnyuyB)[m
 [m
 [m
 ## Installation[m
[1mdiff --git a/arthurs_experiments.py b/arthurs_experiments.py[m
[1mindex 52b3cc7..f346e21 100644[m
[1m--- a/arthurs_experiments.py[m
[1m+++ b/arthurs_experiments.py[m
[36m@@ -114,9 +114,10 @@[m [mdef e(mess=""):[m
 [m
 [m
 #%%[m
[31m-model = EasyTransformer("gpt2", use_attn_result=True).cuda()[m
[32m+[m[32mmodel = EasyTransformer.from_pretrained("gpt2") # , use_attn_result=True)[m
[32m+[m[32mmodel.set_use_attn_result(True)[m
 N = 100[m
[31m-ioi_dataset = IOIDataset(prompt_type="mixed", N=N, tokenizer=model.tokenizer)[m
[32m+[m[32mioi_dataset = IOIDataset(prompt_type="mixed", N=N, tokenizer=model.tokenizer, prepend_bos=False, has_start_padding_and_start_is_end=False)[m
 abca_dataset = ioi_dataset.gen_flipped_prompts([m
     ("S2", "RAND")[m
 )  # we flip the second b for a random c[m
[1mdiff --git a/easy_transformer/EasyTransformer.py b/easy_transformer/EasyTransformer.py[m
[1mindex 26edc10..4d03082 100644[m
[1m--- a/easy_transformer/EasyTransformer.py[m
[1m+++ b/easy_transformer/EasyTransformer.py[m
[36m@@ -1,37 +1,15 @@[m
[31m-from mimetypes import init[m
 from typing import Callable, Union, List, Tuple, Dict, Optional[m
[32m+[m[32mfrom mimetypes import init[m
 import torch[m
 import torch.nn as nn[m
 import torch.nn.functional as F[m
[31m-import torch.optim as optim[m
 import numpy as np[m
 import einops[m
 import logging[m
[31m-[m
[31m-from tqdm import tqdm[m
[31m-import random[m
[31m-import time[m
[31m-[m
[31m-from pathlib import Path[m
[31m-import pickle[m
[31m-import os[m
[31m-[m
[31m-import matplotlib.pyplot as plt[m
[31m-import plotly.express as px[m
[31m-import plotly.io as pio[m
[31m-[m
[31m-import plotly.graph_objects as go[m
[31m-[m
[31m-from torch.utils.data import DataLoader[m
[31m-[m
[31m-from functools import *[m
[31m-import pandas as pd[m
[31m-import gc[m
[31m-import collections[m
[31m-import copy[m
[31m-[m
[31m-# import comet_ml[m
[31m-import itertools[m
[32m+[m[32mimport tqdm.auto as tqdm[m
[32m+[m[32mimport re[m
[32m+[m[32mfrom huggingface_hub import HfApi[m
[32m+[m[32mfrom functools import partial, lru_cache[m
 [m
 from transformers import ([m
     AutoModelForCausalLM,[m
[36m@@ -41,70 +19,21 @@[m [mfrom transformers import ([m
 )[m
 [m
 from easy_transformer.hook_points import HookedRootModule, HookPoint[m
[31m-from easy_transformer.utils import ([m
[31m-    gelu_new,[m
[31m-    to_numpy,[m
[31m-    get_corner,[m
[31m-    print_gpu_mem,[m
[31m-    get_sample_from_dataset,[m
[31m-    solu,[m
[31m-    reglu,[m
[31m-    geglu,[m
[31m-    swiglu,[m
[31m-)[m
[31m-from easy_transformer.EasyTransformerConfig import EasyTransformerConfig[m
[31m-[m
[31m-VALID_MODEL_NAMES = set([m
[31m-    [[m
[31m-        "gpt2",[m
[31m-        "gpt2-medium",[m
[31m-        "gpt2-large",[m
[31m-        "gpt2-xl",[m
[31m-        "facebook/opt-125m",[m
[31m-        "facebook/opt-1.3b",[m
[31m-        "facebook/opt-2.7b",[m
[31m-        "facebook/opt-6.7b",[m
[31m-        "facebook/opt-13b",[m
[31m-        "facebook/opt-30b",[m
[31m-        "facebook/opt-66b",[m
[31m-        "EleutherAI/gpt-neo-125M",[m
[31m-        "EleutherAI/gpt-neo-1.3B",[m
[31m-        "EleutherAI/gpt-neo-2.7B",[m
[31m-        "stanford-gpt2-small-A",[m
[31m-        "stanford-gpt2-small-B",[m
[31m-        "stanford-gpt2-small-C",[m
[31m-        "stanford-gpt2-small-D",[m
[31m-        "stanford-gpt2-small-E",[m
[31m-        "stanford-gpt2-medium-A",[m
[31m-        "stanford-gpt2-medium-B",[m
[31m-        "stanford-gpt2-medium-C",[m
[31m-        "stanford-gpt2-medium-D",[m
[31m-        "stanford-gpt2-medium-E",[m
[31m-    ][m
[31m-)[m
[32m+[m[32mfrom easy_transformer import EasyTransformerConfig[m
 [m
[31m-MODEL_NAMES_DICT = {[m
[31m-    "stanford-gpt2-small-A": "stanford-crfm/alias-gpt2-small-x21",[m
[31m-    "stanford-gpt2-small-B": "stanford-crfm/battlestar-gpt2-small-x49",[m
[31m-    "stanford-gpt2-small-C": "stanford-crfm/caprica-gpt2-small-x81",[m
[31m-    "stanford-gpt2-small-D": "stanford-crfm/darkmatter-gpt2-small-x343",[m
[31m-    "stanford-gpt2-small-E": "stanford-crfm/expanse-gpt2-small-x777",[m
[31m-    "stanford-gpt2-medium-A": "stanford-crfm/arwen-gpt2-medium-x21",[m
[31m-    "stanford-gpt2-medium-B": "stanford-crfm/beren-gpt2-medium-x49",[m
[31m-    "stanford-gpt2-medium-C": "stanford-crfm/celebrimbor-gpt2-medium-x81",[m
[31m-    "stanford-gpt2-medium-D": "stanford-crfm/durin-gpt2-medium-x343",[m
[31m-    "stanford-gpt2-medium-E": "stanford-crfm/eowyn-gpt2-medium-x777",[m
[31m-}[m
[31m-# The steps for which there are checkpoints in the stanford crfm models - provided as reference[m
[31m-STANFORD_CRFM_CHECKPOINTS = ([m
[31m-    list(range(0, 100, 10))[m
[31m-    + list(range(100, 2000, 50))[m
[31m-    + list(range(2000, 20000, 100))[m
[31m-    + list(range(20000, 400000 + 1, 1000))[m
[32m+[m[32mfrom easy_transformer.caching import ([m
[32m+[m[32m    EasyTransformerKeyValueCache,[m
[32m+[m[32m    EasyTransformerKeyValueCacheEntry,[m
 )[m
 [m
[31m-# TODO: Add Bloom, GPT-J and GPT-NeoX[m
[32m+[m[32mfrom easy_transformer.components import *[m
[32m+[m[32mimport easy_transformer.weight_conversion as weight_conversion[m
[32m+[m[32mfrom easy_transformer.utils import lm_cross_entropy_loss, sample_logits, download_file_from_hf, FactoredMatrix, composition_scores[m
[32m+[m
[32m+[m
[32m+[m
 """[m
[32m+[m[32mTODO: Add Bloom, GPT-J and GPT-NeoX[m
 EleutherAI/gpt-j-6B[m
 EleutherAI/gpt-neox-20b[m
 bloom-350m[m
[36m@@ -116,469 +45,85 @@[m [mbloom (176B parameters)[m
 https://huggingface.co/docs/transformers/model_doc/bloom[m
 """[m
 [m
[31m-# Define network architecture[m
[31m-[m
[31m-# Embed & Unembed[m
[31m-class Embed(nn.Module):[m
[31m-    def __init__(self, cfg: Union[Dict, EasyTransformerConfig]):[m
[31m-        super().__init__()[m
[31m-        if isinstance(cfg, Dict):[m
[31m-            cfg = EasyTransformerConfig.from_dict(cfg)[m
[31m-        self.cfg = cfg[m
[31m-        self.W_E = nn.Parameter(torch.empty(self.cfg.d_model, self.cfg.d_vocab))[m
[31m-[m
[31m-    def forward(self, tokens):[m
[31m-        # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d][m
[31m-        # B acts as a tensor of indices into the second dimension (so >=0 and <b)[m
[31m-        return einops.rearrange([m
[31m-            self.W_E[:, tokens], "d_model batch pos -> batch pos d_model"[m
[31m-        )[m
[31m-[m
[31m-[m
[31m-class Unembed(nn.Module):[m
[31m-    def __init__(self, cfg: Union[Dict, EasyTransformerConfig]):[m
[31m-        super().__init__()[m
[31m-        if isinstance(cfg, Dict):[m
[31m-            cfg = EasyTransformerConfig.from_dict(cfg)[m
[31m-        self.cfg = cfg[m
[31m-        self.W_U = nn.Parameter(torch.empty(self.cfg.d_vocab, self.cfg.d_model))[m
[31m-        self.b_U = nn.Parameter(torch.empty(self.cfg.d_vocab))[m
[31m-[m
[31m-    def forward(self, tokens):[m
[31m-        return ([m
[31m-            torch.einsum("vm,bpm->bpv", self.W_U, tokens) + self.b_U[m
[31m-        )  # [batch, pos, d_vocab][m
[31m-[m
[31m-[m
[31m-# Positional Embeddings[m
[31m-class PosEmbed(nn.Module):[m
[31m-    def __init__(self, cfg: Union[Dict, EasyTransformerConfig]):[m
[31m-        super().__init__()[m
[31m-        if isinstance(cfg, Dict):[m
[31m-            cfg = EasyTransformerConfig.from_dict(cfg)[m
[31m-        self.cfg = cfg[m
[31m-        self.W_pos = nn.Parameter(torch.empty(self.cfg.d_model, self.cfg.n_ctx))[m
[31m-[m
[31m-    def forward(self, x):[m
[31m-        # Output shape [pos, d_model] - will be broadcast along batch dim[m
[31m-        return self.W_pos[:, : x.size(-1)].T  # [pos, d_model][m
[31m-[m
[31m-[m
[31m-# LayerNormPre[m
[31m-# I fold the LayerNorm weights and biases into later weights and biases.[m
[31m-# This is just the 'center and normalise' part of LayerNorm[m
[31m-# Centering is equivalent to just deleting one direction of residual space,[m
[31m-# and is equivalent to centering the weight matrices of everything writing to the residual stream[m
[31m-# Normalising is a funkier non-linear operation, that projects the residual stream onto the unit hypersphere[m
[31m-class LayerNormPre(nn.Module):[m
[31m-    def __init__(self, cfg: Union[Dict, EasyTransformerConfig]):[m
[31m-        """LayerNormPre - the 'center and normalise' part of LayerNorm. Length is[m
[31m-        normally d_model, but is d_mlp for softmax. Not needed as a parameter. This[m
[31m-        should only be used in inference mode after folding in LayerNorm weights"""[m
[31m-        super().__init__()[m
[31m-        if isinstance(cfg, Dict):[m
[31m-            cfg = EasyTransformerConfig.from_dict(cfg)[m
[31m-        self.cfg = cfg[m
[31m-        self.eps = self.cfg.eps[m
[31m-[m
[31m-        # Adds a hook point for the normalisation scale factor[m
[31m-        self.hook_scale = HookPoint()  # [batch, pos][m
[31m-        self.hook_normalized = HookPoint()  # [batch, pos, length][m
[31m-[m
[31m-    def forward(self, x):[m
[31m-        x = x - x.mean(axis=-1, keepdim=True)  # [batch, pos, length][m
[31m-        scale = self.hook_scale([m
[31m-            ([m
[31m-                einops.reduce(x.pow(2), "batch pos embed -> batch pos 1", "mean")[m
[31m-                + self.eps[m
[31m-            ).sqrt()[m
[31m-        )  # [batch, pos, 1][m
[31m-        return self.hook_normalized(x / scale)  # [batch, pos, length][m
[31m-[m
[31m-[m
[31m-class LayerNorm(nn.Module):[m
[31m-    def __init__([m
[31m-        self, cfg: Union[Dict, EasyTransformerConfig], length: Optional[int] = None[m
[31m-    ):[m
[31m-[m
[31m-        """[m
[31m-        LayerNorm with optional length parameter[m
[31m-[m
[31m-        length (Optional[int]): If the dimension of the LayerNorm. If not provided, assumed to be d_model[m
[31m-        """[m
[31m-        super().__init__()[m
[31m-        if isinstance(cfg, Dict):[m
[31m-            cfg = EasyTransformerConfig.from_dict(cfg)[m
[31m-        self.cfg = cfg[m
[31m-        self.eps = self.cfg.eps[m
[31m-        if length is None:[m
[31m-            self.length = self.cfg.d_model[m
[31m-        else:[m
[31m-            self.length = length[m
[31m-[m
[31m-        self.w = nn.Parameter(torch.ones(self.length))[m
[31m-        self.b = nn.Parameter(torch.zeros(self.length))[m
[31m-[m
[31m-        # Adds a hook point for the normalisation scale factor[m
[31m-        self.hook_scale = HookPoint()  # [batch, pos, 1][m
[31m-        self.hook_normalized = HookPoint()  # [batch, pos, length][m
[31m-[m
[31m-    def forward(self, x):[m
[31m-        x = x - x.mean(axis=-1, keepdim=True)  # [batch, pos, length][m
[31m-        scale = self.hook_scale([m
[31m-            ([m
[31m-                einops.reduce(x.pow(2), "batch pos embed -> batch pos 1", "mean")[m
[31m-                + self.eps[m
[31m-            ).sqrt()[m
[31m-        )  # [batch, pos, 1][m
[31m-        x = self.hook_normalized(x / scale)  # [batch, pos, length][m
[31m-        return x * self.w + self.b[m
[31m-[m
[31m-[m
[31m-# Attention[m
[31m-class Attention(nn.Module):[m
[31m-    def __init__(self, cfg: Union[Dict, EasyTransformerConfig], attn_type="global"):[m
[31m-        super().__init__()[m
[31m-        if isinstance(cfg, Dict):[m
[31m-            cfg = EasyTransformerConfig.from_dict(cfg)[m
[31m-        self.cfg = cfg[m
[31m-        self.W_Q = nn.Parameter([m
[31m-            torch.empty(self.cfg.n_heads, self.cfg.d_head, self.cfg.d_model)[m
[31m-        )[m
[31m-        self.W_K = nn.Parameter([m
[31m-            torch.empty(self.cfg.n_heads, self.cfg.d_head, self.cfg.d_model)[m
[31m-        )[m
[31m-        self.W_V = nn.Parameter([m
[31m-            torch.empty(self.cfg.n_heads, self.cfg.d_head, self.cfg.d_model)[m
[31m-        )[m
[31m-        self.W_O = nn.Parameter([m
[31m-            torch.empty(self.cfg.n_heads, self.cfg.d_model, self.cfg.d_head)[m
[31m-        )[m
[31m-        self.b_Q = nn.Parameter(torch.empty(self.cfg.n_heads, self.cfg.d_head))[m
[31m-        self.b_K = nn.Parameter(torch.empty(self.cfg.n_heads, self.cfg.d_head))[m
[31m-        self.b_V = nn.Parameter(torch.empty(self.cfg.n_heads, self.cfg.d_head))[m
[31m-        self.b_O = nn.Parameter(torch.empty(self.cfg.d_model))[m
[31m-[m
[31m-        self.attn_type = attn_type[m
[31m-        # Create a query_pos x key_pos mask, with True iff that query position[m
[31m-        # can attend to that key position[m
[31m-        causal_mask = torch.tril(torch.ones((self.cfg.n_ctx, self.cfg.n_ctx)).bool())[m
[31m-        if self.attn_type == "global":[m
[31m-            # For global attention, this is a lower triangular matrix - key <= query[m
[31m-            self.register_buffer("mask", causal_mask)[m
[31m-        elif self.attn_type == "local":[m
[31m-            # For local, this is banded, query - window_size < key <= query[m
[31m-            assert isinstance(self.cfg.window_size, int)[m
[31m-            self.register_buffer([m
[31m-                "mask", torch.triu(causal_mask, 1 - self.cfg.window_size)[m
[31m-            )[m
[31m-        else:[m
[31m-            raise ValueError(f"Invalid attention type: {self.attn_type}")[m
[31m-[m
[31m-        self.register_buffer("IGNORE", torch.tensor(-1e5))[m
[31m-[m
[31m-        if self.cfg.use_attn_scale:[m
[31m-            self.attn_scale = np.sqrt(self.cfg.d_head)[m
[31m-        else:[m
[31m-            self.attn_scale = 1.0[m
[31m-[m
[31m-        self.hook_k = HookPoint()  # [batch, pos, head_index, d_head][m
[31m-        self.hook_q = HookPoint()  # [batch, pos, head_index, d_head][m
[31m-        self.hook_v = HookPoint()  # [batch, pos, head_index, d_head][m
[31m-        self.hook_z = HookPoint()  # [batch, pos, head_index, d_head][m
[31m-        self.hook_attn_scores = HookPoint()  # [batch, head_index, query_pos, key_pos][m
[31m-        self.hook_attn = HookPoint()  # [batch, head_index, query_pos, key_pos][m
[31m-        self.hook_result = HookPoint()  # [batch, head_index, head_index, d_model][m
[31m-[m
[31m-    def forward(self, x):[m
[31m-        q = self.hook_q([m
[31m-            torch.einsum("ihm,bpm->bpih", self.W_Q, x) + self.b_Q[m
[31m-        )  # [batch, pos, head_index, d_head][m
[31m-        k = self.hook_k([m
[31m-            torch.einsum("ihm,bpm->bpih", self.W_K, x) + self.b_K[m
[31m-        )  # [batch, pos, head_index, d_head][m
[31m-        v = self.hook_v([m
[31m-            torch.einsum("ihm,bpm->bpih", self.W_V, x) + self.b_V[m
[31m-        )  # [batch, pos, head_index, d_head][m
[31m-        attn_scores = ([m
[31m-            torch.einsum("bpih,bqih->bipq", q, k) / self.attn_scale[m
[31m-        )  # [batch, head_index, query_pos, key_pos][m
[31m-        attn_scores = self.hook_attn_scores([m
[31m-            self.causal_mask(attn_scores)[m
[31m-        )  # [batch, head_index, query_pos, key_pos][m
[31m-        attn_matrix = self.hook_attn([m
[31m-            F.softmax(attn_scores, dim=-1)[m
[31m-        )  # [batch, head_index, query_pos, key_pos][m
[31m-        z = self.hook_z([m
[31m-            torch.einsum("bpih,biqp->bqih", v, attn_matrix)[m
[31m-        )  # [batch, pos, head_index, d_head][m
[31m-        if self.cfg.use_attn_result:[m
[31m-            result = self.hook_result([m
[31m-                torch.einsum("imh,bqih->bqim", self.W_O, z)[m
[31m-            )  # [batch, pos, head_index, d_model][m
[31m-            out = ([m
[31m-                einops.reduce([m
[31m-                    result, "batch position index model->batch position model", "sum"[m
[31m-                )[m
[31m-                + self.b_O[m
[31m-            )  # [batch, pos, d_model][m
[31m-        else:[m
[31m-            out = ([m
[31m-                torch.einsum("idh,bqih->bqd", self.W_O, z) + self.b_O[m
[31m-            )  # [batch, pos, d_model][m
[31m-        return out[m
[31m-[m
[31m-    def causal_mask(self, attn_scores):[m
[31m-        return torch.where([m
[31m-            self.mask[: attn_scores.size(-2), : attn_scores.size(-1)],[m
[31m-            attn_scores,[m
[31m-            self.IGNORE,[m
[31m-        )[m
[31m-[m
[31m-[m
[31m-# MLP Layers[m
[31m-class MLP(nn.Module):[m
[31m-    def __init__(self, cfg: Union[Dict, EasyTransformerConfig]):[m
[31m-        super().__init__()[m
[31m-        if isinstance(cfg, Dict):[m
[31m-            cfg = EasyTransformerConfig.from_dict(cfg)[m
[31m-        self.cfg = cfg[m
[31m-        self.W_in = nn.Parameter(torch.empty(self.cfg.d_mlp, self.cfg.d_model))[m
[31m-        self.b_in = nn.Parameter(torch.empty(self.cfg.d_mlp))[m
[31m-        if self.cfg.gated_act_fn:[m
[31m-            self.W_gate = nn.Parameter(torch.empty(self.cfg.d_mlp, self.cfg.d_model))[m
[31m-            self.b_gate = nn.Parameter(torch.empty(self.cfg.d_mlp))[m
[31m-            self.hook_gate = HookPoint()[m
[31m-        self.W_out = nn.Parameter(torch.empty(self.cfg.d_model, self.cfg.d_mlp))[m
[31m-        self.b_out = nn.Parameter(torch.empty(self.cfg.d_model))[m
[31m-[m
[31m-        self.hook_pre = HookPoint()  # [batch, pos, d_mlp][m
[31m-        self.hook_post = HookPoint()  # [batch, pos, d_mlp][m
[31m-[m
[31m-        if self.cfg.act_fn == "relu":[m
[31m-            self.act_fn = F.relu[m
[31m-        elif self.cfg.act_fn == "gelu":[m
[31m-            self.act_fn = F.gelu[m
[31m-        elif self.cfg.act_fn == "silu":[m
[31m-            self.act_fn = F.silu[m
[31m-        elif self.cfg.act_fn == "glu":[m
[31m-            self.act_fn = F.glu[m
[31m-        elif self.cfg.act_fn == "gelu_new":[m
[31m-            self.act_fn = gelu_new[m
[31m-        elif self.cfg.act_fn == "solu_ln":[m
[31m-            self.act_fn = solu[m
[31m-            self.hook_post_ln = HookPoint()  # [batch, pos, d_mlp][m
[31m-            self.ln = LayerNorm(self.cfg, self.cfg.d_mlp)[m
[31m-        elif self.cfg.act_fn == "reglu":[m
[31m-            self.act_fn = reglu[m
[31m-        elif self.cfg.act_fn == "geglu":[m
[31m-            self.act_fn = geglu[m
[31m-        elif self.cfg.act_fn == "swiglu":[m
[31m-            self.act_fn = swiglu[m
[31m-        else:[m
[31m-            raise ValueError(f"Invalid activation function name: {self.cfg.act_fn}")[m
[31m-[m
[31m-    def forward(self, x):[m
[31m-        pre_act = self.hook_pre([m
[31m-            torch.einsum("md,bpd->bpm", self.W_in, x) + self.b_in[m
[31m-        )  # [batch, pos, d_mlp][m
[31m-        if self.cfg.gated_act_fn:[m
[31m-            gate = self.hook_gate([m
[31m-                torch.einsum("md,bpd->bpm", self.W_gate, x) + self.b_gate[m
[31m-            )[m
[31m-            post_act = self.hook_post(self.act_fn(pre_act, gate))  # [batch, pos, d_mlp][m
[31m-        else:[m
[31m-            post_act = self.hook_post(self.act_fn(pre_act))  # [batch, pos, d_mlp][m
[31m-        if self.cfg.act_fn == "solu_ln":[m
[31m-            post_act = self.hook_post_ln(self.ln(post_act))[m
[31m-        mlp_out = ([m
[31m-            torch.einsum("dm,bpm->bpd", self.W_out, post_act) + self.b_out[m
[31m-        )  # [batch, pos, d_model][m
[31m-        return mlp_out[m
[31m-[m
[31m-[m
[31m-# Transformer Block[m
[31m-class TransformerBlock(nn.Module):[m
[31m-    def __init__(self, cfg: Union[Dict, EasyTransformerConfig], block_index):[m
[31m-        super().__init__()[m
[31m-        if isinstance(cfg, Dict):[m
[31m-            cfg = EasyTransformerConfig.from_dict(cfg)[m
[31m-        self.cfg = cfg[m
[31m-        if self.cfg.normalization_type == "LN":[m
[31m-            self.ln1 = LayerNorm(cfg)[m
[31m-            self.ln2 = LayerNorm(cfg)[m
[31m-        elif self.cfg.normalization_type == "LNPre":[m
[31m-            # We've folded in LayerNorm weights, so just need the center + scale parts[m
[31m-            self.ln1 = LayerNormPre(cfg)[m
[31m-            self.ln2 = LayerNormPre(cfg)[m
[31m-        elif self.cfg.normalization_type is None:[m
[31m-            # If it's None, don't create either layer[m
[31m-            pass[m
[31m-        else:[m
[31m-            logging.warning([m
[31m-                f"Invalid normalization_type passed in {self.cfg.normalization_type}"[m
[31m-            )[m
[31m-[m
[31m-        if not self.cfg.use_local_attn:[m
[31m-            self.attn = Attention(cfg, "global")[m
[31m-        else:[m
[31m-            assert self.cfg.attn_types is not None[m
[31m-            attn_type = self.cfg.attn_types[block_index][m
[31m-            self.attn = Attention(cfg, attn_type)[m
[31m-        self.mlp = MLP(cfg)[m
[31m-[m
[31m-        self.hook_attn_out = HookPoint()  # [batch, pos, d_model][m
[31m-        self.hook_mlp_out = HookPoint()  # [batch, pos, d_model][m
[31m-        self.hook_resid_pre = HookPoint()  # [batch, pos, d_model][m
[31m-        self.hook_resid_mid = HookPoint()  # [batch, pos, d_model][m
[31m-        self.hook_resid_post = HookPoint()  # [batch, pos, d_model][m
[31m-[m
[31m-    def forward(self, x):[m
[31m-        resid_pre = self.hook_resid_pre(x)  # [batch, pos, d_model][m
[31m-        if self.cfg.normalization_type is not None:[m
[31m-            normalized_resid_pre = self.ln1(resid_pre)[m
[31m-        else:[m
[31m-            normalized_resid_pre = resid_pre[m
[31m-        attn_out = self.hook_attn_out([m
[31m-            self.attn(normalized_resid_pre)[m
[31m-        )  # [batch, pos, d_model][m
[31m-        resid_mid = self.hook_resid_mid(resid_pre + attn_out)  # [batch, pos, d_model][m
[31m-[m
[31m-        if self.cfg.normalization_type is not None:[m
[31m-            normalized_resid_mid = self.ln2(resid_mid)[m
[31m-        else:[m
[31m-            normalized_resid_mid = resid_mid[m
[31m-        mlp_out = self.hook_mlp_out([m
[31m-            self.mlp(normalized_resid_mid)[m
[31m-        )  # [batch, pos, d_model][m
[31m-        resid_post = self.hook_resid_post(resid_mid + mlp_out)  # [batch, pos, d_model][m
[31m-        return resid_post[m
[31m-[m
[31m-[m
 # Full transformer[m
 class EasyTransformer(HookedRootModule):[m
     """[m
[31m-    This class implements a full Transformer using the above components, with[m
[32m+[m[32m    This class implements a full Transformer using the components in ./components.py, with[m
     HookPoints on every interesting activation. It inherits from HookedRootModule.[m
 [m
[31m-    It can be initialised with a model name, and then will automatically load the model weights[m
[31m-    for that model, loads them into this model, as well as fold in LayerNorm and center[m
[31m-    the weights.[m
[31m-[m
[31m-    It can also be initilised with an EasyTransformerConfig or a config dictionary, which can be used to instantiate a custom model without loading pretrained weights and will instead use Pytorch's default weight initialisation.[m
[32m+[m[32m    It can have a pretrained Transformer's weights automatically loaded in via the EasyTransformer.from_pretrained class method. It can also be instantiated with randomly initialized weights via __init__ and being passed a dict or EasyTransformerConfig object.[m[41m [m
     """[m
[32m+[m[41m    [m
[32m+[m[32m    VALID_PRETRAINED_MODEL_NAMES = weight_conversion.VALID_PRETRAINED_MODEL_NAMES[m
[32m+[m[32m    PRETRAINED_MODEL_NAMES_DICT = weight_conversion.PRETRAINED_MODEL_NAMES_DICT[m
[32m+[m[32m    STANFORD_CRFM_CHECKPOINTS = weight_conversion.STANFORD_CRFM_CHECKPOINTS[m
 [m
     def __init__([m
         self,[m
[31m-        model_name,[m
[31m-        cfg=None,[m
[31m-        tokenizer=None,[m
[31m-        use_attn_result=False,[m
[31m-        model=None,[m
[31m-        keep_original_model=False,[m
[31m-        center_weights=True,[m
[31m-        checkpoint=None,[m
[32m+[m[32m        cfg,[m
[32m+[m[32m        tokenizer = None,[m
[32m+[m[32m        move_to_device = True,[m
     ):[m
         """[m
[31m-        model_name (str: The name of the model to load, via HuggingFace. If[m
[31m-            "custom", then cfg must be provided.[m
[31m-        cfg (EasyTransformerConfig, *optional*): The config to use for the[m
[31m-            model. If not provided, a model name must be passed via model_name.[m
[32m+[m[32m        Model initialization. Note that if you want to load the model from pretrained weights, you should use the EasyTransformer.from_pretrained() class method instead of this one.[m
[32m+[m
[32m+[m[32m        cfg Union[EasyTransformerConfig, Dict]: The config to use for the[m
[32m+[m[32m            model.[m[41m [m
         tokenizer (*optional): The tokenizer to use for the model. If not[m
[31m-            provided, initialized to None, though the user must initialize one[m
[31m-            before passing strings to the model.[m
[31m-        use_attn_result (bool): Says whether to explicitly calculate the amount[m
[31m-            each head adds to the residual stream (with a hook) and THEN add it[m
[31m-            up, vs just calculating the sum. This can be very memory intensive[m
[31m-            for large models, so defaults to False[m
[31m-        model: The model loaded from HuggingFace or separately initialized. If[m
[31m-            None, it is automatically loaded from HuggingFace if model_name is[m
[31m-            passed - this just saves memory if the model was already loaded into[m
[31m-            RAM.[m
[31m-        keep_original_model (bool): If False, the original model is deleted,[m
[31m-            otherwise it's kept as a self.model attribute[m
[31m-        center_weights (bool): If True, the weights are centered[m
[31m-        checkpoint (int, *optional): The checkpoint number of the model to load[m
[31m-            if it is a model with multiple possible checkpoints to load from.[m
[32m+[m[32m            provided, it is inferred from cfg.tokenizer_name or initialized to None.[m[41m [m
[32m+[m[32m            If None, then the model cannot be passed strings, and d_vocab must be explicitly set.[m
[32m+[m[32m        move_to_device (bool): Whether to move the model to the device specified in cfg.[m
[32m+[m[32m            device.[m
         """[m
         super().__init__()[m
[31m-        if model_name == "custom":[m
[31m-            assert cfg is not None, "Must provide a config for custom model"[m
[31m-            self.cfg = cfg[m
[31m-            self.model_name = cfg.model_name[m
[31m-            self.model_type = cfg.model_type[m
[31m-            if self.cfg.tokenizer_name is not None:[m
[31m-                self.tokenizer = AutoTokenizer.from_pretrained(self.cfg.tokenizer_name)[m
[31m-            else:[m
[31m-                # If no tokenizer name is provided, we assume we're training on an algorithmic task and will pass in tokens directly. In this case, we don't need a tokenizer.[m
[31m-                self.tokenizer = None[m
[31m-            self.use_attn_result = use_attn_result[m
[31m-            self.model = None[m
[31m-            self.keep_original_model = False[m
[31m-            # We're initializing a model, no need to load weights from a checkpoint[m
[31m-            self.checkpoint = None[m
[31m-        else:[m
[31m-            assert ([m
[31m-                model_name in VALID_MODEL_NAMES[m
[31m-            ), f"Invalid model name: {model_name}. Valid model names are: {VALID_MODEL_NAMES}"[m
[31m-            self.model_name = model_name[m
[31m-            if self.model_name in MODEL_NAMES_DICT:[m
[31m-                self.full_model_name = MODEL_NAMES_DICT[self.model_name][m
[31m-            else:[m
[31m-                self.full_model_name = self.model_name[m
[31m-            self.model_type = self.get_model_type(self.full_model_name)[m
[31m-            if model is not None:[m
[31m-                self.model = model[m
[31m-            else:[m
[31m-                if checkpoint is not None:[m
[31m-                    if "stanford" not in self.model_name:[m
[31m-                        logging.warning([m
[31m-                            f"Loading checkpoints is not supported for the model {self.model_name}. Loading without checkpoints"[m
[31m-                        )[m
[31m-                        self.model = AutoModelForCausalLM.from_pretrained([m
[31m-                            self.full_model_name[m
[31m-                        )[m
[31m-                    else:[m
[31m-                        assert ([m
[31m-                            checkpoint in STANFORD_CRFM_CHECKPOINTS[m
[31m-                        ), f"Checkpoint {checkpoint} is not valid. Available checkpoints are {STANFORD_CRFM_CHECKPOINTS}"[m
[31m-                        self.model = AutoModelForCausalLM.from_pretrained([m
[31m-                            self.full_model_name, revision=f"checkpoint-{checkpoint}"[m
[31m-                        )[m
[31m-                else:[m
[31m-                    self.model = AutoModelForCausalLM.from_pretrained([m
[31m-                        self.full_model_name[m
[31m-                    )[m
[31m-[m
[31m-            self.cfg = self.convert_hf_config([m
[31m-                self.model.config, model_type=self.model_type[m
[31m-            )[m
[31m-            self.cfg.use_attn_result = use_attn_result[m
[31m-            self.cfg.checkpoint = checkpoint[m
[31m-            self.cfg.model_type = self.model_type[m
[31m-            self.cfg.model_name = self.model_name[m
[31m-            self.cfg.tokenizer_name = self.full_model_name[m
[31m-            self.cfg.normalization_type = "LNPre"[m
[32m+[m[32m        if isinstance(cfg, Dict):[m
[32m+[m[32m            cfg = EasyTransformerConfig(**cfg)[m
[32m+[m[32m        elif isinstance(cfg, str):[m
[32m+[m[32m            raise ValueError("Please pass in a config dictionary or EasyTransformerConfig object. If you want to load a pretrained model, use EasyTransformer.from_pretrained() instead.")[m
[32m+[m[32m        self.cfg = cfg[m
[32m+[m[32m        if tokenizer is not None:[m
[32m+[m[32m            self.tokenizer = tokenizer[m
[32m+[m[32m        if self.cfg.tokenizer_name is not None:[m
[32m+[m[32m            # If we have a tokenizer name, we can load it from HuggingFace[m
             self.tokenizer = AutoTokenizer.from_pretrained(self.cfg.tokenizer_name)[m
             self.tokenizer.pad_token = self.tokenizer.eos_token[m
[32m+[m[32m        else:[m
[32m+[m[32m            # If no tokenizer name is provided, we assume we're training on an algorithmic task and will pass in tokens directly. In this case, we don't need a tokenizer.[m
[32m+[m[32m            self.tokenizer = None[m
[32m+[m[41m        [m
[32m+[m[32m        if not self.cfg.d_vocab:[m
[32m+[m[32m            # If we have a tokenizer, vocab size can be inferred from it.[m
[32m+[m[32m            assert ([m
[32m+[m[32m                self.tokenizer is not None[m
[32m+[m[32m            ), "Must provide a tokenizer if d_vocab is not provided"[m
[32m+[m[32m            self.cfg.d_vocab = max(self.tokenizer.vocab.values()) + 1[m
[32m+[m[32m            self.cfg.d_vocab_out = self.cfg.d_vocab[m
 [m
         self.embed = Embed(self.cfg)[m
         self.hook_embed = HookPoint()  # [batch, pos, d_model][m
 [m
[31m-        self.pos_embed = PosEmbed(self.cfg)[m
[31m-        self.hook_pos_embed = HookPoint()  # [batch, pos, d__dictmodel][m
[31m-[m
[32m+[m[32m        if self.cfg.positional_embedding_type != "rotary":[m
[32m+[m[32m            self.pos_embed = PosEmbed(self.cfg)[m
[32m+[m[32m            self.hook_pos_embed = HookPoint()  # [batch, pos, d__dictmodel][m
[32m+[m[41m        [m
         self.blocks = nn.ModuleList([m
             [[m
                 TransformerBlock(self.cfg, block_index)[m
                 for block_index in range(self.cfg.n_layers)[m
             ][m
         )[m
[32m+[m[41m        [m
         if self.cfg.normalization_type == "LN":[m
[31m-            self.ln_final = LayerNorm(self.cfg)[m
[32m+[m[32m            if self.cfg.final_rms:[m
[32m+[m[32m                self.ln_final = RMSNorm(self.cfg)[m
[32m+[m[32m            else:[m
[32m+[m[32m                self.ln_final = LayerNorm(self.cfg)[m
         elif self.cfg.normalization_type == "LNPre":[m
             # We've folded in LayerNorm weights, so just need the center + scale parts[m
[31m-            self.ln_final = LayerNormPre(self.cfg)[m
[32m+[m[32m            if self.cfg.final_rms:[m
[32m+[m[32m                self.ln_final = RMSNormPre(self.cfg)[m
[32m+[m[32m            else:[m
[32m+[m[32m                self.ln_final = LayerNormPre(self.cfg)[m
         elif self.cfg.normalization_type is None:[m
             # If it's None, don't create either layer[m
             pass[m
[36m@@ -588,60 +133,88 @@[m [mclass EasyTransformer(HookedRootModule):[m
             )[m
         self.unembed = Unembed(self.cfg)[m
 [m
[32m+[m[32m        if self.cfg.init_weights:[m
[32m+[m[32m            self.init_weights()[m
[32m+[m
[32m+[m[32m        if move_to_device:[m
[32m+[m[32m            self.to(self.cfg.device)[m
[32m+[m[41m        [m
         # Gives each module a parameter with its name (relative to this root module)[m
         # Needed for HookPoints to work[m
         self.setup()[m
 [m
[31m-        # Load model weights, and fold in layer norm weights[m
[31m-        if self.model_type == "gpt2":[m
[31m-            self.load_gpt2_weights(self.model)[m
[31m-        elif self.model_type == "neo":[m
[31m-            self.load_neo_weights(self.model)[m
[31m-        elif self.model_type == "gptj":[m
[31m-            self.load_gptj_weights(self.model)[m
[31m-        elif self.model_type == "neox":[m
[31m-            self.load_neox_weights(self.model)[m
[31m-        elif self.model_type == "opt":[m
[31m-            self.load_opt_weights(self.model)[m
[31m-        elif self.model_type == "custom":[m
[31m-            self.init_weights()[m
[31m-[m
[31m-        # Set the average of each weight matrix writing to the residual stream to zero[m
[31m-        # (Layer Norm removes the mean anyway, so this simplifies the weights[m
[31m-        # without changing the computation)[m
[31m-        if center_weights:[m
[31m-            self.center_weights()[m
[31m-[m
[31m-        if not keep_original_model and self.model is not None:[m
[31m-            # Delete the original model to save memory[m
[31m-            del self.model[m
[31m-[m
[31m-    def forward(self, input, return_type: Optional[str] = "logits"):[m
[31m-        """Input is either a batch of tokens ([batch, pos]) or a text string.[m
[32m+[m[32m    def forward([m
[32m+[m[32m        self,[m[41m [m
[32m+[m[32m        input: Union[str, torch.Tensor],[m[41m [m
[32m+[m[32m        return_type: Optional[str] = "logits",[m[41m [m
[32m+[m[32m        prepend_bos: bool = True,[m[41m [m
[32m+[m[32m        past_kv_cache: Optional[EasyTransformerKeyValueCache] = None[m
[32m+[m[32m        ) -> Union[None, torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:[m
[32m+[m[32m        """Input is either a batch of tokens ([batch, pos]) or a text string, a string is automatically tokenized to a batch of a single element. The prepend_bos flag only applies when inputting a text string.[m
 [m
         return_type Optional[str]: The type of output to return. Can be one of: None (return nothing, don't calculate logits), 'logits' (return logits), 'loss' (return cross-entropy loss), 'both' (return logits and loss)[m
[32m+[m
[32m+[m[32m        Note that loss is the standard "predict the next token" cross-entropy loss for GPT-2 style language models - if you want a custom loss function, the recommended behaviour is returning the logits and then applying your custom loss function.[m
         """[m
         if type(input) == str or type(input) == list:[m
             # If text, convert to tokens (batch_size=1)[m
             assert ([m
                 self.tokenizer is not None[m
             ), "Must provide a tokenizer if passing a string to the model"[m
[31m-            tokens = self.to_tokens(input)[m
[31m-        elif type(input) == torch.Tensor or type(input) == np.ndarray:[m
[31m-            assert len(input.shape) == 2, (input.shape, "Input must be a 2D tensor")[m
[32m+[m[32m            tokens = self.to_tokens(input, prepend_bos=prepend_bos)[m
[32m+[m[32m        else:[m
             tokens = input[m
[32m+[m[32m        if tokens.device.type != self.cfg.device:[m
[32m+[m[32m            tokens = tokens.to(self.cfg.device)[m
[32m+[m[32m        assert isinstance(tokens, torch.Tensor)[m
[32m+[m[32m        # If we're doing caching, then we reuse keys and values from previous runs, as that's the only[m[41m [m
[32m+[m[32m        # way that past activations will affect the final logits. The cache contains those so we don't[m[41m [m
[32m+[m[32m        # need to recompute them. This is useful for generating text. As we have absolute positional[m[41m [m
[32m+[m[32m        # encodings, to implement this we have a `pos_offset` variable, defaulting to zero, which says[m[41m [m
[32m+[m[32m        # to offset which positional encodings are used (cached keys and values were calculated with[m[41m [m
[32m+[m[32m        # their own positional encodings).[m
[32m+[m[32m        if past_kv_cache is None:[m
[32m+[m[32m            pos_offset = 0[m
[32m+[m[32m        else:[m
[32m+[m[32m            batch_size, ctx_length = tokens.shape[m
[32m+[m[32m            cached_batch_size, cache_ctx_length, num_heads_in_cache, d_head_in_cache = past_kv_cache[0].past_keys.shape[m
[32m+[m[32m            assert cached_batch_size == batch_size[m
[32m+[m[32m            assert num_heads_in_cache == self.cfg.n_heads[m
[32m+[m[32m            assert d_head_in_cache == self.cfg.d_head[m
[32m+[m[32m            # If we want to generate from the empty string, we'd pass in an empty cache, so we need to handle that case[m
[32m+[m[32m            assert cache_ctx_length == 0 or ctx_length == 1, "Pass in one token at a time after loading cache"[m
[32m+[m[32m            pos_offset = cache_ctx_length[m
[32m+[m[32m        embed = self.hook_embed(self.embed(tokens))  # [batch, pos, d_model][m
[32m+[m[32m        if self.cfg.positional_embedding_type == "standard":[m
[32m+[m[32m            pos_embed = self.hook_pos_embed([m
[32m+[m[32m                self.pos_embed(tokens, pos_offset)[m
[32m+[m[32m            )  # [batch, pos, d_model][m
[32m+[m[32m            residual = embed + pos_embed  # [batch, pos, d_model][m
[32m+[m[32m            shortformer_pos_embed = None[m
[32m+[m[32m        elif self.cfg.positional_embedding_type == "shortformer":[m
[32m+[m[32m            # If we're using shortformer style attention, we don't add the positional embedding to the residual stream. See EasyTransformerConfig for details[m
[32m+[m[32m            pos_embed = self.hook_pos_embed([m
[32m+[m[32m                self.pos_embed(tokens, pos_offset)[m
[32m+[m[32m            )  # [batch, pos, d_model][m
[32m+[m[32m            residual = embed[m
[32m+[m[32m            shortformer_pos_embed = pos_embed[m
[32m+[m[32m        elif self.cfg.positional_embedding_type == "rotary":[m
[32m+[m[32m            # Rotary doesn't use positional embeddings, instead they're applied when dot producting keys and queries. See EasyTransformerConfig for details[m
[32m+[m[32m            residual = embed[m
[32m+[m[32m            shortformer_pos_embed = None[m
         else:[m
             raise ValueError([m
[31m-                f"Invalid input type: {type(input)}. Must be a string, list, or tensor"[m
[32m+[m[32m                f"Invalid positional_embedding_type passed in {self.cfg.positional_embedding_type}"[m
             )[m
 [m
[31m-        embed = self.hook_embed(self.embed(tokens))  # [batch, pos, d_model][m
[31m-        pos_embed = self.hook_pos_embed(self.pos_embed(tokens))  # [batch, pos, d_model][m
[31m-        residual = embed + pos_embed  # [batch, pos, d_model][m
[31m-        for block in self.blocks:[m
[32m+[m[32m        for i, block in enumerate(self.blocks):[m
             # Note that each block includes skip connections, so we don't need[m
             # residual + block(residual)[m
[31m-            residual = block(residual)  # [batch, pos, d_model][m
[32m+[m[32m            residual = block([m
[32m+[m[32m                residual,[m[41m [m
[32m+[m[32m                past_kv_cache_entry = past_kv_cache[i] if past_kv_cache is not None else None, # Cache is contains a list of EasyTransformerKeyValueCache objects, one for each block[m
[32m+[m[32m                shortformer_pos_embed = shortformer_pos_embed[m
[32m+[m[32m            )  # [batch, pos, d_model][m
         if return_type is None:[m
             return None[m
         else:[m
[36m@@ -651,15 +224,17 @@[m [mclass EasyTransformer(HookedRootModule):[m
             if return_type == "logits":[m
                 return logits[m
             else:[m
[31m-                loss = self.cross_entropy_loss(logits, tokens)[m
[32m+[m[32m                loss = lm_cross_entropy_loss(logits, tokens)[m
                 if return_type == "loss":[m
                     return loss[m
                 elif return_type == "both":[m
                     return {"logits": logits, "loss": loss}[m
[32m+[m[32m                elif return_type == "logits_and_tokens":[m
[32m+[m[32m                    return logits, tokens[m
                 else:[m
                     logging.warning(f"Invalid return_type passed in: {return_type}")[m
                     return None[m
[31m-[m
[32m+[m[41m                [m
     def set_tokenizer(self, tokenizer):[m
         """[m
         Sets the tokenizer to use for this model.[m
[36m@@ -669,58 +244,365 @@[m [mclass EasyTransformer(HookedRootModule):[m
         self.tokenizer = tokenizer[m
         self.tokenizer.pad_token = self.tokenizer.eos_token[m
 [m
[31m-    def set_tokenizer(self, tokenizer):[m
[32m+[m[32m    def to_tokens(self, input, prepend_bos=True):[m
[32m+[m[32m        assert self.tokenizer is not None, "Cannot use to_tokens without a tokenizer"[m
[32m+[m[32m        if prepend_bos:[m
[32m+[m[32m            if isinstance(input, str):[m
[32m+[m[32m                input = self.tokenizer.bos_token + input[m
[32m+[m[32m            else:[m
[32m+[m[32m                input = [self.tokenizer.bos_token + string for string in input][m
[32m+[m[32m        return self.tokenizer(input, return_tensors="pt", padding=True)["input_ids"][m
[32m+[m[41m    [m
[32m+[m[32m    def to_str_tokens([m
[32m+[m[32m        self,[m[41m [m
[32m+[m[32m        input: Union[str, torch.Tensor, list],[m[41m [m
[32m+[m[32m        prepend_bos: bool = True[m
[32m+[m[32m        ):[m
[32m+[m[32m        """Method to map text, a list of text or tokens to a list of tokens as strings[m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            input (Union[str, list, torch.Tensor]): The input - either a string or a tensor of tokens. If tokens, should be a tensor of shape [pos] or [1, pos][m
[32m+[m[32m            prepend_bos (bool, optional): Whether to prepend a BOS token. Only applies if input is a string. Defaults to True.[m
[32m+[m
[32m+[m[32m        Returns:[m
[32m+[m[32m            str_tokens: List of individual tokens as strings[m
         """[m
[31m-        Sets the tokenizer to use for this model.[m
[31m-        tokenizer (PreTrainedTokenizer): a pretrained HuggingFace tokenizer[m
[32m+[m[32m        if isinstance(input, list):[m
[32m+[m[32m            return list(map(lambda tokens: self.to_str_tokens(tokens, prepend_bos), input))[m
[32m+[m[32m        elif isinstance(input, str):[m
[32m+[m[32m            tokens = self.to_tokens(input, prepend_bos=prepend_bos)[0][m
[32m+[m[32m        elif isinstance(input, torch.Tensor):[m
[32m+[m[32m            tokens = input[m
[32m+[m[32m            tokens = tokens.squeeze() # Get rid of a trivial batch dimension[m
[32m+[m[32m            assert tokens.dim() == 1, f"Invalid tokens input to to_str_tokens, has shape: {tokens.shape}"[m
[32m+[m[32m        else:[m
[32m+[m[32m            raise ValueError(f"Invalid input type to to_str_tokens: {type(input)}")[m
[32m+[m[32m        str_tokens = self.tokenizer.batch_decode(tokens, clean_up_tokenization_spaces=False)[m
[32m+[m[32m        return str_tokens[m
[32m+[m[41m    [m
[32m+[m[32m    def to_single_token(self, string):[m
[32m+[m[32m        """Maps a string that makes up a single token to the id for that token. Raises an error for strings that are not a single token! If uncertain use to_tokens[m
         """[m
[31m-        assert isinstance(tokenizer, PreTrainedTokenizer)[m
[31m-        self.tokenizer = tokenizer[m
[31m-        self.tokenizer.pad_token = self.tokenizer.eos_token[m
 [m
[31m-    def to_tokens(self, text):[m
[31m-        assert self.tokenizer is not None[m
[31m-        return self.tokenizer(text, return_tensors="pt", padding=True)["input_ids"][m
[32m+[m[32m        # We use the to_tokens method, do not append a BOS token[m
[32m+[m[32m        token = self.to_tokens(string, prepend_bos=False).squeeze()[m
[32m+[m[32m        # If token shape is non-empty, raise error[m
[32m+[m[32m        assert not token.shape, f"Input string: {string} is not a single token!"[m
[32m+[m[32m        return token.item()[m
[32m+[m[41m    [m
[32m+[m[32m    def single_token_to_residual([m
[32m+[m[32m        self,[m[41m [m
[32m+[m[32m        token: Union[str, int, torch.Tensor][m
[32m+[m[32m        ):[m
[32m+[m[32m        """Maps a token to the unembedding vector for that token, ie the vector in the residual stream that we do with to the get the logit for that token.[m[41m [m
 [m
[31m-    @classmethod[m
[31m-    def from_pretrained(cls, model_name, **kwargs):[m
[31m-        return cls(model_name, **kwargs)[m
[32m+[m[32m        WARNING: If you use this without folding in LayerNorm, the results will be misleading and may be incorrect, as the LN weights change the unembed map.[m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            token (Union[str, int, torch.Tensor]): The single token. Can be a single element tensor, an integer, or string. If string, will be mapped to a single token using to_single_token, and an error raised if it's multiply tokens.[m
[32m+[m[41m        [m
[32m+[m[32m        Returns:[m
[32m+[m[32m            residual_direction torch.Tensor: The unembedding vector for the token, a [d_model] tensor.[m
[32m+[m[32m        """[m
[32m+[m[32m        if isinstance(token, str):[m
[32m+[m[32m            token = self.to_single_token(token).item()[m
[32m+[m[32m        elif isinstance(token, int):[m
[32m+[m[32m            pass[m
[32m+[m[32m        elif isinstance(token, torch.Tensor):[m
[32m+[m[32m            token = token.item()[m
[32m+[m[32m        else:[m
[32m+[m[32m            raise ValueError(f"Invalid token type: {type(token)}")[m
[32m+[m[41m        [m
[32m+[m[32m        residual_direction = self.W_U[:, token][m
[32m+[m[32m        return residual_direction[m
[32m+[m[41m    [m
[32m+[m[32m    def to(self, device):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        Wrapper around to that also changes self.cfg.device if it's a torch.device or string. If torch.dtype, just passes through[m
[32m+[m[32m        """[m
[32m+[m[32m        if isinstance(device, torch.device):[m
[32m+[m[32m            self.cfg.device = device.type[m
[32m+[m[32m        elif isinstance(device, str):[m
[32m+[m[32m            self.cfg.device = device[m
[32m+[m[32m            print("Moving model to device: ", self.cfg.device)[m
[32m+[m[32m        elif isinstance(device, torch.dtype):[m
[32m+[m[32m            self.cfg.dtype = device[m
[32m+[m[32m            print("Changing model dtype to", self.cfg.dtype)[m
[32m+[m[32m        nn.Module.to(self, device)[m
 [m
     @classmethod[m
[31m-    def from_config(cls, cfg):[m
[31m-        """Used to generate a model from a config object to train from[m
[32m+[m[32m    def from_pretrained(cls,[m[41m [m
[32m+[m[32m                        model_name: str,[m[41m [m
[32m+[m[32m                        fold_ln = True,[m[41m [m
[32m+[m[32m                        center_writing_weights = True,[m[41m [m
[32m+[m[32m                        center_unembed = True,[m
[32m+[m[32m                        checkpoint = None,[m
[32m+[m[32m                        hf_model = None,[m
[32m+[m[32m                        device = None,[m
[32m+[m[32m                        **model_kwargs):[m
[32m+[m[32m        """Class method to load a pretrained model from HuggingFace and to automatically convert and load those weights into EasyTransformer format.[m
[32m+[m[41m        [m
[32m+[m[32m        See fold_layer_norm for more details on the folding and centering.[m
 [m
         Args:[m
[31m-            cfg (EasyTransformerConfig): Config for the model[m
[31m-[m
[31m-        Returns:[m
[31m-            EasyTransformer: An initialised EasyTransformer model[m
[32m+[m[32m            model_name (str): The model name - must be in VALID_MODEL_NAMES[m
[32m+[m[32m            fold_ln (bool, optional): Whether to fold in the LayerNorm weights to the[m[41m [m
[32m+[m[32m                subsequent linear layer. This does not change the computation. Defaults to True.[m
[32m+[m[32m            center_writing_weights (bool, optional): Whether to center weights writing to[m[41m   [m
[32m+[m[32m                the residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the computation. Defaults to True.[m
[32m+[m[32m            center_unembed (bool, optional): Whether to center W_U (ie set mean to be zero).[m[41m [m
[32m+[m[32m                Softmax is translation invariant so this doesn't affect log probs or loss, but does change logits. Defaults to True.[m
[32m+[m[32m            keep_original_model (bool, optional): Whether to delete the model loaded from    HuggingFace (stored as model.hf_model). Defaults to False.[m
[32m+[m[32m            device (str, optional): The device to load the model onto. By default will load to CUDA if available, else CPU[m
         """[m
[31m-        if isinstance(cfg, Dict):[m
[31m-            cfg = EasyTransformerConfig(**cfg)[m
[31m-        return cls([m
[31m-            "custom",[m
[31m-            cfg,[m
[31m-            use_attn_result=cfg.use_attn_result,[m
[32m+[m[32m        assert ([m
[32m+[m[32m            (model_name in cls.VALID_PRETRAINED_MODEL_NAMES) or (model_name in cls.PRETRAINED_MODEL_NAMES_DICT)[m
[32m+[m[32m        ), f"Invalid model name: {model_name}. Valid model names are: {cls.VALID_PRETRAINED_MODEL_NAMES}"[m
[32m+[m
[32m+[m[32m        if model_name.endswith("-old"):[m
[32m+[m[32m            return cls.from_pretrained_solu_old(model_name, fold_ln, center_writing_weights, center_unembed, **model_kwargs)[m
[32m+[m[32m        elif model_name.endswith("-c4-code") and model_name.startswith("solu"):[m
[32m+[m[32m            return cls.from_pretrained_solu_c4_code(model_name, fold_ln, center_writing_weights, center_unembed, **model_kwargs)[m
[32m+[m
[32m+[m[32m        elif model_name == "attn-only-2l-induction-demo":[m
[32m+[m[32m            return cls.from_pretrained_attn_only_old(center_unembed, **model_kwargs)[m
[32m+[m
[32m+[m[32m        # hf_model_name is the model's name on HuggingFace[m
[32m+[m[32m        if model_name in cls.PRETRAINED_MODEL_NAMES_DICT:[m
[32m+[m[32m            hf_model_name = cls.PRETRAINED_MODEL_NAMES_DICT[model_name][m
[32m+[m[32m        else:[m
[32m+[m[32m            hf_model_name = model_name[m
[32m+[m[32m        # The model family (eg "gpt2" or "neo")[m
[32m+[m[32m        model_family = cls.get_model_family(hf_model_name)[m
[32m+[m[41m        [m
[32m+[m[32m        if hf_model is None:[m
[32m+[m[32m            if checkpoint is not None:[m
[32m+[m[32m                if "stanford" not in model_name:[m
[32m+[m[32m                    logging.warning([m
[32m+[m[32m                        f"Loading checkpoints is not supported for the model {model_name}. Loading without checkpoints"[m
[32m+[m[32m                    )[m
[32m+[m[32m                    hf_model = AutoModelForCausalLM.from_pretrained([m
[32m+[m[32m                        hf_model_name[m
[32m+[m[32m                    )[m
[32m+[m[32m                else:[m
[32m+[m[32m                    assert ([m
[32m+[m[32m                        checkpoint in cls.STANFORD_CRFM_CHECKPOINTS[m
[32m+[m[32m                    ), f"Checkpoint {checkpoint} is not valid. Available checkpoints are {cls.STANFORD_CRFM_CHECKPOINTS}"[m
[32m+[m[32m                    hf_model = AutoModelForCausalLM.from_pretrained([m
[32m+[m[32m                        hf_model_name, revision=f"checkpoint-{checkpoint}"[m
[32m+[m[32m                    )[m
[32m+[m[32m            else:[m
[32m+[m[32m                hf_model = AutoModelForCausalLM.from_pretrained([m
[32m+[m[32m                    hf_model_name[m
[32m+[m[32m                )[m
[32m+[m
[32m+[m[32m        cfg = cls.convert_hf_config([m
[32m+[m[32m            hf_model.config, model_family=model_family[m
         )[m
[32m+[m[32m        if device is not None:[m
[32m+[m[32m            cfg.device = device[m
[32m+[m[32m        cfg.checkpoint = checkpoint[m
[32m+[m[32m        cfg.model_family = model_family[m
[32m+[m[32m        cfg.model_name = model_name[m
[32m+[m
[32m+[m[32m        cfg.normalization_type = "LNPre" if fold_ln else "LN"[m
[32m+[m[32m        cfg.tokenizer_name = hf_model_name[m
[32m+[m[32m        cfg.init_weights = False[m
[32m+[m[32m        tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer_name)[m
[32m+[m[32m        tokenizer.pad_token = tokenizer.eos_token[m
[32m+[m
[32m+[m[32m        model = cls(cfg, **model_kwargs)[m
[32m+[m
[32m+[m[32m        # Load model weights, and fold in layer norm weights[m
[32m+[m[32m        if model_family == "gpt2":[m
[32m+[m[32m            state_dict = weight_conversion.convert_gpt2_weights(hf_model, model.cfg)[m
[32m+[m[32m        elif model_family == "mistral":[m
[32m+[m[32m            # Stanford (Mistral) models have identical structure to GPT-2, but scale attention scores by 1/(layer_id+1) before softmax.[m
[32m+[m[32m            state_dict = weight_conversion.convert_gpt2_weights(hf_model, model.cfg)[m
[32m+[m[32m        elif model_family == "neo":[m
[32m+[m[32m            state_dict = weight_conversion.convert_neo_weights(hf_model, model.cfg)[m
[32m+[m[32m        elif model_family == "gptj":[m
[32m+[m[32m            state_dict = weight_conversion.convert_gptj_weights(hf_model, model.cfg)[m
[32m+[m[32m        elif model_family == "neox":[m
[32m+[m[32m            state_dict = weight_conversion.convert_neox_weights(hf_model, model.cfg)[m
[32m+[m[32m        elif model_family == "opt":[m
[32m+[m[32m            state_dict = weight_conversion.convert_opt_weights(hf_model, model.cfg)[m
[32m+[m[32m        else:[m
[32m+[m[32m            raise ValueError(f"Loading weights from this model family is not currently supported: {model_family}, generated from model name {model_name}. Feel free to open an issue on GitHub to request this feature.")[m
[32m+[m[41m        [m
[32m+[m[32m        model.load_and_process_state_dict(state_dict,[m[41m [m
[32m+[m[32m                        fold_ln=fold_ln,[m[41m [m
[32m+[m[32m                        center_writing_weights=center_writing_weights,[m[41m [m
[32m+[m[32m                        center_unembed=center_unembed,[m
[32m+[m[32m                        move_dict_to_device=True)[m
[32m+[m[32m        print(f"Finished loading pretrained model {model_name} into EasyTransformer!")[m
[32m+[m[32m        return model[m
[32m+[m[41m    [m
[32m+[m[32m    @classmethod[m
[32m+[m[32m    def from_pretrained_solu_old(cls,[m[41m [m
[32m+[m[32m                                model_name: str,[m[41m [m
[32m+[m[32m                                fold_ln = True,[m[41m [m
[32m+[m[32m                                center_writing_weights = True,[m[41m [m
[32m+[m[32m                                center_unembed = True,[m
[32m+[m[32m                                **model_kwargs):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        A helper function to load in SoLU models trained with my original code[m
[32m+[m
[32m+[m[32m        Model name format: solu-{n_layers}l-old[m
[32m+[m
[32m+[m[32m        These models were all trained on 15B tokens of the Pile[m
[32m+[m[32m        """[m
[32m+[m[32m        layer_number = int(re.match("solu-(\d*)l-old", model_name, re.IGNORECASE).group(1))[m
[32m+[m[32m        api = HfApi()[m
[32m+[m[32m        repo_names = {1:'SoLU_1L_v9_old', 2:'SoLU_2L_v10_old', 4:'SoLU_4L_v11_old', 6:'SoLU_6L_v13_old', 8:'SoLU_8L_v21_old', 10:'SoLU_10L_v22_old'}[m
[32m+[m[32m        repo_name = f"NeelNanda/{repo_names[layer_number]}"[m
[32m+[m[41m        [m
[32m+[m[32m        files = api.list_repo_files(repo_name)[m
[32m+[m[32m        model_files = [f for f in files if "final" in f][m
[32m+[m[32m        file_name = model_files[0][m
[32m+[m[41m        [m
[32m+[m[32m        # Early models have left facing W_pos[m
[32m+[m[32m        reverse_pos = layer_number <= 8[m
[32m+[m
[32m+[m[32m        # Models prior to 8L have left facing everything (8L has JUST left facing W_pos - sorry! Stupid bug)[m
[32m+[m[32m        reverse_weights = layer_number <= 6[m
[32m+[m
[32m+[m[32m        # Download weights from HuggingFace. AutoModel is not supported for these models.[m
[32m+[m[32m        state_dict = download_file_from_hf(repo_name, file_name, force_is_torch=True)[m
[32m+[m[32m        # String munging to get the weights correctly named[m
[32m+[m[32m        new_state_dict = {}[m
[32m+[m[32m        for k, v in state_dict.items():[m
[32m+[m[32m            k = k.replace("norm", "ln")[m
[32m+[m[32m            if k.startswith("ln."):[m
[32m+[m[32m                k = k.replace("ln.", "ln_final.")[m
[32m+[m[32m            new_state_dict[k] = v[m
[32m+[m[32m        # Dumb bug where early models were trained with RMS Norm not LayerNorm pre the final layer[m
[32m+[m[32m        if "ln_final.b" not in new_state_dict:[m
[32m+[m[32m            final_rms = True[m
[32m+[m[32m        else:[m
[32m+[m[32m            final_rms = False[m
[32m+[m[41m        [m
[32m+[m[32m        if reverse_pos:[m
[32m+[m[32m            new_state_dict["pos_embed.W_pos"] = new_state_dict["pos_embed.W_pos"].T[m
[32m+[m[32m        if reverse_weights:[m
[32m+[m[32m            for k, v in new_state_dict.items():[m
[32m+[m[32m                if "W_" in k and "W_pos" not in k:[m
[32m+[m[32m                    new_state_dict[k] = v.transpose(-2, -1)[m
[32m+[m[32m        state_dict = new_state_dict[m
[32m+[m
[32m+[m[32m        config = {[m
[32m+[m[32m            'n_layers': layer_number,[m
[32m+[m[32m            'd_vocab': new_state_dict['embed.W_E'].size(0),[m
[32m+[m[32m            'd_model': new_state_dict['embed.W_E'].size(1),[m
[32m+[m[32m            'd_head': 64,[m
[32m+[m[32m            'n_ctx': 1024,[m
[32m+[m[32m            'act_fn': 'solu_ln',[m
[32m+[m[32m            'final_rms': final_rms,[m
[32m+[m[32m            'tokenizer_name': 'EleutherAI/gpt-neox-20b',[m
[32m+[m[32m            'normalization_type': 'LNPre' if fold_ln else "LN"[m
[32m+[m[32m        }[m
[32m+[m
[32m+[m[32m        model = cls(config, **model_kwargs)[m
[32m+[m[32m        model.load_and_process_state_dict(state_dict, fold_ln, center_writing_weights, center_unembed)[m
[32m+[m[32m        return model[m
[32m+[m[41m    [m
[32m+[m[32m    @classmethod[m
[32m+[m[32m    def from_pretrained_solu_c4_code(cls,[m[41m [m
[32m+[m[32m                                model_name: str,[m[41m [m
[32m+[m[32m                                fold_ln = True,[m[41m [m
[32m+[m[32m                                center_writing_weights = True,[m[41m [m
[32m+[m[32m                                center_unembed = True,[m
[32m+[m[32m                                **model_kwargs):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        A helper function to load in SoLU models trained with my new code, custom tokenizer[m
[32m+[m
[32m+[m[32m        Model name format: solu-{n_layers}l-c4-code[m
[32m+[m
[32m+[m[32m        These models were all trained on 22B tokens of 80% C4 and 20% Code[m
[32m+[m[32m        """[m
[32m+[m[32m        layer_number = int(re.match("solu-(\d*)l-c4-code", model_name, re.IGNORECASE).group(1))[m
[32m+[m[32m        api = HfApi()[m
[32m+[m[32m        repo_name = f"NeelNanda/SoLU_{layer_number}L512W_C4_Code"[m
[32m+[m[41m        [m
[32m+[m[32m        files = api.list_repo_files(repo_name)[m
[32m+[m[32m        model_files = [f for f in files if "final" in f][m
[32m+[m[32m        file_name = model_files[0][m
[32m+[m
[32m+[m[32m        # Download weights from HuggingFace. AutoModel is not supported for these models.[m
[32m+[m[32m        state_dict = download_file_from_hf(repo_name, "model_final.pth", force_is_torch=True)[m
[32m+[m[41m        [m
[32m+[m
[32m+[m[32m        config = {[m
[32m+[m[32m            'n_layers': layer_number,[m
[32m+[m[32m            'd_vocab': state_dict['embed.W_E'].size(0),[m
[32m+[m[32m            'd_model': state_dict['embed.W_E'].size(1),[m
[32m+[m[32m            'd_head': 64,[m
[32m+[m[32m            'n_ctx': 1024,[m
[32m+[m[32m            'act_fn': 'solu_ln',[m
[32m+[m[32m            'tokenizer_name': 'NeelNanda/gpt-neox-tokenizer-digits',[m
[32m+[m[32m            'normalization_type': 'LNPre' if fold_ln else "LN"[m
[32m+[m[32m        }[m
[32m+[m
[32m+[m[32m        model = cls(config, **model_kwargs)[m
[32m+[m[32m        model.load_and_process_state_dict(state_dict, fold_ln, center_writing_weights, center_unembed)[m
[32m+[m[32m        return model[m
[32m+[m[41m    [m
[32m+[m[32m    @classmethod[m
[32m+[m[32m    def from_pretrained_attn_only_old(cls, center_unembed = True, **model_kwargs):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        A helper function to load in a 2L512W attn-only model trained on 6B tokens of the Pile - used in an induction heads tutorial.[m
[32m+[m
[32m+[m[32m        The model has no LN, and was trained with a shortformer variant - positional embeddings are only added in to the residual stream when input to the queries and keys and NOT when input to values (ie is never actually in the residual stream and cannot be moved between positions)[m
[32m+[m
[32m+[m[32m        Model name is attn-only-2l-induction-demo[m
[32m+[m[32m        """[m
[32m+[m
[32m+[m[32m        # Download model weights from HuggingFace[m
[32m+[m[32m        repo_name = f"NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr"[m
[32m+[m[32m        file_name = "model_final.pth"[m
[32m+[m[32m        state_dict = download_file_from_hf(repo_name, file_name, force_is_torch=True)[m
[32m+[m
[32m+[m[32m        # This model was trained with EasyTransformer, so weights already have the right format![m
[32m+[m[32m        config = {[m
[32m+[m[32m            'model_name': 'attn-only-2l-induction-demo',[m
[32m+[m[32m            'n_layers': 2,[m
[32m+[m[32m            'd_vocab': state_dict['embed.W_E'].size(0),[m
[32m+[m[32m            'd_model': 512,[m
[32m+[m[32m            'd_head': 64,[m
[32m+[m[32m            'n_ctx': 1024,[m
[32m+[m[32m            'tokenizer_name': 'EleutherAI/gpt-neox-20b',[m
[32m+[m[32m            'normalization_type': None,[m
[32m+[m[32m            'positional_embedding_type':'shortformer',[m
[32m+[m[32m            'attn_only': True,[m
[32m+[m[32m        }[m
[32m+[m
[32m+[m[32m        model = cls(config, **model_kwargs)[m
[32m+[m[32m        model.load_and_process_state_dict(state_dict, fold_ln=False, center_writing_weights=False, center_unembed=center_unembed)[m
[32m+[m[32m        return model[m
 [m
[31m-    def get_model_type(self, model_name):[m
[31m-        if "gpt2" in model_name or "stanford" in model_name:[m
[32m+[m
[32m+[m[32m    @classmethod[m
[32m+[m[32m    def get_model_family(cls, model_name):[m
[32m+[m[32m        if "stanford" in model_name:[m
[32m+[m[32m            return "mistral"[m
[32m+[m[32m        elif "gpt2" in model_name and "stanford" not in model_name:[m
             return "gpt2"[m
         elif "opt" in model_name:[m
             return "opt"[m
[31m-        elif model_name == "EleutherAI/gpt-neox-20b":[m
[32m+[m[32m        elif model_name == "EleutherAI/gpt-neox-20b" or "pythia" in model_name:[m
             return "neox"[m
         elif model_name == "EleutherAI/gpt-j-6B":[m
             return "gptj"[m
[31m-        elif "neo" in model_name:[m
[32m+[m[32m        elif model_name in ['EleutherAI/gpt-neo-125M', 'EleutherAI/gpt-neo-1.3B', 'EleutherAI/gpt-neo-2.7B',]:[m
[32m+[m[32m            # Important to exclude GPT-J and GPT-NeoX, they have different config.[m
             return "neo"[m
         else:[m
             raise ValueError(f"Invalid model name: {model_name}")[m
 [m
[31m-    def convert_hf_config(self, hf_config, model_type):[m
[32m+[m[32m    @classmethod[m
[32m+[m[32m    def convert_hf_config(cls, hf_config, model_family):[m
         cfg_dict = {}[m
[31m-        if model_type == "neo":[m
[32m+[m[32m        if model_family == "neo":[m
             cfg_dict = {[m
                 "d_model": hf_config.hidden_size,[m
                 "d_head": hf_config.hidden_size // hf_config.num_heads,[m
[36m@@ -735,8 +617,24 @@[m [mclass EasyTransformer(HookedRootModule):[m
                 "use_attn_scale": False,[m
                 "use_local_attn": True,[m
                 "window_size": hf_config.window_size,[m
[32m+[m[32m                "scale_attn_by_inverse_layer_idx": False,[m
[32m+[m[32m            }[m
[32m+[m[32m        elif model_family == "gpt2":[m
[32m+[m[32m            cfg_dict = {[m
[32m+[m[32m                "d_model": hf_config.n_embd,[m
[32m+[m[32m                "d_head": hf_config.n_embd // hf_config.n_head,[m
[32m+[m[32m                "n_heads": hf_config.n_head,[m
[32m+[m[32m                "d_mlp": hf_config.n_embd * 4,[m
[32m+[m[32m                "n_layers": hf_config.n_layer,[m
[32m+[m[32m                "n_ctx": hf_config.n_ctx,[m
[32m+[m[32m                "eps": hf_config.layer_norm_epsilon,[m
[32m+[m[32m                "d_vocab": hf_config.vocab_size,[m
[32m+[m[32m                "act_fn": hf_config.activation_function,[m
[32m+[m[32m                "use_attn_scale": True,[m
[32m+[m[32m                "use_local_attn": False,[m
[32m+[m[32m                "scale_attn_by_inverse_layer_idx": False,[m
             }[m
[31m-        elif model_type == "gpt2":[m
[32m+[m[32m        elif model_family == "mistral":[m
             cfg_dict = {[m
                 "d_model": hf_config.n_embd,[m
                 "d_head": hf_config.n_embd // hf_config.n_head,[m
[36m@@ -749,8 +647,9 @@[m [mclass EasyTransformer(HookedRootModule):[m
                 "act_fn": hf_config.activation_function,[m
                 "use_attn_scale": True,[m
                 "use_local_attn": False,[m
[32m+[m[32m                "scale_attn_by_inverse_layer_idx": True,[m
             }[m
[31m-        elif model_type == "opt":[m
[32m+[m[32m        elif model_family == "opt":[m
             cfg_dict = {[m
                 "d_model": hf_config.hidden_size,[m
                 "d_head": hf_config.hidden_size // hf_config.num_attention_heads,[m
[36m@@ -763,294 +662,429 @@[m [mclass EasyTransformer(HookedRootModule):[m
                 "act_fn": hf_config.activation_function,[m
                 "use_attn_scale": True,[m
                 "use_local_attn": False,[m
[32m+[m[32m                "scale_attn_by_inverse_layer_idx": False,[m
             }[m
[31m-        elif model_type == "gptj":[m
[31m-            raise NotImplementedError[m
[31m-        elif model_type == "neox":[m
[31m-            raise NotImplementedError[m
[32m+[m[32m        elif model_family == "gptj":[m
[32m+[m[32m            cfg_dict = {[m
[32m+[m[32m                "d_model": hf_config.n_embd,[m
[32m+[m[32m                "d_head": hf_config.n_embd // hf_config.n_head,[m
[32m+[m[32m                "n_heads": hf_config.n_head,[m
[32m+[m[32m                "d_mlp": 4 * hf_config.n_embd,[m
[32m+[m[32m                "n_layers": hf_config.n_layer,[m
[32m+[m[32m                "n_ctx": hf_config.n_positions,[m
[32m+[m[32m                "eps": 1e-5,[m
[32m+[m[32m                "d_vocab": hf_config.vocab_size,[m
[32m+[m[32m                "act_fn": hf_config.activation_function,[m
[32m+[m[32m                "use_attn_scale": True,[m
[32m+[m[32m                "use_local_attn": False,[m
[32m+[m[32m                "scale_attn_by_inverse_layer_idx": False,[m
[32m+[m[32m                "parallel_attn_mlp": True,[m
[32m+[m[32m                "positional_embedding_type": "rotary",[m
[32m+[m[32m                "rotary_dim": hf_config.rotary_dim,[m
[32m+[m[32m            }[m
[32m+[m[32m        elif model_family == "neox":[m
[32m+[m[32m            cfg_dict = {[m
[32m+[m[32m                "d_model": hf_config.hidden_size,[m
[32m+[m[32m                "d_head": hf_config.hidden_size // hf_config.num_attention_heads,[m
[32m+[m[32m                "n_heads": hf_config.num_attention_heads,[m
[32m+[m[32m                "d_mlp": hf_config.intermediate_size,[m
[32m+[m[32m                "n_layers": hf_config.num_hidden_layers,[m
[32m+[m[32m                "n_ctx": hf_config.max_position_embeddings,[m
[32m+[m[32m                "eps": hf_config.layer_norm_eps,[m
[32m+[m[32m                "d_vocab": hf_config.vocab_size,[m
[32m+[m[32m                "act_fn": hf_config.hidden_act,[m
[32m+[m[32m                "use_attn_scale": True,[m
[32m+[m[32m                "use_local_attn": False,[m
[32m+[m[32m                "scale_attn_by_inverse_layer_idx": False,[m
[32m+[m[32m                "parallel_attn_mlp": True,[m
[32m+[m[32m                "positional_embedding_type": "rotary",[m
[32m+[m[32m            }[m
[32m+[m[32m            rotary_pct = hf_config.rotary_pct[m
[32m+[m[32m            cfg_dict["rotary_dim"] = round(rotary_pct * cfg_dict["d_head"])[m
         else:[m
             raise NotImplementedError[m
[31m-        cfg_dict["model_name"] = self.model_name[m
[31m-        cfg_dict["model_type"] = model_type[m
         cfg = EasyTransformerConfig.from_dict(cfg_dict)[m
         return cfg[m
 [m
[31m-    def center_weights(self):[m
[31m-        # Sets the average of each row of each weight matrix writing to the[m
[31m-        # residual stream to zero[m
[31m-        # LayerNorm subtracts the mean of the residual stream, and it's always[m
[31m-        # applied when reading from the residual stream, so this dimension is[m
[31m-        # purely noise[m
[31m-        # Also does the same for W_U, since translating the logits doesn't affect[m
[31m-        # the log_probs or loss[m
[31m-        self.embed.W_E.data -= self.embed.W_E.mean(0, keepdim=True)[m
[31m-        self.pos_embed.W_pos.data -= self.pos_embed.W_pos.mean(0, keepdim=True)[m
[31m-        self.unembed.W_U.data -= self.unembed.W_U.mean(0, keepdim=True)[m
[31m-        for block in self.blocks:[m
[31m-            block.attn.W_O.data -= einops.reduce([m
[31m-                block.attn.W_O, "index d_model d_head -> index 1 d_head", "mean"[m
[31m-            )[m
[31m-            block.mlp.W_out.data -= block.mlp.W_out.mean(0, keepdim=True)[m
[32m+[m[32m    def init_weights(self):[m
[32m+[m[32m        """[m
[32m+[m[32m        Initialize weights matrices with a normal of std=initializer_range (default=0.02). This roughly follows the GPT-2 paper's scheme (but with truncation, and not halving the std for W_pos).[m
 [m
[31m-    def load_gpt2_weights(self, gpt2):[m
[31m-        sd = self.state_dict()[m
[32m+[m[32m        LayerNorm weights are already initialized to 1.0, and all biases are initialized to 0.0 (including LayerNorm), so this just initializes weight matrices.[m[41m [m
[32m+[m[41m        [m
[32m+[m[32m        Weight matrices are set to empty by default (to save space + compute, since they're the bulk of the parameters), so it is important to call this if you are not loading in pretrained weights! Note that this function assumes that weight names being with W_[m
 [m
[31m-        sd["embed.W_E"] = gpt2.transformer.wte.weight.T[m
[31m-        sd["pos_embed.W_pos"] = gpt2.transformer.wpe.weight.T[m
[32m+[m[32m        Set seed here to ensure determinism.[m
 [m
[31m-        for l in range(self.cfg.n_layers):[m
[31m-            # In GPT-2, q,k,v are produced by one big linear map, whose output is[m
[31m-            # concat([q, k, v])[m
[31m-            W = gpt2.transformer.h[l].attn.c_attn.weight[m
[31m-            w_ln_attn = gpt2.transformer.h[l].ln_1.weight[m
[31m-            W_Q, W_K, W_V = torch.tensor_split(W, 3, dim=1)[m
[31m-            W_Q = einops.rearrange(W_Q, "m (i h)->i h m", i=self.cfg.n_heads)[m
[31m-            W_K = einops.rearrange(W_K, "m (i h)->i h m", i=self.cfg.n_heads)[m
[31m-            W_V = einops.rearrange(W_V, "m (i h)->i h m", i=self.cfg.n_heads)[m
[31m-[m
[31m-            # Fold in layer norm weights[m
[31m-            sd[f"blocks.{l}.attn.W_Q"] = W_Q * w_ln_attn[m
[31m-            sd[f"blocks.{l}.attn.W_K"] = W_K * w_ln_attn[m
[31m-            sd[f"blocks.{l}.attn.W_V"] = W_V * w_ln_attn[m
[31m-[m
[31m-            b_ln = gpt2.transformer.h[l].ln_1.bias[m
[31m-            qkv_bias = gpt2.transformer.h[l].attn.c_attn.bias[m
[31m-            qkv_bias = einops.rearrange([m
[31m-                qkv_bias,[m
[31m-                "(qkv index head)->qkv index head",[m
[31m-                qkv=3,[m
[31m-                index=self.cfg.n_heads,[m
[31m-                head=self.cfg.d_head,[m
[31m-            )[m
[31m-            # Fold in layer norm biases[m
[31m-            sd[f"blocks.{l}.attn.b_Q"] = W_Q @ b_ln + qkv_bias[0][m
[31m-            sd[f"blocks.{l}.attn.b_K"] = W_K @ b_ln + qkv_bias[1][m
[31m-            sd[f"blocks.{l}.attn.b_V"] = W_V @ b_ln + qkv_bias[2][m
[31m-[m
[31m-            W_O = gpt2.transformer.h[l].attn.c_proj.weight[m
[31m-            W_O = einops.rearrange(W_O, "(i h) m->i m h", i=self.cfg.n_heads)[m
[31m-            sd[f"blocks.{l}.attn.W_O"] = W_O[m
[31m-            sd[f"blocks.{l}.attn.b_O"] = gpt2.transformer.h[l].attn.c_proj.bias[m
[31m-[m
[31m-            W_in = gpt2.transformer.h[l].mlp.c_fc.weight.T[m
[31m-            W_out = gpt2.transformer.h[l].mlp.c_proj.weight.T[m
[31m-            # Fold in layer norm weights[m
[31m-            W_in_adj = gpt2.transformer.h[l].ln_2.weight[None, :] * W_in[m
[31m-            sd[f"blocks.{l}.mlp.W_in"] = W_in_adj[m
[31m-            # Fold in layer norm biases[m
[31m-            sd[f"blocks.{l}.mlp.b_in"] = gpt2.transformer.h[l].mlp.c_fc.bias + ([m
[31m-                W_in @ gpt2.transformer.h[l].ln_2.bias[m
[31m-            )[m
[31m-            sd[f"blocks.{l}.mlp.W_out"] = W_out[m
[31m-            sd[f"blocks.{l}.mlp.b_out"] = gpt2.transformer.h[l].mlp.c_proj.bias[m
[31m-        W_U = gpt2.lm_head.weight[m
[31m-        # Fold in layer norm weights[m
[31m-        sd["unembed.W_U"] = gpt2.transformer.ln_f.weight[None, :] * W_U[m
[31m-        # Fold in layer norm biases[m
[31m-        sd["unembed.b_U"] = gpt2.lm_head.weight @ gpt2.transformer.ln_f.bias[m
[31m-        self.load_state_dict(sd)[m
[32m+[m[32m        This does NOT follow the PyTorch scheme, which as far as I can tell is super out of date but no one has gotten round to updating it?[m
[32m+[m[32m        https://github.com/pytorch/pytorch/issues/18182[m
[32m+[m[41m        [m
[32m+[m[32m        PyTorch Transformers are especially bad - TransformerEncoder initializes all layers to the exact same weights?! https://github.com/pytorch/pytorch/issues/72253[m
 [m
[31m-    def load_neo_weights(self, neo):[m
[31m-        sd = self.state_dict()[m
[32m+[m[32m        The best paper I've found on transformer initialization is the muP paper, but haven't integrated those ideas yet: https://arxiv.org/abs/2203.03466[m
[32m+[m[32m        """[m
[32m+[m[41m        [m
[32m+[m[32m        if self.cfg.seed is not None:[m
[32m+[m[32m            torch.manual_seed(self.cfg.seed)[m
[32m+[m[41m        [m
[32m+[m[32m        for name, param in self.named_parameters():[m
[32m+[m[32m            if "W_" in name:[m
[32m+[m[32m                nn.init.normal_(param, std=self.cfg.initializer_range)[m
[32m+[m[41m    [m
[32m+[m[32m    def load_and_process_state_dict(self,[m[41m [m
[32m+[m[32m                                    state_dict: Dict[str, torch.Tensor],[m[41m [m
[32m+[m[32m                                    fold_ln: bool=True,[m[41m [m
[32m+[m[32m                                    center_writing_weights: bool = True,[m[41m [m
[32m+[m[32m                                    center_unembed: bool = True,[m
[32m+[m[32m                                    move_dict_to_device: bool = True):[m
[32m+[m[32m        """Method to load a state dict into the model, and to apply processing to simplify it. The state dict is assumed to be in the EasyTransformer format.[m
[32m+[m[41m        [m
[32m+[m[32m        See fold_layer_norm for more details on the folding and centering.[m
 [m
[31m-        sd["embed.W_E"] = neo.transformer.wte.weight.T[m
[31m-        sd["pos_embed.W_pos"] = neo.transformer.wpe.weight.T[m
[32m+[m[32m        Args:[m
[32m+[m[32m            state_dict (dict): The state dict of the model, in EasyTransformer format[m
[32m+[m[32m            fold_ln (bool, optional): Whether to fold in the LayerNorm weights to the[m[41m   [m
[32m+[m[32m                subsequent linear layer. This does not change the computation. Defaults to True.[m
[32m+[m[32m            center_writing_weights (bool, optional): Whether to center weights writing to the[m[41m [m
[32m+[m[32m                residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the computation. Defaults to True.[m
[32m+[m[32m            center_unembed (bool, optional): Whether to center W_U (ie set mean to be zero).[m[41m [m
[32m+[m[32m                Softmax is translation invariant so this doesn't affect log probs or loss, but does change logits. Defaults to True.[m
[32m+[m[32m            move_dict_to_device (bool, optional): Whether to move the state dict to the device of the model. Defaults to True.[m
[32m+[m[32m        """[m
[32m+[m[32m        if move_dict_to_device:[m
[32m+[m[32m            state_dict = {k: v.to(self.cfg.device) for k, v in state_dict.items()}[m
[32m+[m[32m        state_dict = self.fill_missing_keys(state_dict)[m
[32m+[m[32m        if fold_ln:[m
[32m+[m[32m            state_dict = self.fold_layer_norm(state_dict)[m
[32m+[m[32m        if center_writing_weights:[m
[32m+[m[32m            state_dict = self.center_writing_weights(state_dict)[m
[32m+[m[32m        if center_unembed:[m
[32m+[m[32m            state_dict = self.center_unembed(state_dict)[m
[32m+[m[32m        self.load_state_dict(state_dict)[m
 [m
[31m-        for l in range(self.cfg.n_layers):[m
[31m-            w_ln_attn = neo.transformer.h[l].ln_1.weight[m
[31m-            W_Q = neo.transformer.h[l].attn.attention.q_proj.weight[m
[31m-            W_K = neo.transformer.h[l].attn.attention.k_proj.weight[m
[31m-            W_V = neo.transformer.h[l].attn.attention.v_proj.weight[m
[31m-            W_Q = einops.rearrange(W_Q, "(i h) m->i h m", i=self.cfg.n_heads)[m
[31m-            W_K = einops.rearrange(W_K, "(i h) m->i h m", i=self.cfg.n_heads)[m
[31m-            W_V = einops.rearrange(W_V, "(i h) m->i h m", i=self.cfg.n_heads)[m
[31m-[m
[31m-            sd[f"blocks.{l}.attn.W_Q"] = W_Q * w_ln_attn[m
[31m-            sd[f"blocks.{l}.attn.W_K"] = W_K * w_ln_attn[m
[31m-            sd[f"blocks.{l}.attn.W_V"] = W_V * w_ln_attn[m
[31m-[m
[31m-            b_ln = neo.transformer.h[l].ln_1.bias[m
[31m-            sd[f"blocks.{l}.attn.b_Q"] = W_Q @ b_ln[m
[31m-            sd[f"blocks.{l}.attn.b_K"] = W_K @ b_ln[m
[31m-            sd[f"blocks.{l}.attn.b_V"] = W_V @ b_ln[m
[31m-[m
[31m-            W_O = neo.transformer.h[l].attn.attention.out_proj.weight[m
[31m-            W_O = einops.rearrange(W_O, "m (i h)->i m h", i=self.cfg.n_heads)[m
[31m-            sd[f"blocks.{l}.attn.W_O"] = W_O[m
[31m-            sd[f"blocks.{l}.attn.b_O"] = neo.transformer.h[[m
[31m-                l[m
[31m-            ].attn.attention.out_proj.bias[m
[31m-[m
[31m-            W_in = neo.transformer.h[l].mlp.c_fc.weight[m
[31m-            W_out = neo.transformer.h[l].mlp.c_proj.weight[m
[31m-            W_in_adj = neo.transformer.h[l].ln_2.weight[None, :] * W_in[m
[31m-            sd[f"blocks.{l}.mlp.W_in"] = W_in_adj[m
[31m-            sd[f"blocks.{l}.mlp.b_in"] = neo.transformer.h[l].mlp.c_fc.bias + ([m
[31m-                W_in @ neo.transformer.h[l].ln_2.bias[m
[31m-            )[m
[31m-            sd[f"blocks.{l}.mlp.W_out"] = W_out[m
[31m-            sd[f"blocks.{l}.mlp.b_out"] = neo.transformer.h[l].mlp.c_proj.bias[m
[31m-        W_U = neo.lm_head.weight[m
[31m-        sd["unembed.W_U"] = neo.transformer.ln_f.weight[None, :] * W_U[m
[31m-        sd["unembed.b_U"] = neo.lm_head.weight @ neo.transformer.ln_f.bias[m
[31m-        self.load_state_dict(sd)[m
 [m
[31m-    def load_neox_weights(self, neox):[m
[31m-        raise NotImplementedError[m
[32m+[m[32m    def fill_missing_keys(self, state_dict):[m
[32m+[m[32m        """Takes in a state dict from a pretrained model, and fills in any missing keys with the default initialization.[m
 [m
[31m-    def load_gptj_weights(self, gptj):[m
[31m-        raise NotImplementedError[m
[32m+[m[32m        This function is assumed to be run before weights are initialized.[m
 [m
[31m-    def load_opt_weights(self, opt):[m
[31m-        sd = self.state_dict()[m
[32m+[m[32m        Args:[m
[32m+[m[32m            state_dict (dict): State dict from a pretrained model[m
 [m
[31m-        sd["embed.W_E"] = opt.model.decoder.embed_tokens.weight.T[m
[31m-        sd["pos_embed.W_pos"] = opt.model.decoder.embed_positions.weight.T[:, 2:][m
[32m+[m[32m        Returns:[m
[32m+[m[32m            dict: State dict with missing keys filled in[m
[32m+[m[32m        """[m
[32m+[m[32m        # Get the default state dict[m
[32m+[m[32m        default_state_dict = self.state_dict()[m
[32m+[m[32m        # Get the keys that are missing from the pretrained model[m
[32m+[m[32m        missing_keys = set(default_state_dict.keys()) - set(state_dict.keys())[m
[32m+[m[32m        # Fill in the missing keys with the default initialization[m
[32m+[m[32m        for key in missing_keys:[m
[32m+[m[32m            if 'hf_model' in key:[m
[32m+[m[32m                # Skip keys that are from the HuggingFace model, if loading from HF.[m
[32m+[m[32m                continue[m
[32m+[m[32m            if 'W_' in key:[m
[32m+[m[32m                logging.warning("Missing key for a weight matrix in pretrained, filled in with an empty tensor: {}".format(key))[m
[32m+[m[32m            state_dict[key] = default_state_dict[key][m
[32m+[m[32m        return state_dict[m
[32m+[m[41m    [m
[32m+[m[32m    def fold_layer_norm(self, state_dict: Dict[str, torch.Tensor]):[m
[32m+[m[32m        """Takes in a state dict from a pretrained model, formatted to be consistent with EasyTransformer but with LayerNorm weights and biases. Folds these into the neighbouring weights. See EasyTransformerConfig for more details[m
 [m
[32m+[m[32m        Args:[m
[32m+[m[32m            state_dict (Dict[str, torch.Tensor]): State dict of pretrained model[m
[32m+[m[32m        """[m
         for l in range(self.cfg.n_layers):[m
[31m-            w_ln_attn = opt.model.decoder.layers[l].self_attn_layer_norm.weight[m
[31m-            W_Q = opt.model.decoder.layers[l].self_attn.q_proj.weight[m
[31m-            W_K = opt.model.decoder.layers[l].self_attn.k_proj.weight[m
[31m-            W_V = opt.model.decoder.layers[l].self_attn.v_proj.weight[m
[31m-            W_Q = einops.rearrange([m
[31m-                W_Q,[m
[31m-                "(index d_head) d_model->index d_head d_model",[m
[31m-                index=self.cfg.n_heads,[m
[31m-            )[m
[31m-            W_K = einops.rearrange([m
[31m-                W_K,[m
[31m-                "(index d_head) d_model->index d_head d_model",[m
[31m-                index=self.cfg.n_heads,[m
[31m-            )[m
[31m-            W_V = einops.rearrange([m
[31m-                W_V,[m
[31m-                "(index d_head) d_model->index d_head d_model",[m
[31m-                index=self.cfg.n_heads,[m
[31m-            )[m
[31m-[m
[31m-            sd[f"blocks.{l}.attn.W_Q"] = W_Q * w_ln_attn[m
[31m-            sd[f"blocks.{l}.attn.W_K"] = W_K * w_ln_attn[m
[31m-            sd[f"blocks.{l}.attn.W_V"] = W_V * w_ln_attn[m
[31m-[m
[31m-            b_ln = opt.model.decoder.layers[l].self_attn_layer_norm.bias[m
[31m-            q_bias = einops.rearrange([m
[31m-                opt.model.decoder.layers[l].self_attn.q_proj.bias,[m
[31m-                "(head_index d_head)->head_index d_head",[m
[31m-                head_index=self.cfg.n_heads,[m
[31m-                d_head=self.cfg.d_head,[m
[31m-            )[m
[31m-            k_bias = einops.rearrange([m
[31m-                opt.model.decoder.layers[l].self_attn.k_proj.bias,[m
[31m-                "(head_index d_head)->head_index d_head",[m
[31m-                head_index=self.cfg.n_heads,[m
[31m-                d_head=self.cfg.d_head,[m
[31m-            )[m
[31m-            v_bias = einops.rearrange([m
[31m-                opt.model.decoder.layers[l].self_attn.v_proj.bias,[m
[31m-                "(head_index d_head)->head_index d_head",[m
[31m-                head_index=self.cfg.n_heads,[m
[31m-                d_head=self.cfg.d_head,[m
[31m-            )[m
[31m-[m
[31m-            sd[f"blocks.{l}.attn.b_Q"] = W_Q @ b_ln + q_bias[m
[31m-            sd[f"blocks.{l}.attn.b_K"] = W_K @ b_ln + k_bias[m
[31m-            sd[f"blocks.{l}.attn.b_V"] = W_V @ b_ln + v_bias[m
[31m-[m
[31m-            W_O = opt.model.decoder.layers[l].self_attn.out_proj.weight[m
[31m-            W_O = einops.rearrange([m
[31m-                W_O,[m
[31m-                "d_model (index d_head)->index d_model d_head",[m
[31m-                index=self.cfg.n_heads,[m
[31m-            )[m
[31m-            sd[f"blocks.{l}.attn.W_O"] = W_O[m
[31m-            sd[f"blocks.{l}.attn.b_O"] = opt.model.decoder.layers[[m
[31m-                l[m
[31m-            ].self_attn.out_proj.bias[m
[31m-[m
[31m-            W_in = opt.model.decoder.layers[l].fc1.weight[m
[31m-            W_out = opt.model.decoder.layers[l].fc2.weight[m
[31m-            W_in_adj = ([m
[31m-                opt.model.decoder.layers[l].final_layer_norm.weight[None, :] * W_in[m
[31m-            )[m
[31m-            sd[f"blocks.{l}.mlp.W_in"] = W_in_adj[m
[31m-            sd[f"blocks.{l}.mlp.b_in"] = opt.model.decoder.layers[l].fc1.bias + ([m
[31m-                W_in @ opt.model.decoder.layers[l].final_layer_norm.bias[m
[31m-            )[m
[31m-            sd[f"blocks.{l}.mlp.W_out"] = W_out[m
[31m-            sd[f"blocks.{l}.mlp.b_out"] = opt.model.decoder.layers[l].fc2.bias[m
[31m-        W_U = opt.lm_head.weight[m
[31m-        sd["unembed.W_U"] = opt.model.decoder.final_layer_norm.weight[None, :] * W_U[m
[31m-        sd["unembed.b_U"] = W_U @ opt.model.decoder.final_layer_norm.bias[m
[31m-        self.load_state_dict(sd)[m
[32m+[m[32m            # Fold ln1 into attention - it's important to fold biases first,[m[41m [m
[32m+[m[32m            # since biases depend on weights but not vice versa[m
[32m+[m[32m            # The various indexing is just to broadcast ln.b and ln.w along every axis other than d_model. Each weight matrix right multiplies.[m
[32m+[m[32m            # To fold in the bias, we use the W_ matrix to map it to the hidden space of the layer, so we need to sum along axis -2, which is the residual stream space axis.[m
[32m+[m[32m            state_dict[f"blocks.{l}.attn.b_Q"] = state_dict[f"blocks.{l}.attn.b_Q"] + (state_dict[f"blocks.{l}.attn.W_Q"] * state_dict[f"blocks.{l}.ln1.b"][None, :, None]).sum(-2)[m
[32m+[m[32m            state_dict[f"blocks.{l}.attn.b_K"] = state_dict[f"blocks.{l}.attn.b_K"] + (state_dict[f"blocks.{l}.attn.W_K"] * state_dict[f"blocks.{l}.ln1.b"][None, :, None]).sum(-2)[m
[32m+[m[32m            state_dict[f"blocks.{l}.attn.b_V"] = state_dict[f"blocks.{l}.attn.b_V"] + (state_dict[f"blocks.{l}.attn.W_V"] * state_dict[f"blocks.{l}.ln1.b"][None, :, None]).sum(-2)[m
[32m+[m[41m            [m
[32m+[m[32m            state_dict[f"blocks.{l}.attn.W_Q"] = state_dict[f"blocks.{l}.attn.W_Q"] * state_dict[f"blocks.{l}.ln1.w"][None, :, None][m
[32m+[m[32m            state_dict[f"blocks.{l}.attn.W_K"] = state_dict[f"blocks.{l}.attn.W_K"] * state_dict[f"blocks.{l}.ln1.w"][None, :, None][m
[32m+[m[32m            state_dict[f"blocks.{l}.attn.W_V"] = state_dict[f"blocks.{l}.attn.W_V"] * state_dict[f"blocks.{l}.ln1.w"][None, :, None][m
[32m+[m[32m            del state_dict[f"blocks.{l}.ln1.w"], state_dict[f"blocks.{l}.ln1.b"],[m[41m [m
[32m+[m[41m            [m
[32m+[m[41m            [m
[32m+[m[32m            # Fold ln2 into MLP[m
[32m+[m[32m            if not self.cfg.attn_only:[m
[32m+[m[32m                state_dict[f"blocks.{l}.mlp.b_in"] = state_dict[f"blocks.{l}.mlp.b_in"] + (state_dict[f"blocks.{l}.mlp.W_in"] * state_dict[f"blocks.{l}.ln2.b"][:, None]).sum(-2)[m
[32m+[m[32m                state_dict[f"blocks.{l}.mlp.W_in"] = state_dict[f"blocks.{l}.mlp.W_in"] * state_dict[f"blocks.{l}.ln2.w"][:, None][m
[32m+[m[32m                del state_dict[f"blocks.{l}.ln2.w"], state_dict[f"blocks.{l}.ln2.b"][m
[32m+[m
[32m+[m[32m            if self.cfg.act_fn.startswith("solu"):[m
[32m+[m[32m                # Fold ln3 into activation[m
[32m+[m[32m                state_dict[f"blocks.{l}.mlp.b_out"] = state_dict[f"blocks.{l}.mlp.b_out"] + (state_dict[f"blocks.{l}.mlp.W_out"] * state_dict[f"blocks.{l}.mlp.ln.b"][:, None]).sum(-2)[m
[32m+[m[32m                state_dict[f"blocks.{l}.mlp.W_out"] = state_dict[f"blocks.{l}.mlp.W_out"] * state_dict[f"blocks.{l}.mlp.ln.w"][:, None][m
[32m+[m[32m                del state_dict[f"blocks.{l}.mlp.ln.w"], state_dict[f"blocks.{l}.mlp.ln.b"][m
[32m+[m[32m        # Fold ln_final into Unembed[m
[32m+[m[32m        if not self.cfg.final_rms:[m
[32m+[m[32m            # Dumb bug from my old SoLU training code, some models have RMSNorm instead of LayerNorm pre unembed.[m
[32m+[m[32m            state_dict[f"unembed.b_U"] = state_dict[f"unembed.b_U"] + (state_dict[f"unembed.W_U"] * state_dict[f"ln_final.b"][:, None]).sum(dim=-2)[m
[32m+[m[32m            del state_dict[f"ln_final.b"][m
[32m+[m[32m        state_dict[f"unembed.W_U"] = state_dict[f"unembed.W_U"] * state_dict[f"ln_final.w"][:, None][m
[32m+[m[32m        del state_dict[f"ln_final.w"][m
[32m+[m[32m        return state_dict[m
[32m+[m[41m    [m
[32m+[m[41m    [m
[32m+[m[32m    def center_writing_weights(self, state_dict: Dict[str, torch.Tensor]):[m
[32m+[m[32m        """Centers the weights of the model that write to the residual stream - W_out, W_E, W_pos and W_out. This is done by subtracting the mean of the weights from the weights themselves. This is done in-place. See fold_layer_norm for more details.[m
[32m+[m[32m        """[m
[32m+[m[32m        state_dict['embed.W_E'] = state_dict['embed.W_E'] - state_dict['embed.W_E'].mean(-1, keepdim=True)[m
[32m+[m[32m        if self.cfg.positional_embedding_type != "rotary":[m
[32m+[m[32m            state_dict['pos_embed.W_pos'] = state_dict['pos_embed.W_pos'] - state_dict['pos_embed.W_pos'].mean(-1, keepdim=True)[m
[32m+[m[32m        for l in range(self.cfg.n_layers):[m
[32m+[m[32m            state_dict[f'blocks.{l}.attn.W_O'] = state_dict[f'blocks.{l}.attn.W_O'] - state_dict[f'blocks.{l}.attn.W_O'].mean(-1, keepdim=True) # W_O is [head_index, d_model, d_head][m
[32m+[m[32m            state_dict[f'blocks.{l}.attn.b_O'] = state_dict[f'blocks.{l}.attn.b_O'] - state_dict[f'blocks.{l}.attn.b_O'].mean() # b_O is [d_model][m
[32m+[m[32m            state_dict[f'blocks.{l}.mlp.W_out'] = state_dict[f'blocks.{l}.mlp.W_out'] - state_dict[f'blocks.{l}.mlp.W_out'].mean(-1, keepdim=True)[m
[32m+[m[32m            state_dict[f'blocks.{l}.mlp.b_out'] = state_dict[f'blocks.{l}.mlp.b_out'] - state_dict[f'blocks.{l}.mlp.b_out'].mean()[m
[32m+[m[32m        return state_dict[m
[32m+[m[41m    [m
[32m+[m[32m    def center_unembed(self, state_dict: Dict[str, torch.Tensor]):[m
[32m+[m[32m        """Centers the unembedding weights W_U. This is done by subtracting the mean of the weights from the weights themselves. This is done in-place. As softmax is translation invariant, this changes the logits but not the log probs, and makes the model logits (slightly) more interpretable - when trying to understand how components contribute to the logits, we'll be less misled by components that just add something to every logit.[m
[32m+[m[32m        """[m
[32m+[m[32m        state_dict['unembed.W_U'] = state_dict['unembed.W_U'] - state_dict['unembed.W_U'].mean(-1, keepdim=True)[m
[32m+[m[32m        state_dict['unembed.b_U'] = state_dict['unembed.b_U'] - state_dict['unembed.b_U'].mean()[m
[32m+[m[32m        return state_dict[m
[32m+[m[41m    [m
[32m+[m[32m    def set_use_attn_result(self, use_attn_result):[m
[32m+[m[32m        """[m
[32m+[m[32m        Toggles whether to explicitly calculate and expose the result for each attention head - useful for interpretability but can easily burn through GPU memory.[m
[32m+[m[32m        """[m
[32m+[m[32m        self.cfg.use_attn_result = use_attn_result[m
[32m+[m[41m        [m
[32m+[m[32m    @torch.inference_mode()[m
[32m+[m[32m    def generate([m
[32m+[m[32m        self,[m
[32m+[m[32m        input: Union[str, torch.Tensor] = "",[m
[32m+[m[32m        max_new_tokens: int = 10,[m
[32m+[m[32m        stop_at_eos: bool = True,[m
[32m+[m[32m        eos_token_id: Optional[int] = None,[m
[32m+[m[32m        do_sample: bool = False,[m
[32m+[m[32m        top_k: Optional[int] = None,[m
[32m+[m[32m        top_p: Optional[float] = None,[m
[32m+[m[32m        temperature: float = 1.0,[m
[32m+[m[32m        freq_penalty: float = 0.0,[m
[32m+[m[32m        num_return_sequences: int = 1,[m
[32m+[m[32m        use_past_kv_cache: bool = True,[m
[32m+[m[32m        prepend_bos = True,[m
[32m+[m[32m        return_type: Optional[str] = "input",[m
[32m+[m[32m    ):[m
[32m+[m[32m        """[m
[32m+[m[32m        Sample tokens from the model until the model outputs eos_token or max_new_tokens is reached.[m
 [m
[31m-    def load_bloom_weights(self, bloom):[m
[31m-        raise NotImplementedError[m
[32m+[m[32m        To avoid fiddling with ragged tensors, if we input a batch of text and some sequences finish (by producing an EOT token), we keep running the model on the entire batch, but throw away the output for a finished sequence and just keep adding EOTs to pad.[m
 [m
[31m-    def init_weights(self):[m
[31m-        """[m
[31m-        Initialize weights according to default Pytorch initialization.[m
[32m+[m[32m        This supports entering a single string, but not a list of strings - if the strings don't tokenize to exactly the same length, this gets messy. If that functionality is needed, convert them to a batch of tokens and input that instead.[m
 [m
[31m-        LayerNorm weights are already initialized to 1.0 (and biases to 0.0)[m
[31m-        in the constructor[m
[32m+[m[32m        Args:[m
[32m+[m[32m            input (int): Either a batch of tokens ([batch, pos]) or a text string (this will be converted to a batch of tokens with batch size 1)[m
[32m+[m[32m            max_new_tokens (int): Maximum number of tokens to generate[m
[32m+[m[32m            stop_at_eos (bool): If True, stop generating tokens when the model outputs eos_token[m
[32m+[m[32m            eos_token_id (int, *optional*): The token ID to use for end of sentence. If None, use the tokenizer's eos_token_id - required if using stop_at_eos[m
[32m+[m[32m            do_sample (bool): If True, sample from the model's output distribution. Otherwise, use greedy search (take the max logit each time).[m
[32m+[m[32m            top_k (int): Number of tokens to sample from. If None, sample from all tokens[m
[32m+[m[32m            top_p (float): Probability mass to sample from. If 1.0, sample from all tokens. If <1.0, we take the top tokens with cumulative probability >= top_p[m
[32m+[m[32m            temperature (float): Temperature for sampling. Higher values will make the model more random (limit of temp -> 0 is just taking the top token, limit of temp -> inf is sampling from a uniform distribution)[m
[32m+[m[32m            freq_penalty (float): Frequency penalty for sampling - how much to penalise previous tokens. Higher values will make the model more random[m
[32m+[m[32m            use_past_kv_cache (bool): If True, create and use cache to speed up generation[m
[32m+[m[32m            prepend_bos (bool): If True, prepend the model's bos_token_id to the input, if it's a string. Irrelevant if input is a tensor.[m
[32m+[m[32m            return_type (str, *optional*): The type of the output to return - either a string (str), a tensor of tokens (tensor) or whatever the format of the input was (input).[m[41m [m
[32m+[m[32m        Returns:[m
[32m+[m[32m            outputs (torch.Tensor): [batch, pos + max_new_tokens], generated sequence of new tokens - by default returns same type as input[m
         """[m
[31m-        # Initialize weights with std 1/sqrt(d_model) so the vector has variance 1[m
[31m-        nn.init.normal_(self.embed.W_E, std=self.cfg.d_model ** (-0.5))[m
[31m-        nn.init.normal_(self.pos_embed.W_pos, std=self.cfg.d_model ** (-0.5))[m
[32m+[m[32m        if type(input) == str:[m
[32m+[m[32m            # If text, convert to tokens (batch_size=1)[m
[32m+[m[32m            assert ([m
[32m+[m[32m                self.tokenizer is not None[m
[32m+[m[32m            ), "Must provide a tokenizer if passing a string to the model"[m
[32m+[m[32m            tokens = self.to_tokens(input, prepend_bos=prepend_bos)[m
[32m+[m[32m        else:[m
[32m+[m[32m            tokens = input[m
 [m
[31m-        def init_linear_weight_and_bias(weight, bias):[m
[31m-            nn.init.kaiming_uniform_(weight, a=np.sqrt(5))[m
[31m-            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(weight)[m
[31m-            bound = 1 / np.sqrt(fan_in) if fan_in > 0 else 0[m
[31m-            nn.init.uniform_(bias, -bound, bound)[m
[32m+[m[32m        if return_type == "input":[m
[32m+[m[32m            if type(input) == str:[m
[32m+[m[32m                return_type = "str"[m
[32m+[m[32m            else:[m
[32m+[m[32m                return_type = "tensor"[m
 [m
[31m-        for l in range(self.cfg.n_layers):[m
[31m-            init_linear_weight_and_bias([m
[31m-                self.blocks[l].attn.W_Q, self.blocks[l].attn.b_Q[m
[31m-            )[m
[31m-            init_linear_weight_and_bias([m
[31m-                self.blocks[l].attn.W_K, self.blocks[l].attn.b_K[m
[31m-            )[m
[31m-            init_linear_weight_and_bias([m
[31m-                self.blocks[l].attn.W_V, self.blocks[l].attn.b_V[m
[31m-            )[m
[31m-            init_linear_weight_and_bias([m
[31m-                self.blocks[l].attn.W_O, self.blocks[l].attn.b_O[m
[31m-            )[m
[31m-            init_linear_weight_and_bias([m
[31m-                self.blocks[l].mlp.W_in, self.blocks[l].mlp.b_in[m
[31m-            )[m
[31m-            init_linear_weight_and_bias([m
[31m-                self.blocks[l].mlp.W_out, self.blocks[l].mlp.b_out[m
[31m-            )[m
[32m+[m[32m        assert isinstance(tokens, torch.Tensor)[m
[32m+[m[32m        batch_size, ctx_length = tokens.shape[m
[32m+[m[32m        tokens = tokens.to(self.cfg.device)[m
[32m+[m[32m        if use_past_kv_cache:[m
[32m+[m[32m            past_kv_cache = EasyTransformerKeyValueCache.init_cache(self.cfg, self.cfg.device, batch_size)[m
[32m+[m[32m        else:[m
[32m+[m[32m            past_kv_cache = None[m
 [m
[31m-            if self.cfg.gated_act_fn:[m
[31m-                init_linear_weight_and_bias([m
[31m-                    self.blocks[l].mlp.W_gate, self.blocks[l].mlp.b_gate[m
[31m-                )[m
[32m+[m[32m        if stop_at_eos and eos_token_id is None:[m
[32m+[m[32m            assert ([m
[32m+[m[32m                self.tokenizer is not None and self.tokenizer.eos_token_id is not None[m
[32m+[m[32m            ), "Must pass a eos_token_id if stop_at_eos is True and tokenizer is None or has no eos_token_id"[m
[32m+[m
[32m+[m[32m            eos_token_id = self.tokenizer.eos_token_id[m
[32m+[m[41m            [m
[32m+[m[32m            # An array to track which sequences in the batch have finished.[m
[32m+[m[32m            finished_sequences = torch.zeros(batch_size, dtype=torch.bool, device=self.cfg.device)[m
[32m+[m[41m        [m
[32m+[m[32m        # Currently nothing in EasyTransformer changes with eval, but this is here in case that changes in the future[m
[32m+[m[32m        self.eval()[m
[32m+[m[32m        for index in tqdm.tqdm(range(max_new_tokens)):[m
[32m+[m[32m            # While generating, we keep generating logits, throw away all but the final logits, and then use those logits to sample from the distribution[m
[32m+[m[32m            # We keep adding the sampled tokens to the end of tokens.[m
[32m+[m[32m            if use_past_kv_cache:[m
[32m+[m[32m                # We just take the final tokens, as a [batch, 1] tensor[m
[32m+[m[32m                if index>0:[m
[32m+[m[32m                    logits = self.forward(tokens[:, -1:], return_type="logits", past_kv_cache=past_kv_cache)[m
[32m+[m[32m                else:[m
[32m+[m[32m                    logits = self.forward(tokens, return_type="logits", past_kv_cache=past_kv_cache)[m
 [m
[31m-        init_linear_weight_and_bias(self.unembed.W_U, self.unembed.b_U)[m
[32m+[m[32m            else:[m
[32m+[m[32m                # We input the entire sequence, as a [batch, pos] tensor, since we aren't using the cache[m
[32m+[m[32m                logits = self.forward(tokens, return_type="logits")[m
[32m+[m[32m            final_logits = logits[:, -1, :][m
[32m+[m
[32m+[m[32m            sampled_tokens = sample_logits([m
[32m+[m[32m                final_logits,[m
[32m+[m[32m                top_k=top_k,[m
[32m+[m[32m                top_p=top_p,[m
[32m+[m[32m                temperature=temperature,[m
[32m+[m[32m                freq_penalty=freq_penalty,[m
[32m+[m[32m                tokens=tokens,[m
[32m+[m[32m            )[m
[32m+[m[41m            [m
[32m+[m[32m            if stop_at_eos:[m
[32m+[m[32m                # For all unfinished sequences, add on the next token. If a sequence finished, we throw away the generated token and instead add an EOS token to pad.[m
[32m+[m[32m                sampled_tokens[finished_sequences] = eos_token_id[m
[32m+[m[32m                finished_sequences.logical_or_(sampled_tokens == eos_token_id)[m
[32m+[m[41m            [m
[32m+[m[32m            tokens = torch.cat([tokens, sampled_tokens.unsqueeze(-1)], dim=-1)[m
[32m+[m
[32m+[m[32m            if stop_at_eos and finished_sequences.all():[m
[32m+[m[32m                break[m
[32m+[m[41m        [m
[32m+[m[32m        if return_type == "str":[m
[32m+[m[32m            if prepend_bos:[m
[32m+[m[32m                # If we prepended a BOS token, remove it when returning output.[m
[32m+[m[32m                return self.tokenizer.decode(tokens[0, 1:])[m
[32m+[m[32m            else:[m
[32m+[m[32m                return self.tokenizer.decode(tokens[0])[m
 [m
[31m-    def cross_entropy_loss([m
[31m-        self, logits: torch.Tensor, tokens: torch.Tensor, return_per_token: bool = False[m
[31m-    ):[m
[31m-        """Cross entropy loss for the language model.[m
[32m+[m[32m        else:[m
[32m+[m[32m            return tokens[m
[32m+[m[41m    [m
[32m+[m[32m    # Give access to all weights as properties. Layer weights are stacked into one massive tensor and a cache is used to avoid repeated computation. If GPU memory is a bottleneck, don't use these properties![m
[32m+[m[32m    @property[m
[32m+[m[32m    def W_U(self):[m
[32m+[m[32m        return self.unembed.W_U[m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    def W_E(self):[m
[32m+[m[32m        return self.embed.W_E[m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    def W_pos(self):[m
[32m+[m[32m        return self.pos_embed.W_pos[m
[32m+[m
[32m+[m[32m    @property[m
[32m+[m[32m    def W_E_pos(self):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        Concatenated W_E and W_pos. Used as a full (overcomplete) basis of the input space, useful for full QK and full OV circuits.[m
[32m+[m[32m        """[m
[32m+[m[32m        return torch.cat([self.W_E, self.W_pos], dim=1)[m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    @lru_cache(maxsize=None)[m
[32m+[m[32m    def W_K(self):[m
[32m+[m[32m        """Stacks the key weights across all layers"""[m
[32m+[m[32m        return torch.stack([block.attn.W_K for block in self.blocks], dim=0)[m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    @lru_cache(maxsize=None)[m
[32m+[m[32m    def W_Q(self):[m
[32m+[m[32m        """Stacks the query weights across all layers"""[m
[32m+[m[32m        return torch.stack([block.attn.W_Q for block in self.blocks], dim=0)[m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    @lru_cache(maxsize=None)[m
[32m+[m[32m    def W_V(self):[m
[32m+[m[32m        """Stacks the value weights across all layers"""[m
[32m+[m[32m        return torch.stack([block.attn.W_V for block in self.blocks], dim=0)[m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    @lru_cache(maxsize=None)[m
[32m+[m[32m    def W_O(self):[m
[32m+[m[32m        """Stacks the attn output weights across all layers"""[m
[32m+[m[32m        return torch.stack([block.attn.W_O for block in self.blocks], dim=0)[m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    @lru_cache(maxsize=None)[m
[32m+[m[32m    def W_in(self):[m
[32m+[m[32m        """Stacks the MLP input weights across all layers"""[m
[32m+[m[32m        return torch.stack([block.mlp.W_in for block in self.blocks], dim=0)[m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    @lru_cache(maxsize=None)[m
[32m+[m[32m    def W_out(self):[m
[32m+[m[32m        """Stacks the MLP output weights across all layers"""[m
[32m+[m[32m        return torch.stack([block.mlp.W_out for block in self.blocks], dim=0)[m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    def QK(self):[m
[32m+[m[32m        return FactoredMatrix(self.W_Q, self.W_K.transpose(-2, -1))[m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    def OV(self):[m
[32m+[m[32m        return FactoredMatrix(self.W_V, self.W_O)[m
[32m+[m[41m       [m
[32m+[m[32m    # Various utility functions[m
[32m+[m[32m    def accumulated_bias([m
[32m+[m[32m        self,[m[41m [m
[32m+[m[32m        layer: int,[m[41m [m
[32m+[m[32m        mlp_input: bool=False):[m
[32m+[m[32m        """Returns the accumulated bias from all layer outputs (ie the b_Os and b_outs), up to the input of layer L.[m
 [m
         Args:[m
[31m-            logits (torch.Tensor): Logits. Shape [batch, pos, d_vocab][m
[31m-            tokens (torch.Tensor[int64]): Input tokens. Shape [batch, pos][m
[31m-            return_per_token (bool, optional): Whether to return the log probs predicted for the correct token, or the loss (ie mean of the predicted log probs). Defaults to False.[m
[31m-[m
[32m+[m[32m            layer (int): Layer number, in [0, n_layers]. layer==0 means no layers, layer==n_layers means all layers.[m
[32m+[m[32m            mlp_input (bool): If True, we take the bias up to the input of the MLP of layer L (ie we include the bias from the attention output of the current layer, otherwise just biases from previous layers)[m
         Returns:[m
[31m-            _type_: _description_[m
[32m+[m[32m            bias (torch.Tensor): [d_model], accumulated bias[m
[32m+[m[32m        """[m
[32m+[m[32m        accumulated_bias = torch.zeros(self.cfg.d_model, device=self.cfg.device)[m
[32m+[m
[32m+[m[32m        for i in range(layer):[m
[32m+[m[32m            accumulated_bias += self.blocks[i].attn.b_O[m
[32m+[m[32m            accumulated_bias += self.blocks[i].mlp.b_out[m
[32m+[m[32m        if mlp_input:[m
[32m+[m[32m            assert layer<self.cfg.n_layers, "Cannot include attn_bias from beyond the final layer"[m
[32m+[m[32m            accumulated_bias += self.blocks[layer].attn.b_O[m
[32m+[m[32m        return accumulated_bias[m
[32m+[m[41m    [m
[32m+[m[32m    def all_composition_scores(self, mode):[m
[32m+[m[32m        """Returns the Composition scores for all pairs of heads, as a L1, H1, L2, H2 tensor (which is upper triangular on the first and third axes)[m
[32m+[m[41m        [m
[32m+[m[32m        mode is one of ["Q", "K", "V"][m
[32m+[m
[32m+[m[32m        See https://transformer-circuits.pub/2021/framework/index.html#:~:text=The%20above%20diagram%20shows%20Q%2D%2C%20K%2D%2C%20and%20V%2DComposition for three metrics used[m
         """[m
[31m-        log_probs = F.log_softmax(logits, dim=-1)[m
[31m-        # Use torch.gather to find the log probs of the correct tokens[m
[31m-        # Offsets needed because we're predicting the NEXT token (this means the final logit is meaningless)[m
[31m-        # None and [..., 0] needed because the tensor used in gather must have the same rank.[m
[31m-        predicted_log_probs = log_probs[..., :-1, :].gather([m
[31m-            dim=-1, index=tokens[..., 1:, None][m
[31m-        )[..., 0][m
[31m-        if return_per_token:[m
[31m-            return -predicted_log_probs[m
[32m+[m[32m        left = self.OV[m
[32m+[m[32m        if mode=="Q":[m
[32m+[m[32m            right = self.QK[m
[32m+[m[32m        elif mode=="K":[m
[32m+[m[32m            right = self.QK.T[m
[32m+[m[32m        elif mode=="V":[m
[32m+[m[32m            right = self.OV[m
         else:[m
[31m-            return -predicted_log_probs.mean()[m
[32m+[m[32m            raise ValueError(f"mode must be one of ['Q', 'K', 'V'] not {mode}")[m
[32m+[m
[32m+[m[32m        scores = composition_scores(left, right, broadcast_dims=True)[m
[32m+[m[32m        # Mask scores to be zero for all pairs with the right head in the same layer or earlier layer than the left head.[m
[32m+[m[32m        mask = torch.arange(self.cfg.n_layers, device=self.cfg.device)[:, None, None, None] < torch.arange(self.cfg.n_layers, device=self.cfg.device)[None, None, :, None][m
[32m+[m[32m        scores = torch.where(mask, scores, torch.zeros_like(scores))[m
[32m+[m[32m        return scores[m
[32m+[m[41m    [m
[32m+[m[32m    def all_head_labels(self):[m
[32m+[m[32m        return [f"L{l}H{h}" for l in range(self.cfg.n_layers) for h in range(self.cfg.n_heads)][m
[41m+[m
[1mdiff --git a/easy_transformer/EasyTransformerConfig.py b/easy_transformer/EasyTransformerConfig.py[m
[1mindex b8624dd..1d4b874 100644[m
[1m--- a/easy_transformer/EasyTransformerConfig.py[m
[1m+++ b/easy_transformer/EasyTransformerConfig.py[m
[36m@@ -1,23 +1,31 @@[m
[31m-# %%[m
 from dataclasses import dataclass[m
 from typing import Union, Tuple, List, Dict, Any, Optional[m
[32m+[m[32mfrom easy_transformer.utils import set_seed_everywhere[m
 import torch[m
 import torch.nn as nn[m
[32m+[m[32mimport random[m
[32m+[m[32mimport numpy as np[m
[32m+[m[32mimport logging[m
 [m
[31m-# %%[m
[32m+[m[32mSUPPORTED_ACTIVATIONS = ['relu', 'gelu', 'silu', 'gelu_new', 'solu_ln', 'gelu_fast'][m
 @dataclass[m
 class EasyTransformerConfig:[m
     """[m
     Configuration class to store the configuration of a EasyTransformer model.[m
[32m+[m
[32m+[m[32m    See further_comments.md for more details on the more complex arguments.[m
[32m+[m
     Args:[m
         d_model (int): The dimensionality of the embeddings.[m
         d_head (int): The dimensionality of each attention head.[m
[31m-        n_heads (int): The number of attention heads.[m
[31m-        d_mlp (int): The dimensionality of the feedforward mlp network.[m
         n_layers (int): The number of attention layers.[m
         n_ctx (int): The maximum sequence length.[m
[31m-        d_vocab (int): The size of the vocabulary.[m
[31m-        act_fn (str): The activation function to use. Always lowercase. Supports ['relu', 'gelu', 'silu', 'glu'm 'gelu_new', 'solu_ln', 'reglu', 'geglu', 'swiglu'].[m
[32m+[m[32m        n_heads (int, *optional*): The number of attention heads. If not specified, will be set to d_model // d_head.[m
[32m+[m[32m        d_mlp (int, *optional*): The dimensionality of the feedforward mlp network. Defaults to 4 * d_model, and in an attn-only model is None.[m
[32m+[m[32m        d_vocab (int): The size of the vocabulary. If not set, will be automatically set[m[41m [m
[32m+[m[32m            from the tokenizer's vocab size.[m
[32m+[m[32m        act_fn (str, *optional"): The activation function to use. Always lowercase.[m[41m [m
[32m+[m[32m            Supports ['relu', 'gelu', 'silu', 'gelu_new', 'solu_ln', 'gelu_fast']. Must be set unless using an attn-only model.[m
         eps (float): The epsilon value to use for layer normalization. Defaults to 1e-5[m
         use_attn_result (bool): whether to explicitly calculate the amount[m
             each head adds to the residual stream (with a hook) and THEN add it[m
[36m@@ -26,12 +34,11 @@[m [mclass EasyTransformerConfig:[m
         use_attn_scale (bool): whether to scale the attention weights by[m
         1/sqrt(d_head)[m
         use_local_attn (bool): whether to use local attention[m
[31m-        model_name (str, *optional*): the name of the model, used to load[m
[32m+[m[32m        model_name (str): the name of the model, used to load[m
             weights from HuggingFace or initialized to "custom" if not passed[m
[31m-        model_type (str, *optional*): the type of the model, used to help load[m
[32m+[m[32m        model_family (str, *optional*): the family of the model, used to help load[m
             weights from HuggingFace or initialized to "custom" if not passed[m
[31m-        full_model_name (str, *optional*): the full name of the model,[m
[31m-            initialized to "custom" if not passed[m
[32m+[m[32m        checkpoint (str, *optional*): the checkpoint to load weights from, if using a checkpointed pretrained model.[m
         tokenizer_name (str, *optional*): the full name of the model, passed into [m
             HuggingFace to access the tokenizer. Only used when passing in custom [m
             config, if loading from pretrained then this is not needed.[m
[36m@@ -47,34 +54,78 @@[m [mclass EasyTransformerConfig:[m
             are None (no normalization), 'LN' (use LayerNorm, including weights & [m
             biases) and 'LNPre' (use LayerNorm, but no weights & biases). Defaults to [m
             None[m
[31m-        gated_act_fn (bool): Whether a gated activation function is being used (geglu, reglu, swiglu). Automatically set from act_fn. Used to determine whether to create an extra MLP weight matrix W_gate[m
[32m+[m[32m        device(str): The device to use for the model. Defaults to 'cuda' if available,[m[41m [m
[32m+[m[32m            else 'cpu[m
[32m+[m[32m        attention_dir (str): Whether to use causal (aka unidirectional aka GPT-2[m[41m [m
[32m+[m[32m            style) or bidirectional attention. Options are 'causal' and 'bidirectional'.[m[41m [m
[32m+[m[32m            Defaults to 'causal'[m
[32m+[m[32m        attn_only (bool): Whether to only use attention layers, no feedforward[m[41m [m
[32m+[m[32m            layers. Defaults to False[m
[32m+[m[32m        seed (int, *optional*): The seed to use for the model. Defaults to 42. Used to set sources of randomness (Python, PyTorch and[m[41m [m
[32m+[m[32m            NumPy) and to initialize weights. If set to None, does nothing.[m
[32m+[m[32m        initializer_range (float): The standard deviation of the normal used to initialise the weights, initialized to 0.8 / sqrt(d_model) .[m
[32m+[m[32m        init_weights (bool): Whether to initialize the weights. Defaults to True. If False, does not initialize weights.[m
[32m+[m[32m        scale_attn_by_inverse_layer_idx (bool): Whether to scale the attention weights by 1/(layer_id[m
[32m+[m[32m            +1), used by Mistral (Stanford) models for numerical stability when training in FP16.[m
[32m+[m[32m            Defaults to False.[m
[32m+[m[32m        positional_embedding_type (str): The positional embedding used. Options are 'standard' (ie[m
[32m+[m[32m            GPT-2 style, absolute, randomly initialized learned positional embeddings, directly added[m
[32m+[m[32m            to the residual stream), 'rotary' (described here: https://blog.eleuther.ai/rotary-embeddings/ ) and 'shortformer' (GPT-2 style absolute &[m[41m [m
[32m+[m[32m            learned, but rather than being added to the residual stream they're only added to the[m[41m [m
[32m+[m[32m            inputs to the keys and the queries (ie key = W_K(res_stream + pos_embed), but values and[m[41m [m
[32m+[m[32m            MLPs don't get any positional info)). Sinusoidal are not currently[m[41m [m
[32m+[m[32m            supported. Defaults to 'standard'.[m
[32m+[m[32m        final_rms (bool): Whether to replace the final normalization (just before the unembed) with RMSNorm (ie no centering or bias, just scaling + weights). Only included because of a dumb bug in my original SoLU code. Defaults to False.[m
[32m+[m[32m        d_vocab_out (int, *optional*): The size of the output vocabulary. If not set, will be equal to d_vocab. Mainly useful for algorithmic tasks where the input and output vocabularies may be different.[m
[32m+[m[32m        parallel_attn_mlp (bool): Whether to parallelize the attention and MLP layers - a weird cursed thing done by GPT-J. Means that mlp_out=MLP(ln1(resid_pre)) and resid_post=resid_pre+attn_out+mlp_out. Defaults to False.[m
[32m+[m[32m        rotary_dim (int): The dimensionality of the rotary embeddings, may be < d_head in which case only the first rotary_dim dimensions of each head are rotated. Defaults to 64, only used is positional_embedding_type=="rotary".[m
[32m+[m[32m        dtype (torch.dtype): The float encoding to use for the model. Defaults to torch.float32.[m
     """[m
 [m
[31m-    d_model: int[m
[31m-    d_head: int[m
[31m-    n_heads: int[m
[31m-    d_mlp: int[m
     n_layers: int[m
[32m+[m[32m    d_model: int[m
     n_ctx: int[m
[31m-    d_vocab: int[m
[31m-    act_fn: str[m
[32m+[m[32m    d_head: int[m
[32m+[m[32m    model_name: str = "custom"[m
[32m+[m[32m    n_heads: Optional[int] = None[m
[32m+[m[32m    d_mlp: Optional[int] = None[m
[32m+[m[32m    act_fn: Optional[str] = None[m
[32m+[m[32m    d_vocab: Optional[int] = None[m
     eps: float = 1e-5[m
     use_attn_result: bool = False[m
     use_attn_scale: bool = True[m
     use_local_attn: bool = False[m
[31m-    model_name: Optional[str] = None[m
[31m-    model_type: Optional[str] = None[m
[32m+[m[32m    model_family: Optional[str] = None[m
     checkpoint: Optional[int] = None[m
[31m-    full_model_name: Optional[str] = None[m
     tokenizer_name: Optional[str] = None[m
     window_size: Optional[int] = None[m
     attn_types: Optional[List] = None[m
     init_mode: str = 'gpt2'[m
     normalization_type: Optional[str] = None[m
[31m-    gated_act_fn: bool = False[m
[32m+[m[32m    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'[m
[32m+[m[32m    attention_dir: str = 'causal'[m
[32m+[m[32m    attn_only: bool = False[m
[32m+[m[32m    seed: int = 42[m
[32m+[m[32m    initializer_range: float = -1.[m
[32m+[m[32m    init_weights: bool = True[m
[32m+[m[32m    scale_attn_by_inverse_layer_idx: bool = False[m
[32m+[m[32m    positional_embedding_type: str = 'standard'[m
[32m+[m[32m    final_rms: bool = False[m
[32m+[m[32m    d_vocab_out: Optional[int] = None[m
[32m+[m[32m    parallel_attn_mlp: bool = False[m
[32m+[m[32m    rotary_dim: int = 64[m
[32m+[m[32m    dtype: torch.dtype = torch.float32[m
 [m
     def __post_init__(self):[m
[31m-        assert self.d_model % self.n_heads == 0, "d_model must be divisible by n_heads"[m
[32m+[m[32m        if self.n_heads is None:[m
[32m+[m[32m            self.n_heads = self.d_model // self.d_head[m
[32m+[m[41m        [m
[32m+[m
[32m+[m[32m        if not self.d_model == (self.n_heads * self.d_head):[m
[32m+[m[32m            logging.warning(f"d_model={self.d_model} is not divisible by n_heads={self.n_heads} * d_head={self.d_head}")[m
[32m+[m
[32m+[m[32m        if self.seed is not None:[m
[32m+[m[32m            set_seed_everywhere(self.seed)[m
         if self.use_local_attn:[m
             assert ([m
                 self.window_size is not None[m
[36m@@ -82,12 +133,18 @@[m [mclass EasyTransformerConfig:[m
             assert ([m
                 self.attn_types is not None[m
             ), "attn_types must be specified for local attention"[m
[31m-        if self.model_name is None:[m
[31m-            self.model_name = "custom"[m
[31m-            self.model_type = "custom"[m
[31m-            self.full_model_name = "custom"[m
[31m-        if self.act_fn in ['reglu', 'geglu', 'swiglu']:[m
[31m-            self.gated_act_fn = True[m
[32m+[m[32m        if not self.attn_only:[m
[32m+[m[32m            if self.d_mlp is None:[m
[32m+[m[32m                # For some reason everyone hard codes in this hyper-parameter![m
[32m+[m[32m                self.d_mlp = self.d_model * 4[m
[32m+[m[32m            assert self.act_fn is not None, "act_fn must be specified for non-attn-only models"[m
[32m+[m[32m            assert self.act_fn in SUPPORTED_ACTIVATIONS, f"act_fn={self.act_fn} must be one of {SUPPORTED_ACTIVATIONS}"[m
[32m+[m[32m        if self.initializer_range < 0:[m
[32m+[m[32m            # Roughly copy the GPT-2 value, but proportional to sqrt(1/d_model)[m
[32m+[m[32m            self.initializer_range = 0.8 / np.sqrt(self.d_model)[m
[32m+[m[41m        [m
[32m+[m[32m        if self.d_vocab_out is None:[m
[32m+[m[32m            self.d_vocab_out = self.d_vocab[m
 [m
     @classmethod[m
     def from_dict(cls, config_dict: Dict[str, Any]):[m
[36m@@ -95,6 +152,3 @@[m [mclass EasyTransformerConfig:[m
         Instantiates a `EasyTransformerConfig` from a Python dictionary of parameters.[m
         """[m
         return cls(**config_dict)[m
[31m-[m
[31m-[m
[31m-# %%[m
[1mdiff --git a/easy_transformer/__init__.py b/easy_transformer/__init__.py[m
[1mindex fc6f257..03791c3 100644[m
[1m--- a/easy_transformer/__init__.py[m
[1m+++ b/easy_transformer/__init__.py[m
[36m@@ -3,5 +3,13 @@[m [mfrom . import hook_points[m
 from . import EasyTransformer[m
 from . import experiments[m
 from . import utils[m
[31m-from . import EasyTransformerConfig[m
[32m+[m[32mfrom . import evals[m
[32m+[m[32mfrom .activation_cache import ActivationCache[m
[32m+[m[32mfrom .caching import ([m
[32m+[m[32m    EasyTransformerKeyValueCache,[m
[32m+[m[32m    EasyTransformerKeyValueCacheEntry,[m
[32m+[m[32m)[m
[32m+[m[32mfrom .EasyTransformerConfig import EasyTransformerConfig[m
[32m+[m[32m# from . import train[m
[32m+[m[32mfrom . import components[m
 from .EasyTransformer import EasyTransformer[m
[1mdiff --git a/easy_transformer/activation_cache.py b/easy_transformer/activation_cache.py[m
[1mnew file mode 100644[m
[1mindex 0000000..a36a4cb[m
[1m--- /dev/null[m
[1m+++ b/easy_transformer/activation_cache.py[m
[36m@@ -0,0 +1,422 @@[m
[32m+[m
[32m+[m[32mimport easy_transformer.utils as utils[m
[32m+[m[32mfrom easy_transformer.utils import Slice, SliceInput[m
[32m+[m[32mimport torch[m
[32m+[m[32mimport einops[m
[32m+[m[32mfrom fancy_einsum import einsum[m
[32m+[m[32mfrom typing import Optional, Union[m
[32m+[m[32mimport re[m
[32m+[m[32mimport numpy as np[m
[32m+[m[32mimport logging[m
[32m+[m
[32m+[m[32mclass ActivationCache:[m
[32m+[m[32m    """[m[41m [m
[32m+[m[32m    A wrapper around a dictionary of cached activations from a model run, with a variety of helper functions. In general, any utility which is specifically about editing/processing activations should be a method here, while any utility which is more general should be a function in utils.py, and any utility which is specifically about model weights should be in EasyTransformer.py or components.py[m
[32m+[m
[32m+[m[32m    WARNING: The biggest footgun and source of bugs in this code will be keeping track of indexes, dimensions, and the numbers of each. There are several kinds of activations:[m
[32m+[m
[32m+[m[32m    Internal attn head vectors: q, k, v, z. Shape [batch, pos, head_index, d_head][m
[32m+[m[32m    Internal attn pattern style results: attn (post softmax), attn_scores (pre-softmax). Shape [batch, head_index, query_pos, key_pos][m
[32m+[m[32m    Attn head results: result. Shape [batch, pos, head_index, d_model][m
[32m+[m[32m    Internal MLP vectors: pre, post, mid (only used for solu_ln - the part between activation + layernorm). Shape [batch, pos, d_mlp][m
[32m+[m[32m    Residual stream vectors: resid_pre, resid_mid, resid_post, attn_out, mlp_out, embed, pos_embed, normalized (output of each LN or LNPre). Shape [batch, pos, d_model][m
[32m+[m[32m    LayerNorm Scale: scale. Shape [batch, pos, 1][m
[32m+[m
[32m+[m[32m    Sometimes the batch dimension will be missing because we applied remove_batch_dim (used when batch_size=1), and we need functions to be robust to that. I THINK I've got everything working, but could easily be wrong![m
[32m+[m[32m    """[m
[32m+[m[32m    def __init__([m
[32m+[m[32m        self,[m[41m [m
[32m+[m[32m        cache_dict: dict,[m[41m [m
[32m+[m[32m        model):[m
[32m+[m[32m        self.cache_dict = cache_dict[m
[32m+[m[32m        self.model = model[m
[32m+[m[32m        self.has_batch_dim = True[m
[32m+[m[32m        self.batch_size = self.cache_dict["hook_embed"].size(0)[m
[32m+[m[32m        self.ctx_size = self.cache_dict["hook_embed"].size(1)[m
[32m+[m[41m        [m
[32m+[m[32m        # Broadcast pos_embed up to batch size, so it has the same shape as all other residual vectors[m
[32m+[m[32m        if "hook_pos_embed" in self.cache_dict:[m
[32m+[m[32m            self.cache_dict["hook_pos_embed"] = einops.repeat(self.cache_dict['hook_pos_embed'], 'pos d_model -> batch pos d_model', batch=self.batch_size)[m
[32m+[m[41m    [m
[32m+[m[32m    def remove_batch_dim(self):[m
[32m+[m[32m        if self.has_batch_dim:[m
[32m+[m[32m            for key in self.cache_dict:[m
[32m+[m[32m                assert self.cache_dict[key].size(0)==1, f"Cannot remove batch dimension from cache with batch size > 1, for key {key} with shape {self.cache_dict[key].shape}"[m
[32m+[m[32m                self.cache_dict[key] = self.cache_dict[key][0][m
[32m+[m[32m            self.has_batch_dim = False[m
[32m+[m[32m        else:[m
[32m+[m[32m            logging.warning("Tried removing batch dimension after already having removed it.")[m
[32m+[m[41m    [m
[32m+[m[32m    def __repr__(self):[m
[32m+[m[32m        return f"ActivationCache with keys {list(self.cache_dict.keys())}"[m
[32m+[m
[32m+[m[32m    def __getitem__(self, key):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        This allows us to treat the activation cache as a dictionary, and do cache["key"] to it. We add bonus functionality to take in shorthand names or tuples - see utils.act_name for the full syntax and examples.[m
[32m+[m
[32m+[m[32m        Dimension order is (act_name, layer_index, layer_type), where layer_type is either "attn" or "mlp" or "ln1" or "ln2" or "ln_final", act_name is the name of the hook (without the hook_ prefix).[m
[32m+[m[32m        """[m
[32m+[m[32m        if key in self.cache_dict:[m
[32m+[m[32m            return self.cache_dict[key][m
[32m+[m[32m        elif type(key)==str:[m
[32m+[m[32m            return self.cache_dict[utils.act_name(key)][m
[32m+[m[32m        else:[m
[32m+[m[32m            if len(key)>1 and key[1] is not None:[m
[32m+[m[32m                if key[1] < 0:[m
[32m+[m[32m                    # Supports negative indexing on the layer dimension[m
[32m+[m[32m                    key = (key[0], self.model.cfg.n_layers+key[1], *key[2:])[m
[32m+[m[32m            return self.cache_dict[utils.act_name(*key)][m
[32m+[m[41m    [m
[32m+[m[32m    def to(self, device, move_model=False):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        Moves the cache to a device - mostly useful for moving it to CPU after model computation finishes to save GPU memory. Matmuls will be much slower on the CPU.[m
[32m+[m
[32m+[m[32m        Note that some methods will break unless the model is also moved to the same device, eg compute_head_results[m
[32m+[m[32m        """[m
[32m+[m[32m        self.cache_dict = {key: value.to(device) for key, value in self.cache_dict.items()}[m
[32m+[m
[32m+[m[32m        if move_model:[m
[32m+[m[32m            self.model.to(device)[m
[32m+[m[41m    [m
[32m+[m[32m    def toggle_autodiff([m
[32m+[m[32m        self,[m[41m [m
[32m+[m[32m        mode: bool=False[m
[32m+[m[32m        ):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        Sets autodiff to mode (defaults to turning it off).[m[41m [m
[32m+[m[32m        WARNING: This is pretty dangerous, since autodiff is global state - this turns off torch's ability to take gradients completely and it's easy to get a bunch of errors if you don't realise what you're doing.[m
[32m+[m
[32m+[m[32m        But autodiff consumes a LOT of GPU memory (since every intermediate activation is cached until all downstream activations are deleted - this means that computing the loss and storing it in a list will keep every activation sticking around!). So often when you're analysing a model's activations, and don't need to do any training, autodiff is more trouble than its worth.[m
[32m+[m
[32m+[m[32m        If you don't want to mess with global state, using torch.inference_mode as a context manager or decorator achieves similar effects :)[m
[32m+[m[32m        """[m
[32m+[m[32m        logging.warning(f"Changed the global state, set autodiff to {mode}")[m
[32m+[m[32m        torch.set_grad_enabled(mode)[m
[32m+[m[41m    [m
[32m+[m[32m    def keys(self):[m
[32m+[m[32m        return self.cache_dict.keys()[m
[32m+[m[32m    def values(self):[m
[32m+[m[32m        return self.cache_dict.values()[m
[32m+[m[32m    def items(self):[m
[32m+[m[32m        return self.cache_dict.items()[m
[32m+[m[41m    [m
[32m+[m[32m    def accumulated_resid([m
[32m+[m[32m        self,[m[41m [m
[32m+[m[32m        layer,[m[41m [m
[32m+[m[32m        incl_mid=False,[m[41m [m
[32m+[m[32m        mlp_input=False,[m
[32m+[m[32m        return_labels=False):[m
[32m+[m[32m        """Returns the accumulated residual stream up to a given layer, ie a stack of previous residual streams up to that layer's input. This can be thought of as a series of partial values of the residual stream, where the model gradually accumulates what it wants.[m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            layer (int): The layer to take components up to - by default includes resid_pre for that layer and excludes resid_mid and resid_post for that layer. layer==n_layers means to return all residual streams, including the final one (ie immediately pre logits). The indices are taken such that this gives the accumulated streams up to the input to layer l[m
[32m+[m[32m            incl_mid (bool, optional): Whether to return resid_mid for all previous layers. Defaults to False.[m
[32m+[m[32m            mlp_input (bool, optional): Whether to include resid_mid for the current layer - essentially giving MLP input rather than Attn input. Defaults to False.[m
[32m+[m[32m            return_labels (bool, optional): Whether to return a list of labels for the residual stream components. Useful for labelling graphs. Defaults to True.[m
[32m+[m
[32m+[m[32m        Returns:[m
[32m+[m[32m            Components: A [num_components, batch_size, pos, d_model] tensor of the accumulated residual streams.[m
[32m+[m[32m            (labels): An optional list of labels for the components.[m
[32m+[m[32m        """[m
[32m+[m[32m        if layer is None or layer==-1:[m
[32m+[m[32m            # Default to the residual stream immediately pre unembed[m
[32m+[m[32m            layer = self.model.cfg.n_layers[m
[32m+[m[32m        labels = [][m
[32m+[m[32m        components = [][m
[32m+[m[32m        for l in range(layer+1):[m
[32m+[m[32m            if l==self.model.cfg.n_layers:[m
[32m+[m[32m                components.append(self[('resid_post', self.model.cfg.n_layers-1)])[m
[32m+[m[32m                labels.append("final_post")[m
[32m+[m[32m                continue[m
[32m+[m[32m            components.append(self[("resid_pre", l)])[m
[32m+[m[32m            labels.append(f"{l}_pre")[m
[32m+[m[32m            if (incl_mid and l<layer) or (mlp_input and l==layer):[m
[32m+[m[32m                components.append(self[("resid_mid", l)])[m
[32m+[m[32m                labels.append(f"{l}_mid")[m
[32m+[m[41m        [m
[32m+[m[32m        components = torch.stack(components, dim=0)[m
[32m+[m[32m        if return_labels:[m
[32m+[m[32m            return components, labels[m[41m [m
[32m+[m[32m        else:[m
[32m+[m[32m            return components[m
[32m+[m[41m    [m
[32m+[m[32m    def decompose_resid([m
[32m+[m[32m        self,[m[41m [m
[32m+[m[32m        layer,[m
[32m+[m[32m        mlp_input=False,[m
[32m+[m[32m        mode="all",[m
[32m+[m[32m        incl_embeds=True,[m
[32m+[m[32m        return_labels=False):[m
[32m+[m[32m        """Decomposes the residual stream input to layer L into a stack of the output of previous layers. The sum of these is the input to layer L (plus embedding and pos embedding). This is useful for attributing model behaviour to different components of the residual stream[m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            layer (int): The layer to take components up to - by default includes resid_pre for that layer and excludes resid_mid and resid_post for that layer. layer==n_layers means to return all layer outputs incl in the final layer, layer==0 means just embed and pos_embed. The indices are taken such that this gives the accumulated streams up to the input to layer l[m
[32m+[m[32m            incl_mid (bool, optional): Whether to return resid_mid for all previous layers. Defaults to False.[m
[32m+[m[32m            mlp_input (bool, optional): Whether to include attn_out for the current layer - essentially giving MLP input rather than Attn input. Defaults to False.[m
[32m+[m[32m            mode (str): Values aare "all", "mlp" or "attn". "all" returns all components, "mlp" returns only the MLP components, and "attn" returns only the attention components. Defaults to "all".[m
[32m+[m[32m            incl_embeds (bool): Whether to include embed & pos_embed[m
[32m+[m[32m            return_labels (bool, optional): Whether to return a list of labels for the residual stream components. Useful for labelling graphs. Defaults to True.[m
[32m+[m
[32m+[m[32m        Returns:[m
[32m+[m[32m            Components: A [num_components, batch_size, pos, d_model] tensor of the accumulated residual streams.[m
[32m+[m[32m            (labels): An optional list of labels for the components.[m
[32m+[m[32m        """[m
[32m+[m[32m        if layer is None or layer==-1:[m
[32m+[m[32m            # Default to the residual stream immediately pre unembed[m
[32m+[m[32m            layer = self.model.cfg.n_layers[m
[32m+[m
[32m+[m[32m        incl_attn = mode != "mlp"[m
[32m+[m[32m        incl_mlp = mode != "attn"[m
[32m+[m[32m        if incl_embeds:[m
[32m+[m[32m            components = [self['embed']][m
[32m+[m[32m            labels = ['embed'][m
[32m+[m[32m            if "hook_pos_embed" in self.cache_dict:[m
[32m+[m[32m                components.append(self['hook_pos_embed'])[m
[32m+[m[32m                labels.append('pos_embed')[m
[32m+[m[32m        else:[m
[32m+[m[32m            components = [][m
[32m+[m[32m            labels = [][m
[32m+[m
[32m+[m[32m        for l in range(layer):[m
[32m+[m[32m            if incl_attn:[m
[32m+[m[32m                components.append(self[("attn_out", l)])[m
[32m+[m[32m                labels.append(f"{l}_attn_out")[m
[32m+[m[32m            if incl_mlp:[m
[32m+[m[32m                components.append(self[("mlp_out", l)])[m
[32m+[m[32m                labels.append(f"{l}_mlp_out")[m
[32m+[m[32m        if mlp_input:[m
[32m+[m[32m            components.append(self[("attn_out", layer)])[m
[32m+[m[32m            labels.append(f"{layer}_attn_out")[m
[32m+[m[32m        components = torch.stack(components, dim=0)[m
[32m+[m[32m        if return_labels:[m
[32m+[m[32m            return components, labels[m[41m [m
[32m+[m[32m        else:[m
[32m+[m[32m            return components[m
[32m+[m[41m    [m
[32m+[m[32m    def compute_head_results([m
[32m+[m[32m        self,[m
[32m+[m[32m    ):[m
[32m+[m[32m        """Computes and caches the results for each attention head, ie the amount contributed to the residual stream from that head. attn_out for a layer is the sum of head results plus b_O. Intended use is to enable use_attn_results when running and caching the model, but this can be useful if you forget.[m
[32m+[m[32m        """[m
[32m+[m[32m        if 'blocks.0.attn.hook_result' in self.cache_dict:[m
[32m+[m[32m            logging.warning("Tried to compute head results when they were already cached")[m
[32m+[m[32m            return[m
[32m+[m[32m        for l in range(self.model.cfg.n_layers):[m
[32m+[m[32m            # Note that we haven't enabled set item on this object so we need to edit the underlying cache_dict directly.[m
[32m+[m[32m            self.cache_dict[f"blocks.{l}.attn.hook_result"] = einsum("... head_index d_head, head_index d_head d_model -> ... head_index d_model", self[("z", l, "attn")], self.model.blocks[l].attn.W_O)[m
[32m+[m[41m    [m
[32m+[m[32m    def stack_head_results([m
[32m+[m[32m        self,[m
[32m+[m[32m        layer: int,[m
[32m+[m[32m        return_labels: bool=False,[m
[32m+[m[32m        incl_remainder: bool=False,[m
[32m+[m[32m        pos_slice: Union[Slice, SliceInput]=None,[m
[32m+[m[32m    ):[m
[32m+[m[32m        """Returns a stack of all head results (ie residual stream contribution) up to layer L. A good way to decompose the outputs of attention layers into attribution by specific heads.[m
[32m+[m
[32m+[m[32m        Assumes that the model has been run with use_attn_results=True[m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            layer (int): Layer index - heads at all layers strictly before this are included. layer must be in [1, n_layers][m
[32m+[m[32m            return_labels (bool, optional): Whether to also return a list of labels of the form "L0H0" for the heads. Defaults to False.[m
[32m+[m[32m            incl_remainder (bool, optional): Whether to return a final term which is "the rest of the residual stream". Defaults to False.[m
[32m+[m[32m            pos_slice (Slice): A slice object to apply to the pos dimension. Defaults to None, do nothing.[m
[32m+[m[32m        """[m
[32m+[m[32m        if not isinstance(pos_slice, Slice):[m
[32m+[m[32m            pos_slice = Slice(pos_slice)[m
[32m+[m[32m        if layer is None or layer==-1:[m
[32m+[m[32m            # Default to the residual stream immediately pre unembed[m
[32m+[m[32m            layer = self.model.cfg.n_layers[m
[32m+[m
[32m+[m[32m        if 'blocks.0.attn.hook_result' not in self.cache_dict:[m
[32m+[m[32m            raise ValueError("Must run model with use_attn_results=True or run cache.compute_head_results to use this method")[m
[32m+[m
[32m+[m[32m        components = [][m
[32m+[m[32m        labels = [][m
[32m+[m[32m        for l in range(layer):[m
[32m+[m[32m            # Note that this has shape batch x pos x head_index x d_model[m
[32m+[m[32m            components.append(pos_slice.apply(self[("result", l, "attn")], dim=-3))[m
[32m+[m[32m            labels.extend([f"L{l}H{h}" for h in range(self.model.cfg.n_heads)])[m
[32m+[m[32m        if components:[m
[32m+[m[32m            components = torch.cat(components, dim=-2)[m
[32m+[m[32m            components = einops.rearrange(components, "... head_index d_model -> head_index ... d_model")[m
[32m+[m[32m            if incl_remainder:[m
[32m+[m[32m                remainder = pos_slice.apply(self[("resid_post", layer-1)], dim=-2) - components.sum(dim=0)[m
[32m+[m[32m                components = torch.cat([components, remainder[None]], dim=0)[m
[32m+[m[32m                labels.append("remainder")[m
[32m+[m[32m        elif incl_remainder:[m
[32m+[m[32m            components = [pos_slice.apply(self[("resid_post", layer-1)], dim=-2)][m
[32m+[m[32m        else:[m
[32m+[m[32m            components = torch.zeros(0, *self["hook_embed"].shape, device=self.model.cfg.device)[m
[32m+[m[41m            [m
[32m+[m[32m        if return_labels:[m
[32m+[m[32m            return components, labels[m
[32m+[m[32m        else:[m
[32m+[m[32m            return components[m
[32m+[m[41m    [m
[32m+[m[32m    def get_neuron_results([m
[32m+[m[32m        self,[m
[32m+[m[32m        layer: int,[m
[32m+[m[32m        neuron_slice: Union[Slice, SliceInput] =None,[m
[32m+[m[32m        pos_slice: Union[Slice, SliceInput] =None,[m
[32m+[m[32m    ):[m
[32m+[m[32m        """Returns the results of for neurons in a specific layer (ie, how much each neuron contributes to the residual stream). Does it for the subset of neurons specified by neuron_slice, defaults to all of them. Does *not* cache these because it's expensive in space and cheap to compute.[m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            layer (int): Layer index[m
[32m+[m[32m            neuron_slice (Slice, optional): Slice of the neuron. Defaults to None.[m
[32m+[m[32m            pos_slice (Slice, optional): Slice of the positions. Defaults to None. See utils.Slice for details.[m
[32m+[m
[32m+[m[32m        Returns:[m
[32m+[m[32m            Tensor: [batch_size, pos, d_mlp, d_model] tensor of the results (d_mlp is the neuron index axis)[m
[32m+[m[32m        """[m
[32m+[m[32m        if type(neuron_slice) is not Slice:[m
[32m+[m[32m            neuron_slice = Slice(neuron_slice)[m
[32m+[m[32m        if type(pos_slice) is not Slice:[m
[32m+[m[32m            pos_slice = Slice(pos_slice)[m
[32m+[m
[32m+[m[32m        neuron_acts = self[("post", layer, "mlp")][m
[32m+[m[32m        W_out = self.model.blocks[layer].mlp.W_out[m
[32m+[m[32m        if pos_slice is not None:[m
[32m+[m[32m            # Note - order is important, as Slice.apply *may* collapse a dimension, so this ensures that position dimension is -2 when we apply position slice[m
[32m+[m[32m            neuron_acts = pos_slice.apply(neuron_acts, dim=-2)[m
[32m+[m[32m        if neuron_slice is not None:[m
[32m+[m[32m            neuron_acts = neuron_slice.apply(neuron_acts, dim=-1)[m
[32m+[m[32m            W_out = neuron_slice.apply(W_out, dim=0)[m
[32m+[m[32m        return neuron_acts[..., None] * W_out[m
[32m+[m
[32m+[m[32m    def stack_neuron_results([m
[32m+[m[32m        self,[m
[32m+[m[32m        layer: int,[m
[32m+[m[32m        pos_slice: Union[Slice, SliceInput] =None,[m
[32m+[m[32m        neuron_slice: Union[Slice, SliceInput] =None,[m
[32m+[m[32m        return_labels: bool=False,[m
[32m+[m[32m        incl_remainder: bool=False,[m
[32m+[m[32m    ):[m
[32m+[m[32m        """Returns a stack of all neuron results (ie residual stream contribution) up to layer L.[m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            layer (int): Layer index - heads at all layers strictly before this are included. layer must be in [1, n_layers][m
[32m+[m[32m            pos_slice (Slice, optional): Slice of the positions. Defaults to None. See utils.Slice for details.[m
[32m+[m[32m            neuron_slice (Slice, optional): Slice of the neurons. Defaults to None. See utils.Slice for details.[m
[32m+[m[32m            return_labels (bool, optional): Whether to also return a list of labels of the form "L0H0" for the heads. Defaults to False.[m
[32m+[m[32m            incl_remainder (bool, optional): Whether to return a final term which is "the rest of the residual stream". Defaults to False.[m
[32m+[m[32m        """[m
[32m+[m
[32m+[m[32m        if layer is None or layer==-1:[m
[32m+[m[32m            # Default to the residual stream immediately pre unembed[m
[32m+[m[32m            layer = self.model.cfg.n_layers[m
[32m+[m
[32m+[m[32m        components = [][m
[32m+[m[32m        labels = [][m
[32m+[m
[32m+[m[32m        if not isinstance(neuron_slice, Slice):[m
[32m+[m[32m            neuron_slice = Slice(neuron_slice)[m
[32m+[m[32m        if not isinstance(pos_slice, Slice):[m
[32m+[m[32m            pos_slice = Slice(pos_slice)[m
[32m+[m
[32m+[m[32m        neuron_labels = neuron_slice.apply(np.arange(self.model.cfg.d_mlp), dim=0)[m
[32m+[m[32m        if type(neuron_labels)==int:[m
[32m+[m[32m            neuron_labels = np.array([neuron_labels])[m
[32m+[m[32m        for l in range(layer):[m
[32m+[m[32m            # Note that this has shape batch x pos x head_index x d_model[m
[32m+[m[32m            components.append(self.get_neuron_results(l, pos_slice=pos_slice, neuron_slice=neuron_slice))[m
[32m+[m[32m            labels.extend([f"L{l}N{h}" for h in neuron_labels])[m
[32m+[m[32m        if components:[m
[32m+[m[32m            components = torch.cat(components, dim=-2)[m
[32m+[m[32m            components = einops.rearrange(components, "... neuron_index d_model -> neuron_index ... d_model")[m
[32m+[m
[32m+[m[32m            if incl_remainder:[m
[32m+[m[32m                remainder = self[("resid_post", layer-1)] - components.sum(dim=0)[m
[32m+[m[32m                components = torch.cat([components, remainder[None]], dim=0)[m
[32m+[m[32m                labels.append("remainder")[m
[32m+[m[32m        elif incl_remainder:[m
[32m+[m[32m            components = [pos_slice.apply(self[("resid_post", layer-1)], dim=-2)][m
[32m+[m[32m        else:[m
[32m+[m[32m            components = torch.zeros(0, *self["hook_embed"].shape, device=self.model.cfg.device)[m
[32m+[m[32m        if return_labels:[m
[32m+[m[32m            return components, labels[m
[32m+[m[32m        else:[m
[32m+[m[32m            return components[m
[32m+[m[41m    [m
[32m+[m[32m    def apply_ln_to_stack([m
[32m+[m[32m        self,[m
[32m+[m[32m        residual_stack: torch.Tensor,[m
[32m+[m[32m        layer: Optional[int]=None,[m
[32m+[m[32m        mlp_input: bool=False,[m
[32m+[m[32m        pos_slice: Union[Slice, SliceInput]  = None,[m
[32m+[m[32m    ):[m
[32m+[m[32m        """Takes a stack of components of the residual stream (eg outputs of decompose_resid or accumulated_resid), treats them as the input to a specific layer, and applies the layer norm scaling of that layer to them, using the cached scale factors.[m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            residual_stack (torch.Tensor): A tensor, whose final dimension is d_model. The other trailing dimensions are assumed to be the same as the stored hook_scale - which may or may not include batch or position dimensions.[m
[32m+[m[32m            layer (int): The layer we're taking the input to. In [0, n_layers], n_layers means the unembed. None maps to the n_layers case, ie the unembed.[m
[32m+[m[32m            mlp_input (bool, optional): Whether the input is to the MLP or attn (ie ln2 vs ln1). Defaults to False, ie ln1. If layer==n_layers, must be False, and we use ln_final[m
[32m+[m[32m            pos_slice: The slice to take of positions, if residual_stack is not over the full context, None means do nothing. See utils.Slice for details. Defaults to None.[m
[32m+[m[32m        """[m
[32m+[m[32m        # First, center[m
[32m+[m[32m        if not isinstance(pos_slice, Slice):[m
[32m+[m[32m            pos_slice = Slice(pos_slice)[m
[32m+[m[32m        if layer is None or layer==-1:[m
[32m+[m[32m            # Default to the residual stream immediately pre unembed[m
[32m+[m[32m            layer = self.model.cfg.n_layers[m
[32m+[m[32m        residual_stack = residual_stack - residual_stack.mean(dim=-1, keepdim=True)[m
[32m+[m
[32m+[m[32m        if layer==self.model.cfg.n_layers or layer is None:[m
[32m+[m[32m            scale = self['ln_final.hook_scale'][m
[32m+[m[32m        else:[m
[32m+[m[32m            hook_name = f"blocks.{layer}.ln{2 if mlp_input else 1}.hook_scale"[m
[32m+[m[32m            scale = self[hook_name][m
[32m+[m
[32m+[m[32m        # The shape of scale is [batch, position, 1] - final dimension is a dummy thing to get broadcoasting to work nicely.[m
[32m+[m[32m        scale = pos_slice.apply(scale, dim=-2)[m
[32m+[m[41m        [m
[32m+[m[32m        return residual_stack / scale[m
[32m+[m
[32m+[m[32m    def get_full_resid_decomposition([m
[32m+[m[32m        self,[m
[32m+[m[32m        layer: Optional[int]=None,[m
[32m+[m[32m        mlp_input=False,[m
[32m+[m[32m        apply_ln=True,[m
[32m+[m[32m        pos_slice: Union[Slice, SliceInput]  = None,[m
[32m+[m[32m        return_labels=False,[m
[32m+[m[32m    ):[m
[32m+[m[32m        """Returns the full decomposition of the residual stream into embed, pos_embed, each head result, each neuron result, and the accumulated biases. We break down the residual stream that is input into some layer.[m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            layer (int): The layer we're inputting into. layer is in [0, n_layers], if layer==n_layers (or None) we're inputting into the unembed (the entire stream), if layer==0 then it's just embed and pos_embed[m
[32m+[m[32m            mlp_input (bool, optional): Are we inputting to the MLP in that layer or the attn? Must be False for final layer, since that's the unembed. Defaults to False.[m
[32m+[m[32m            apply_ln (bool, optional): Whether to apply LayerNorm to the stack. Defaults to True.[m
[32m+[m[32m            pos_slice (Slice, optional): Slice of the positions to take. Defaults to None. See utils.Slice for details.[m
[32m+[m[32m            return_labels (bool): Whether to return the labels. Defaults to False.[m
[32m+[m[32m        """[m
[32m+[m[32m        if layer is None or layer==-1:[m
[32m+[m[32m            # Default to the residual stream immediately pre unembed[m
[32m+[m[32m            layer = self.model.cfg.n_layers[m
[32m+[m
[32m+[m[32m        if not isinstance(pos_slice, Slice):[m
[32m+[m[32m            pos_slice = Slice(pos_slice)[m
[32m+[m[32m        head_stack, head_labels = self.stack_head_results(layer + (1 if mlp_input else 0), pos_slice=pos_slice, return_labels=True)[m
[32m+[m[32m        neuron_stack, neuron_labels = self.stack_neuron_results(layer, pos_slice=pos_slice, return_labels=True)[m
[32m+[m[32m        bias = self.model.accumulated_bias(layer, mlp_input)[m
[32m+[m[32m        if self.has_batch_dim:[m
[32m+[m[32m            bias = einops.repeat(bias, "d_model -> batch ctx d_model", batch=self.batch_size, ctx=self.ctx_size)[m
[32m+[m[32m        else:[m
[32m+[m[32m            bias = einops.repeat(bias, "d_model -> ctx d_model", ctx=self.ctx_size)[m
[32m+[m
[32m+[m[32m        labels = head_labels + neuron_labels + ["embed", "pos_embed", "bias"][m
[32m+[m[32m        embed = pos_slice.apply(self["embed"], -2)[None][m
[32m+[m[32m        pos_embed = pos_slice.apply(self["pos_embed"], -2)[None][m
[32m+[m[32m        bias = pos_slice.apply(bias, -2)[None][m
[32m+[m[32m        l = [head_stack, neuron_stack, embed, pos_embed, bias][m
[32m+[m[32m        for i in l: print(i.shape)[m
[32m+[m[32m        residual_stack = torch.cat(l, dim=0)[m
[32m+[m
[32m+[m[32m        if apply_ln:[m
[32m+[m[32m            residual_stack = self.apply_ln_to_stack(residual_stack, layer, pos_slice=pos_slice)[m
[32m+[m
[32m+[m[32m        if return_labels:[m
[32m+[m[32m            return residual_stack, labels[m
[32m+[m[32m        else:[m
[32m+[m[32m            return residual_stack[m
[32m+[m
[1mdiff --git a/easy_transformer/caching.py b/easy_transformer/caching.py[m
[1mnew file mode 100644[m
[1mindex 0000000..e921f93[m
[1m--- /dev/null[m
[1m+++ b/easy_transformer/caching.py[m
[36m@@ -0,0 +1,64 @@[m
[32m+[m[32mimport torch[m
[32m+[m[32mimport torch.nn as nn[m
[32m+[m[32mfrom dataclasses import dataclass[m
[32m+[m[32mfrom typing import Union, Tuple, List, Dict, Any, Optional[m
[32m+[m[32mfrom easy_transformer.EasyTransformerConfig import EasyTransformerConfig[m
[32m+[m
[32m+[m
[32m+[m[32m@dataclass[m
[32m+[m[32mclass EasyTransformerKeyValueCacheEntry:[m
[32m+[m[32m    past_keys: torch.Tensor[m
[32m+[m[32m    past_values: torch.Tensor[m
[32m+[m
[32m+[m[32m    @classmethod[m
[32m+[m[32m    def init_cache_entry([m
[32m+[m[32m        cls,[m
[32m+[m[32m        cfg: EasyTransformerConfig,[m
[32m+[m[32m        device: torch.device,[m
[32m+[m[32m        batch_size: int = 1,[m
[32m+[m[32m    ):[m
[32m+[m[32m        return cls([m
[32m+[m[32m            past_keys=torch.empty([m
[32m+[m[32m                (batch_size, 0, cfg.n_heads, cfg.d_head), device=device[m
[32m+[m[32m            ),[m
[32m+[m[32m            past_values=torch.empty([m
[32m+[m[32m                (batch_size, 0, cfg.n_heads, cfg.d_head), device=device[m
[32m+[m[32m            ),[m
[32m+[m[32m        )[m
[32m+[m
[32m+[m[32m    def append(self, new_keys: torch.Tensor, new_values: torch.Tensor):[m
[32m+[m[32m        updated_keys = torch.cat([self.past_keys, new_keys], dim=1)[m
[32m+[m[32m        updated_values = torch.cat([self.past_values, new_values], dim=1)[m
[32m+[m[32m        self.past_keys = updated_keys[m
[32m+[m[32m        self.past_values = updated_values[m
[32m+[m[32m        return updated_keys, updated_values[m
[32m+[m
[32m+[m
[32m+[m[32m@dataclass[m
[32m+[m[32mclass EasyTransformerKeyValueCache:[m
[32m+[m[32m    """[m
[32m+[m[32m    A cache for storing past keys and values for the Transformer. This is important for generating text - we can cache a lot of past computation and avoid repeating ourselves![m
[32m+[m
[32m+[m[32m    This cache is a list of EasyTransformerKeyValueCacheEntry objects, one for each layer in the Transformer. Each object stores a [batch, pos_so_far, n_heads, d_head] tensor for both keys and values, and each entry has an append method to add a single new key and value.[m
[32m+[m
[32m+[m[32m    Generation is assumed to be done by initializing with some prompt and then continuing iteratively one token at a time. So append only works for adding a single token's worth of keys and values, and but the cache can be initialized with many.[m
[32m+[m
[32m+[m[32m    """[m
[32m+[m
[32m+[m[32m    entries: List[EasyTransformerKeyValueCacheEntry][m
[32m+[m
[32m+[m[32m    @classmethod[m
[32m+[m[32m    def init_cache([m
[32m+[m[32m        cls, cfg: EasyTransformerConfig, device: torch.device, batch_size: int = 1[m
[32m+[m[32m    ):[m
[32m+[m[32m        return cls([m
[32m+[m[32m            entries=[[m
[32m+[m[32m                EasyTransformerKeyValueCacheEntry.init_cache_entry([m
[32m+[m[32m                    cfg, device, batch_size[m
[32m+[m[32m                )[m
[32m+[m[32m                for _ in range(cfg.n_layers)[m
[32m+[m[32m            ][m
[32m+[m[32m        )[m
[32m+[m
[32m+[m[32m    def __getitem__(self, idx):[m
[32m+[m[32m        return self.entries[idx][m
[1mdiff --git a/easy_transformer/components.py b/easy_transformer/components.py[m
[1mnew file mode 100644[m
[1mindex 0000000..33bfb5d[m
[1m--- /dev/null[m
[1m+++ b/easy_transformer/components.py[m
[36m@@ -0,0 +1,592 @@[m
[32m+[m[32mfrom mimetypes import init[m
[32m+[m[32mfrom typing import Callable, Union, List, Tuple, Dict, Optional[m
[32m+[m[32mimport torch[m
[32m+[m[32mimport torch.nn as nn[m
[32m+[m[32mimport torch.nn.functional as F[m
[32m+[m[32mimport numpy as np[m
[32m+[m[32mimport einops[m
[32m+[m[32mimport logging[m
[32m+[m
[32m+[m[32mfrom functools import *[m
[32m+[m
[32m+[m[32mfrom easy_transformer.hook_points import HookPoint[m
[32m+[m[32mfrom easy_transformer.utils import ([m
[32m+[m[32m    gelu_new,[m
[32m+[m[32m    solu,[m
[32m+[m[32m    gelu_fast[m
[32m+[m[32m)[m
[32m+[m[32mfrom easy_transformer.EasyTransformerConfig import EasyTransformerConfig[m
[32m+[m
[32m+[m[32mfrom fancy_einsum import einsum[m
[32m+[m
[32m+[m[32mfrom easy_transformer.caching import ([m
[32m+[m[32m    EasyTransformerKeyValueCache,[m
[32m+[m[32m    EasyTransformerKeyValueCacheEntry,[m
[32m+[m[32m)[m
[32m+[m
[32m+[m[32m# Embed & Unembed[m
[32m+[m[32mclass Embed(nn.Module):[m
[32m+[m[32m    def __init__(self, cfg: Union[Dict, EasyTransformerConfig]):[m
[32m+[m[32m        super().__init__()[m
[32m+[m[32m        if isinstance(cfg, Dict):[m
[32m+[m[32m            cfg = EasyTransformerConfig.from_dict(cfg)[m
[32m+[m[32m        self.cfg = cfg[m
[32m+[m[32m        self.W_E = nn.Parameter(torch.empty(self.cfg.d_vocab, self.cfg.d_model))[m
[32m+[m
[32m+[m[32m    def forward(self, tokens):[m
[32m+[m[32m        # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d][m
[32m+[m[32m        # B acts as a tensor of indices into the second dimension (so >=0 and <b)[m
[32m+[m[32m        return self.W_E[tokens, :] # Shape [batch pos d_model][m
[32m+[m
[32m+[m
[32m+[m[32mclass Unembed(nn.Module):[m
[32m+[m[32m    def __init__(self, cfg: Union[Dict, EasyTransformerConfig]):[m
[32m+[m[41m        [m
[32m+[m[32m        super().__init__()[m
[32m+[m[32m        if isinstance(cfg, Dict):[m
[32m+[m[32m            cfg = EasyTransformerConfig.from_dict(cfg)[m
[32m+[m[32m        self.cfg = cfg[m
[32m+[m[32m        # Note that there's a separate variable for d_vocab_out and d_vocab (the input vocab size). For language tasks these are always the same, but for algorithmic tasks we may want them to be different.[m
[32m+[m[32m        self.W_U = nn.Parameter(torch.empty(self.cfg.d_model, self.cfg.d_vocab_out))[m
[32m+[m[32m        self.b_U = nn.Parameter(torch.zeros(self.cfg.d_vocab_out))[m
[32m+[m
[32m+[m[32m    def forward(self, residual):[m
[32m+[m[32m        return ([m
[32m+[m[32m            einsum("batch pos d_model, d_model vocab -> batch pos vocab",[m[41m [m
[32m+[m[32m                   residual, self.W_U) + self.b_U[m
[32m+[m[32m        )  # [batch, pos, d_vocab][m
[32m+[m
[32m+[m
[32m+[m[32m# Positional Embeddings[m
[32m+[m[32mclass PosEmbed(nn.Module):[m
[32m+[m[32m    def __init__(self, cfg: Union[Dict, EasyTransformerConfig]):[m
[32m+[m[32m        super().__init__()[m
[32m+[m[32m        if isinstance(cfg, Dict):[m
[32m+[m[32m            cfg = EasyTransformerConfig.from_dict(cfg)[m
[32m+[m[32m        self.cfg = cfg[m
[32m+[m[32m        self.W_pos = nn.Parameter(torch.empty(self.cfg.n_ctx, self.cfg.d_model))[m
[32m+[m
[32m+[m[32m    def forward([m
[32m+[m[32m        self,[m[41m [m
[32m+[m[32m        tokens: torch.Tensor,[m[41m [m
[32m+[m[32m        past_kv_pos_offset: int = 0):[m
[32m+[m[32m        """Tokens have shape [batch, pos][m
[32m+[m[32m        past_kv_pos_offset is the length of tokens in the past_kv_cache (if used, defaults to zero if unused)[m
[32m+[m[32m        Output shape [pos, d_model] - will be broadcast along batch dim"""[m
[32m+[m
[32m+[m[32m        tokens_length = tokens.size(-1)[m
[32m+[m[32m        return self.W_pos[past_kv_pos_offset:tokens_length + past_kv_pos_offset, :]  # [pos, d_model][m
[32m+[m
[32m+[m
[32m+[m[32m# LayerNormPre[m
[32m+[m[32m# I fold the LayerNorm weights and biases into later weights and biases.[m
[32m+[m[32m# This is just the 'center and normalise' part of LayerNorm[m
[32m+[m[32m# Centering is equivalent to just deleting one direction of residual space,[m
[32m+[m[32m# and is equivalent to centering the weight matrices of everything writing to the residual stream[m
[32m+[m[32m# Normalising is a funkier non-linear operation, that projects the residual stream onto the unit hypersphere[m
[32m+[m[32mclass LayerNormPre(nn.Module):[m
[32m+[m[32m    def __init__(self, cfg: Union[Dict, EasyTransformerConfig]):[m
[32m+[m[32m        """LayerNormPre - the 'center and normalise' part of LayerNorm. Length is[m
[32m+[m[32m        normally d_model, but is d_mlp for softmax. Not needed as a parameter. This[m
[32m+[m[32m        should only be used in inference mode after folding in LayerNorm weights"""[m
[32m+[m[32m        super().__init__()[m
[32m+[m[32m        if isinstance(cfg, Dict):[m
[32m+[m[32m            cfg = EasyTransformerConfig.from_dict(cfg)[m
[32m+[m[32m        self.cfg = cfg[m
[32m+[m[32m        self.eps = self.cfg.eps[m
[32m+[m
[32m+[m[32m        # Adds a hook point for the normalisation scale factor[m
[32m+[m[32m        self.hook_scale = HookPoint()  # [batch, pos][m
[32m+[m[32m        # Hook Normalized captures LN output - here it's a vector with std 1 and mean 0[m
[32m+[m[32m        self.hook_normalized = HookPoint()  # [batch, pos, length][m
[32m+[m
[32m+[m[32m    def forward(self, x):[m
[32m+[m[32m        x = x - x.mean(axis=-1, keepdim=True)  # [batch, pos, length][m
[32m+[m[32m        scale = self.hook_scale([m
[32m+[m[32m            ([m
[32m+[m[32m                x.pow(2).mean(-1, keepdim=True)[m
[32m+[m[32m                + self.eps[m
[32m+[m[32m            ).sqrt()[m
[32m+[m[32m        )  # [batch, pos, 1][m
[32m+[m[32m        return self.hook_normalized(x / scale)  # [batch, pos, length][m
[32m+[m
[32m+[m
[32m+[m[32mclass LayerNorm(nn.Module):[m
[32m+[m[32m    def __init__([m
[32m+[m[32m        self, cfg: Union[Dict, EasyTransformerConfig], length: Optional[int] = None[m
[32m+[m[32m    ):[m
[32m+[m
[32m+[m[32m        """[m
[32m+[m[32m        LayerNorm with optional length parameter[m
[32m+[m
[32m+[m[32m        length (Optional[int]): If the dimension of the LayerNorm. If not provided, assumed to be d_model[m
[32m+[m[32m        """[m
[32m+[m[32m        super().__init__()[m
[32m+[m[32m        if isinstance(cfg, Dict):[m
[32m+[m[32m            cfg = EasyTransformerConfig.from_dict(cfg)[m
[32m+[m[32m        self.cfg = cfg[m
[32m+[m[32m        self.eps = self.cfg.eps[m
[32m+[m[32m        if length is None:[m
[32m+[m[32m            self.length = self.cfg.d_model[m
[32m+[m[32m        else:[m
[32m+[m[32m            self.length = length[m
[32m+[m
[32m+[m[32m        self.w = nn.Parameter(torch.ones(self.length))[m
[32m+[m[32m        self.b = nn.Parameter(torch.zeros(self.length))[m
[32m+[m
[32m+[m[32m        # Adds a hook point for the normalisation scale factor[m
[32m+[m[32m        self.hook_scale = HookPoint()  # [batch, pos, 1][m
[32m+[m[32m        # Hook_normalized is on the LN output[m
[32m+[m[32m        self.hook_normalized = HookPoint()  # [batch, pos, length][m
[32m+[m
[32m+[m[32m    def forward(self, x):[m
[32m+[m[32m        x = x - x.mean(axis=-1, keepdim=True)  # [batch, pos, length][m
[32m+[m[32m        scale = self.hook_scale([m
[32m+[m[32m            ([m
[32m+[m[32m                x.pow(2).mean(-1, keepdim=True)[m
[32m+[m[32m                + self.eps[m
[32m+[m[32m            ).sqrt()[m
[32m+[m[32m        )  # [batch, pos, 1][m
[32m+[m[32m        x = (x / scale)  # [batch, pos, length][m
[32m+[m[32m        return self.hook_normalized(x * self.w + self.b)[m
[32m+[m
[32m+[m[32mclass RMSNormPre(nn.Module):[m
[32m+[m[32m    def __init__(self, cfg: Union[Dict, EasyTransformerConfig]):[m
[32m+[m[32m        """RMSNormPre - LayerNormPre without the centering and bias (RMS = Root Mean Square)"""[m
[32m+[m[32m        super().__init__()[m
[32m+[m[32m        if isinstance(cfg, Dict):[m
[32m+[m[32m            cfg = EasyTransformerConfig.from_dict(cfg)[m
[32m+[m[32m        self.cfg = cfg[m
[32m+[m[32m        self.eps = self.cfg.eps[m
[32m+[m
[32m+[m[32m        # Adds a hook point for the normalisation scale factor[m
[32m+[m[32m        self.hook_scale = HookPoint()  # [batch, pos][m
[32m+[m[32m        self.hook_normalized = HookPoint()  # [batch, pos, length][m
[32m+[m
[32m+[m[32m    def forward(self, x):[m
[32m+[m[32m        scale = self.hook_scale([m
[32m+[m[32m            ([m
[32m+[m[32m                x.pow(2).mean(-1, keepdim=True)[m
[32m+[m[32m                + self.eps[m
[32m+[m[32m            ).sqrt()[m
[32m+[m[32m        )  # [batch, pos, 1][m
[32m+[m[32m        return self.hook_normalized(x / scale)  # [batch, pos, length][m
[32m+[m
[32m+[m
[32m+[m[32mclass RMSNorm(nn.Module):[m
[32m+[m[32m    def __init__([m
[32m+[m[32m        self, cfg: Union[Dict, EasyTransformerConfig], length: Optional[int] = None[m
[32m+[m[32m    ):[m
[32m+[m
[32m+[m[32m        """[m
[32m+[m[32m        RMSNorm - LayerNorm without the centering and bias (RMS = Root Mean Square)[m
[32m+[m
[32m+[m[32m        length (Optional[int]): If the dimension of the RMSNorm. If not provided, assumed to be d_model[m
[32m+[m[32m        """[m
[32m+[m[32m        super().__init__()[m
[32m+[m[32m        if isinstance(cfg, Dict):[m
[32m+[m[32m            cfg = EasyTransformerConfig.from_dict(cfg)[m
[32m+[m[32m        self.cfg = cfg[m
[32m+[m[32m        self.eps = self.cfg.eps[m
[32m+[m[32m        if length is None:[m
[32m+[m[32m            self.length = self.cfg.d_model[m
[32m+[m[32m        else:[m
[32m+[m[32m            self.length = length[m
[32m+[m
[32m+[m[32m        self.w = nn.Parameter(torch.ones(self.length))[m
[32m+[m
[32m+[m[32m        # Adds a hook point for the normalisation scale factor[m
[32m+[m[32m        self.hook_scale = HookPoint()  # [batch, pos, 1][m
[32m+[m[32m        self.hook_normalized = HookPoint()  # [batch, pos, length][m
[32m+[m
[32m+[m[32m    def forward(self, x):[m
[32m+[m[32m        scale = self.hook_scale([m
[32m+[m[32m            ([m
[32m+[m[32m                x.pow(2).mean(-1, keepdim=True)[m
[32m+[m[32m                + self.eps[m
[32m+[m[32m            ).sqrt()[m
[32m+[m[32m        )  # [batch, pos, 1][m
[32m+[m[32m        x = self.hook_normalized(x / scale)  # [batch, pos, length][m
[32m+[m[32m        return x * self.w[m
[32m+[m
[32m+[m
[32m+[m[32m# Attention[m
[32m+[m[32mclass Attention(nn.Module):[m
[32m+[m[32m    def __init__(self, cfg: Union[Dict, EasyTransformerConfig], attn_type="global", layer_id=None):[m
[32m+[m[32m        """Attention Block - params have shape [head_index, d_model, d_head] (or [head_index, d_head, d_model] for W_O) and multiply on the right. attn_scores refers to query key dot product immediately before attention softmax[m
[32m+[m
[32m+[m[32m        Convention: All attention pattern-style matrices have shape [batch, head_index, query_pos, key_pos][m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            cfg (Union[Dict, EasyTransformerConfig]): Config[m
[32m+[m[32m            attn_type (str, optional): "global" or "local", used by GPT-Neo. Local attention means the model can only attend back cfg.window_size tokens (here, 256). Not used by any other model at the moment. Defaults to "global".[m
[32m+[m[32m            layer_id (int, optional): The index of the current layer. Used by the Mistal models (labelled here as stanford-gpt2) to scale down attention scores pre softmax for numerical stability reasons by 1/(layer_id+1). Defaults to None.[m
[32m+[m[32m        """[m
[32m+[m[32m        super().__init__()[m
[32m+[m[32m        if isinstance(cfg, Dict):[m
[32m+[m[32m            cfg = EasyTransformerConfig.from_dict(cfg)[m
[32m+[m[32m        self.cfg = cfg[m
[32m+[m[32m        self.W_Q = nn.Parameter([m
[32m+[m[32m            torch.empty(self.cfg.n_heads, self.cfg.d_model, self.cfg.d_head)[m
[32m+[m[32m        )[m
[32m+[m[32m        self.W_K = nn.Parameter([m
[32m+[m[32m            torch.empty(self.cfg.n_heads, self.cfg.d_model, self.cfg.d_head)[m
[32m+[m[32m        )[m
[32m+[m[32m        self.W_V = nn.Parameter([m
[32m+[m[32m            torch.empty(self.cfg.n_heads, self.cfg.d_model, self.cfg.d_head)[m
[32m+[m[32m        )[m
[32m+[m[32m        self.W_O = nn.Parameter([m
[32m+[m[32m            torch.empty(self.cfg.n_heads, self.cfg.d_head, self.cfg.d_model)[m
[32m+[m[32m        )[m
[32m+[m[32m        self.b_Q = nn.Parameter(torch.zeros(self.cfg.n_heads, self.cfg.d_head))[m
[32m+[m[32m        self.b_K = nn.Parameter(torch.zeros(self.cfg.n_heads, self.cfg.d_head))[m
[32m+[m[32m        self.b_V = nn.Parameter(torch.zeros(self.cfg.n_heads, self.cfg.d_head))[m
[32m+[m[32m        self.b_O = nn.Parameter(torch.zeros(self.cfg.d_model))[m
[32m+[m
[32m+[m[32m        self.attn_type = attn_type[m
[32m+[m[32m        # Create a max_ctx x max_ctx mask, with True iff that query position[m
[32m+[m[32m        # can attend to that key position (query is first axis, key is second axis)[m
[32m+[m[32m        causal_mask = torch.tril(torch.ones((self.cfg.n_ctx, self.cfg.n_ctx)).bool())[m
[32m+[m[32m        if self.attn_type == "global":[m
[32m+[m[32m            # For global attention, this is a lower triangular matrix - key <= query[m
[32m+[m[32m            self.register_buffer("mask", causal_mask)[m
[32m+[m[32m        elif self.attn_type == "local":[m
[32m+[m[32m            # For local, this is banded, query - window_size < key <= query[m
[32m+[m[32m            assert isinstance(self.cfg.window_size, int)[m
[32m+[m[32m            self.register_buffer([m
[32m+[m[32m                "mask", torch.triu(causal_mask, 1 - self.cfg.window_size)[m
[32m+[m[32m            )[m
[32m+[m[32m        else:[m
[32m+[m[32m            raise ValueError(f"Invalid attention type: {self.attn_type}")[m
[32m+[m
[32m+[m[32m        self.register_buffer("IGNORE", torch.tensor(-1e5))[m
[32m+[m
[32m+[m[32m        self.layer_id = layer_id[m
[32m+[m
[32m+[m[32m        # attn_scale is a constant that we divide the attention scores by pre-softmax. I'm not entirely sure why it matters, but it's probably a mix of softmax not being scale invariant and numerical stability?[m
[32m+[m[32m        if self.cfg.use_attn_scale:[m
[32m+[m[32m            self.attn_scale = np.sqrt(self.cfg.d_head)[m
[32m+[m[32m        else:[m
[32m+[m[32m            self.attn_scale = 1.0[m
[32m+[m[32m        if self.cfg.scale_attn_by_inverse_layer_idx:[m
[32m+[m[32m            self.attn_scale *= (self.layer_id + 1)[m
[32m+[m
[32m+[m[32m        self.hook_k = HookPoint()  # [batch, pos, head_index, d_head][m
[32m+[m[32m        self.hook_q = HookPoint()  # [batch, pos, head_index, d_head][m
[32m+[m[32m        self.hook_v = HookPoint()  # [batch, pos, head_index, d_head][m
[32m+[m[32m        self.hook_z = HookPoint()  # [batch, pos, head_index, d_head][m
[32m+[m[32m        self.hook_attn_scores = HookPoint()  # [batch, head_index, query_pos, key_pos][m
[32m+[m[32m        self.hook_attn = HookPoint()  # [batch, head_index, query_pos, key_pos][m
[32m+[m[32m        self.hook_result = HookPoint()  # [batch, head_index, head_index, d_model][m
[32m+[m
[32m+[m[32m        # See EasyTransformerConfig for more details.[m
[32m+[m[32m        if self.cfg.positional_embedding_type == "shortformer":[m
[32m+[m[32m            # This tracks the input to the keys and queries, which is resid_pre + pos_embeds[m
[32m+[m[32m            self.hook_attn_input = HookPoint() # [batch, pos, d_model][m
[32m+[m[32m        elif self.cfg.positional_embedding_type == "rotary":[m
[32m+[m[32m            # Applies a rotation to each two-element chunk of keys and queries pre dot producting to bake in relative position. See EasyTransformerConfig for details[m
[32m+[m[32m            self.hook_rot_k = HookPoint()[m
[32m+[m[32m            self.hook_rot_q = HookPoint()[m
[32m+[m[32m            sin, cos = self.calculate_sin_cos_rotary(self.cfg.rotary_dim, self.cfg.n_ctx)[m
[32m+[m[32m            self.register_buffer("rotary_sin", sin)[m
[32m+[m[32m            self.register_buffer("rotary_cos", cos)[m
[32m+[m[41m        [m
[32m+[m
[32m+[m[32m    def forward(self,[m[41m [m
[32m+[m[32m            resid_pre: torch.Tensor,[m[41m [m
[32m+[m[32m            shortformer_pos_embed: Optional[torch.Tensor] = None,[m
[32m+[m[32m            past_kv_cache_entry: Optional[EasyTransformerKeyValueCacheEntry] = None[m
[32m+[m[32m        ):[m
[32m+[m[32m        """[m
[32m+[m[32m        shortformer_pos_embed is only used if self.cfg.positional_embedding_type == "shortformer", else defaults to None and is irrelevant. See EasyTransformerConfig for more details[m
[32m+[m[32m        past_kv_cache_entry is an optional entry of past keys and values for this layer, only relevant if generating text. Defaults to None[m
[32m+[m
[32m+[m[32m        """[m
[32m+[m[32m        if self.cfg.positional_embedding_type in ["standard", "rotary"]:[m
[32m+[m[32m            # Normal attention[m
[32m+[m[32m            q = self.hook_q([m
[32m+[m[32m                einsum("batch pos d_model, head_index d_model d_head \[m
[32m+[m[32m                    -> batch pos head_index d_head",[m[41m [m
[32m+[m[32m                            resid_pre, self.W_Q) + self.b_Q[m
[32m+[m[32m            )  # [batch, pos, head_index, d_head][m
[32m+[m[32m            k = self.hook_k([m
[32m+[m[32m                einsum("batch pos d_model, head_index d_model d_head \[m
[32m+[m[32m                    -> batch pos head_index d_head",[m[41m [m
[32m+[m[32m                            resid_pre, self.W_K) + self.b_K[m
[32m+[m[32m            )  # [batch, pos, head_index, d_head][m
[32m+[m[32m        elif self.cfg.positional_embedding_type == "shortformer":[m
[32m+[m[32m            # Weird shortformer attention see EasyTransformerConfig for details[m
[32m+[m[32m            q, k = self.shortformer_calculate_qk(resid_pre, shortformer_pos_embed)[m
[32m+[m[32m        v = self.hook_v([m
[32m+[m[32m            einsum("batch pos d_model, head_index d_model d_head \[m
[32m+[m[32m                -> batch pos head_index d_head",[m[41m [m
[32m+[m[32m                        resid_pre, self.W_V) + self.b_V[m
[32m+[m[32m        )  # [batch, pos, head_index, d_head][m
[32m+[m
[32m+[m[41m        [m
[32m+[m[32m        if past_kv_cache_entry is not None:[m
[32m+[m[32m            # Appends the new keys and values to the cached values, and automatically updates the cache[m
[32m+[m[32m            kv_cache_pos_offset = past_kv_cache_entry.past_keys.size(1)[m
[32m+[m[32m            k, v = past_kv_cache_entry.append(k, v)[m
[32m+[m[32m        else:[m
[32m+[m[32m            # Not using a cache[m
[32m+[m[32m            kv_cache_pos_offset = 0[m
[32m+[m[41m        [m
[32m+[m[32m        if self.cfg.positional_embedding_type == "rotary":[m
[32m+[m[32m            q, k = self.rotary_rotate_qk(q, k, kv_cache_pos_offset)[m
[32m+[m
[32m+[m[32m        attn_scores = ([m
[32m+[m[32m            einsum("batch query_pos head_index d_head, \[m
[32m+[m[32m                batch key_pos head_index d_head \[m
[32m+[m[32m                -> batch head_index query_pos key_pos",[m[41m [m
[32m+[m[32m                   q, k) / self.attn_scale[m
[32m+[m[32m        )  # [batch, head_index, query_pos, key_pos][m
[32m+[m[32m        if self.cfg.attention_dir == 'causal':[m
[32m+[m[32m            # If causal attention, we mask it to only attend backwards. If bidirectional, we don't mask.[m
[32m+[m[32m            attn_scores = self.apply_causal_mask([m
[32m+[m[32m                attn_scores,[m[41m [m
[32m+[m[32m                kv_cache_pos_offset[m
[32m+[m[32m            ) # [batch, head_index, query_pos, key_pos][m
[32m+[m[32m        attn_matrix = self.hook_attn([m
[32m+[m[32m            F.softmax(attn_scores, dim=-1)[m
[32m+[m[32m        )  # [batch, head_index, query_pos, key_pos][m
[32m+[m[32m        z = self.hook_z([m
[32m+[m[32m            einsum("batch key_pos head_index d_head, \[m
[32m+[m[32m                batch head_index query_pos key_pos -> \[m
[32m+[m[32m                batch query_pos head_index d_head",[m[41m [m
[32m+[m[32m                v, attn_matrix)[m
[32m+[m[32m        )  # [batch, pos, head_index, d_head][m
[32m+[m[32m        if not self.cfg.use_attn_result:[m
[32m+[m[32m            out = ([m
[32m+[m[32m                    einsum("batch pos head_index d_head, \[m
[32m+[m[32m                        head_index d_head d_model -> \[m
[32m+[m[32m                        batch pos d_model",[m[41m [m
[32m+[m[32m                        z,[m[41m [m
[32m+[m[32m                        self.W_O)[m
[32m+[m[32m                ) + self.b_O  # [batch, pos, d_model][m
[32m+[m[32m        else:[m
[32m+[m[32m            # Explicitly calculate the attention result so it can be accessed by a hook[m
[32m+[m[32m            # This is off by default because it can easily eat through your GPU memory.[m
[32m+[m[32m            result = self.hook_result([m
[32m+[m[32m                einsum("batch pos head_index d_head, \[m
[32m+[m[32m                        head_index d_head d_model -> \[m
[32m+[m[32m                        batch pos head_index d_model",[m[41m [m
[32m+[m[32m                       z,[m[41m [m
[32m+[m[32m                       self.W_O)[m
[32m+[m[32m            )  # [batch, pos, head_index, d_model][m
[32m+[m[32m            out = ([m
[32m+[m[32m                einops.reduce([m
[32m+[m[32m                    result, "batch position index model->batch position model", "sum"[m
[32m+[m[32m                )[m
[32m+[m[32m                + self.b_O[m
[32m+[m[32m            )  # [batch, pos, d_model][m
[32m+[m[32m        return out[m
[32m+[m
[32m+[m[32m    def apply_causal_mask(self, attn_scores, past_kv_pos_offset):[m
[32m+[m[32m        # The query context length is the number of positions we take queries from - if not using a past_kv_cache this is just the context length (for the current prompt), but if we're caching it's just a single token.[m
[32m+[m[32m        query_ctx_length = attn_scores.size(-2)[m
[32m+[m[32m        # The key context length is the number of positions in the past - this includes all positions in the cache[m
[32m+[m[32m        # If not caching, query_ctx_length == key_ctx_length[m
[32m+[m[32m        key_ctx_length = attn_scores.size(-1)[m
[32m+[m
[32m+[m[32m        assert query_ctx_length + past_kv_pos_offset == key_ctx_length, f"query_ctx_length {query_ctx_length} + past_kv_pos_offset {past_kv_pos_offset} != key_ctx_length {key_ctx_length} - you likely have a bug."[m
[32m+[m[32m        return torch.where([m
[32m+[m[32m            self.mask[[m
[32m+[m[32m                past_kv_pos_offset : past_kv_pos_offset + query_ctx_length,[m
[32m+[m[32m                : key_ctx_length,[m
[32m+[m[32m            ],[m
[32m+[m[32m            attn_scores,[m
[32m+[m[32m            self.IGNORE,[m
[32m+[m[32m        )[m
[32m+[m[41m    [m
[32m+[m[32m    def shortformer_calculate_qk(self, x, shortformer_pos_embed):[m
[32m+[m[32m        # We add on the positional encodings to the residual stream JUST for the keys and queries, it's not added to the normal residual stream.[m
[32m+[m[32m        attn_input = self.hook_attn_input([m
[32m+[m[32m            x + shortformer_pos_embed[m
[32m+[m[32m            ) # [batch, pos, d_model][m
[32m+[m[32m        q = self.hook_q([m
[32m+[m[32m            einsum("batch pos d_model, head_index d_model d_head \[m
[32m+[m[32m                -> batch pos head_index d_head",[m[41m [m
[32m+[m[32m                        attn_input, self.W_Q) + self.b_Q[m
[32m+[m[32m        )  # [batch, pos, head_index, d_head][m
[32m+[m[32m        k = self.hook_k([m
[32m+[m[32m            einsum("batch pos d_model, head_index d_model d_head \[m
[32m+[m[32m                -> batch pos head_index d_head",[m[41m [m
[32m+[m[32m                        attn_input, self.W_K) + self.b_K[m
[32m+[m[32m        )  # [batch, pos, head_index, d_head][m
[32m+[m[32m        return (q, k)[m
[32m+[m[41m    [m
[32m+[m[32m    def rotary_rotate_qk(self, q, k, past_kv_pos_offset):[m
[32m+[m[32m        # We first apply standard q and k calculation[m
[32m+[m[41m        [m
[32m+[m[32m        q = self.hook_rot_q(self.apply_rotary(q, past_kv_pos_offset))[m
[32m+[m[32m        k = self.hook_rot_k(self.apply_rotary(k))[m
[32m+[m[32m        return q, k[m
[32m+[m[41m    [m
[32m+[m[32m    def calculate_sin_cos_rotary(self, rotary_dim, n_ctx, base=10000):[m
[32m+[m[32m        """[m
[32m+[m[32m        Calculate the sine and cosine waves to use in a rotary embedding. See https://blog.eleuther.ai/rotary-embeddings/ for details[m
[32m+[m[32m        """[m
[32m+[m[32m        pos = torch.arange(n_ctx, dtype=torch.float32)[m
[32m+[m[32m        dim = torch.arange(rotary_dim//2, dtype=torch.float32)[m
[32m+[m[32m        # A set of frequencies evenly spaced in log space[m
[32m+[m[32m        freq = base ** (dim / (rotary_dim / 2))[m
[32m+[m[32m        freq = einops.repeat(freq, "d -> (d 2)")[m
[32m+[m[32m        # Create a n_ctx x rotary_dim tensor, where each column is an arithmetic sequence of angles in that frequency[m
[32m+[m[32m        angles = pos[:, None] / freq[None, :][m
[32m+[m[32m        return torch.sin(angles), torch.cos(angles)[m
[32m+[m
[32m+[m[32m    def rotate_every_two(self, x):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        Rotary helper function, splits x into blocks of size 2 along the final axis and maps [x0, x1] to [-x1, x0][m
[32m+[m[32m        """[m
[32m+[m[32m        rot_x = x.clone()[m
[32m+[m[32m        rot_x[..., 0::2] = -x[..., 1::2][m
[32m+[m[32m        rot_x[..., 1::2] = x[..., 0::2][m
[32m+[m[32m        return rot_x[m
[32m+[m[41m    [m
[32m+[m[32m    def apply_rotary(self, x, past_kv_pos_offset=0):[m
[32m+[m[32m        # Only apply rotary to first rotary_dim dimensions (eg, if rotary_dim=64 and d_head=256, only apply to first 1/4 of dimensions)[m
[32m+[m[32m        x_pos = x.size(1)[m
[32m+[m[32m        x_rot = x[..., :self.cfg.rotary_dim][m
[32m+[m[32m        x_pass = x[..., self.cfg.rotary_dim:][m
[32m+[m[32m        x_flip = self.rotate_every_two(x_rot)[m
[32m+[m[32m        x_rotated = x_rot * self.rotary_cos[past_kv_pos_offset:past_kv_pos_offset+x_pos, None, :] + x_flip * self.rotary_sin[past_kv_pos_offset:past_kv_pos_offset+x_pos, None, :][m
[32m+[m[32m        return torch.cat([x_rotated, x_pass], dim=-1)[m
[32m+[m
[32m+[m[32m# MLP Layers[m
[32m+[m[32mclass MLP(nn.Module):[m
[32m+[m[32m    def __init__(self, cfg: Union[Dict, EasyTransformerConfig]):[m
[32m+[m[32m        super().__init__()[m
[32m+[m[32m        if isinstance(cfg, Dict):[m
[32m+[m[32m            cfg = EasyTransformerConfig.from_dict(cfg)[m
[32m+[m[32m        self.cfg = cfg[m
[32m+[m[32m        self.W_in = nn.Parameter(torch.empty(self.cfg.d_model, self.cfg.d_mlp))[m
[32m+[m[32m        self.b_in = nn.Parameter(torch.zeros(self.cfg.d_mlp))[m
[32m+[m[32m        self.W_out = nn.Parameter(torch.empty(self.cfg.d_mlp, self.cfg.d_model))[m
[32m+[m[32m        self.b_out = nn.Parameter(torch.zeros(self.cfg.d_model))[m
[32m+[m
[32m+[m[32m        self.hook_pre = HookPoint()  # [batch, pos, d_mlp][m
[32m+[m[32m        self.hook_post = HookPoint()  # [batch, pos, d_mlp][m
[32m+[m
[32m+[m[32m        if self.cfg.act_fn == "relu":[m
[32m+[m[32m            self.act_fn = F.relu[m
[32m+[m[32m        elif self.cfg.act_fn == "gelu":[m
[32m+[m[32m            self.act_fn = F.gelu[m
[32m+[m[32m        elif self.cfg.act_fn == "silu":[m
[32m+[m[32m            self.act_fn = F.silu[m
[32m+[m[32m        elif self.cfg.act_fn == "gelu_new":[m
[32m+[m[32m            self.act_fn = gelu_new[m
[32m+[m[32m        elif self.cfg.act_fn == "gelu_fast":[m
[32m+[m[32m            self.act_fn = gelu_fast[m
[32m+[m[32m        elif self.cfg.act_fn == "solu_ln":[m
[32m+[m[32m            self.act_fn = solu[m
[32m+[m[32m            # Hook taken between activation and layer norm[m
[32m+[m[32m            self.hook_mid = HookPoint()  # [batch, pos, d_mlp][m
[32m+[m[32m            if self.cfg.normalization_type=="LN":[m
[32m+[m[32m                self.ln = LayerNorm(self.cfg, self.cfg.d_mlp)[m
[32m+[m[32m            else:[m
[32m+[m[32m                self.ln = LayerNormPre(self.cfg)[m
[32m+[m
[32m+[m[32m        else:[m
[32m+[m[32m            raise ValueError(f"Invalid activation function name: {self.cfg.act_fn}")[m
[32m+[m
[32m+[m[32m    def forward(self, x):[m
[32m+[m[32m        # Technically, all these einsums could be done with a single matmul, but this is more readable.[m
[32m+[m[32m        pre_act = self.hook_pre([m
[32m+[m[32m            einsum("batch pos d_model, d_model d_mlp -> batch pos d_mlp", x, self.W_in) + self.b_in[m
[32m+[m[32m        )  # [batch, pos, d_mlp][m
[32m+[m[32m        if not self.cfg.act_fn.endswith("_ln"):[m
[32m+[m[32m            post_act = self.hook_post(self.act_fn(pre_act))  # [batch, pos, d_mlp][m
[32m+[m[32m        else:[m
[32m+[m[32m            mid_act = self.hook_mid(self.act_fn(pre_act))  # [batch, pos, d_mlp][m
[32m+[m[32m            post_act = self.hook_post(self.ln(mid_act))[m
[32m+[m[32m        mlp_out = ([m
[32m+[m[32m            einsum("batch pos d_mlp, d_mlp d_model -> batch pos d_model", post_act, self.W_out) + self.b_out[m
[32m+[m[32m        )  # [batch, pos, d_model][m
[32m+[m[32m        return mlp_out[m
[32m+[m
[32m+[m
[32m+[m[32m# Transformer Block[m
[32m+[m[32mclass TransformerBlock(nn.Module):[m
[32m+[m[32m    def __init__(self, cfg: Union[Dict, EasyTransformerConfig], block_index):[m
[32m+[m[32m        super().__init__()[m
[32m+[m[32m        if isinstance(cfg, Dict):[m
[32m+[m[32m            cfg = EasyTransformerConfig.from_dict(cfg)[m
[32m+[m[32m        self.cfg = cfg[m
[32m+[m[32m        if self.cfg.normalization_type == "LN":[m
[32m+[m[32m            self.ln1 = LayerNorm(cfg)[m
[32m+[m[32m            if not self.cfg.attn_only:[m
[32m+[m[32m                self.ln2 = LayerNorm(cfg)[m
[32m+[m[32m        elif self.cfg.normalization_type == "LNPre":[m
[32m+[m[32m            # We've folded in LayerNorm weights, so just need the center + scale parts[m
[32m+[m[32m            self.ln1 = LayerNormPre(cfg)[m
[32m+[m[32m            if not self.cfg.attn_only:[m
[32m+[m[32m                self.ln2 = LayerNormPre(cfg)[m
[32m+[m[32m        elif self.cfg.normalization_type is None:[m
[32m+[m[32m            self.ln1 = nn.Identity()[m
[32m+[m[32m            if not self.cfg.attn_only:[m
[32m+[m[32m                self.ln2 = nn.Identity()[m
[32m+[m[32m        else:[m
[32m+[m[32m            logging.warning([m
[32m+[m[32m                f"Invalid normalization_type passed in {self.cfg.normalization_type}"[m
[32m+[m[32m            )[m
[32m+[m
[32m+[m[32m        if not self.cfg.use_local_attn:[m
[32m+[m[32m            self.attn = Attention(cfg, "global", block_index)[m
[32m+[m[32m        else:[m
[32m+[m[32m            assert self.cfg.attn_types is not None[m
[32m+[m[32m            attn_type = self.cfg.attn_types[block_index][m
[32m+[m[32m            self.attn = Attention(cfg, attn_type, block_index)[m
[32m+[m[32m        if not self.cfg.attn_only:[m
[32m+[m[32m            self.mlp = MLP(cfg)[m
[32m+[m
[32m+[m[32m        self.hook_attn_out = HookPoint()  # [batch, pos, d_model][m
[32m+[m[32m        self.hook_mlp_out = HookPoint()  # [batch, pos, d_model][m
[32m+[m[32m        self.hook_resid_pre = HookPoint()  # [batch, pos, d_model][m
[32m+[m[32m        if not self.cfg.attn_only and not self.cfg.parallel_attn_mlp:[m
[32m+[m[32m            self.hook_resid_mid = HookPoint()  # [batch, pos, d_model][m
[32m+[m[32m        self.hook_resid_post = HookPoint()  # [batch, pos, d_model][m
[32m+[m
[32m+[m[32m    def forward([m
[32m+[m[32m            self,[m[41m [m
[32m+[m[32m            resid_pre: torch.Tensor,[m[41m [m
[32m+[m[32m            shortformer_pos_embed: Optional[torch.Tensor] = None,[m
[32m+[m[32m            past_kv_cache_entry: Optional[EasyTransformerKeyValueCacheEntry] = None,[m
[32m+[m[32m        ):[m
[32m+[m[32m        """A single Transformer block.[m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            resid_pre (torch.Tensor): The residual stream - shape [batch, pos, d_model][m
[32m+[m[32m            cache (EasyTransformerKeyValueCache): A cache of previous keys and values, used only when generating text. Defaults to None.[m
[32m+[m[32m            shortformer_pos_embed (torch.Tensor, optional): Only used for positional_embeddings_type == "shortformer". The positional embeddings. See EasyTransformerConfig for details. Defaults to None.[m
[32m+[m
[32m+[m[32m        Returns:[m
[32m+[m[32m            _type_: _description_[m
[32m+[m[32m        """[m
[32m+[m[32m        resid_pre = self.hook_resid_pre(resid_pre)  # [batch, pos, d_model][m
[32m+[m[32m        normalized_resid_pre = self.ln1(resid_pre)[m
[32m+[m[32m        attn_out = self.hook_attn_out([m
[32m+[m[32m            self.attn([m
[32m+[m[32m                normalized_resid_pre,[m[41m [m
[32m+[m[32m                shortformer_pos_embed = shortformer_pos_embed,[m
[32m+[m[32m                past_kv_cache_entry = past_kv_cache_entry)[m
[32m+[m[32m        )  # [batch, pos, d_model][m
[32m+[m[32m        if not self.cfg.attn_only and not self.cfg.parallel_attn_mlp:[m
[32m+[m[32m            resid_mid = self.hook_resid_mid(resid_pre + attn_out)  # [batch, pos, d_model][m
[32m+[m[32m            normalized_resid_mid = self.ln2(resid_mid)[m
[32m+[m[32m            mlp_out = self.hook_mlp_out([m
[32m+[m[32m                self.mlp(normalized_resid_mid)[m
[32m+[m[32m            )  # [batch, pos, d_model][m
[32m+[m[32m            resid_post = self.hook_resid_post(resid_mid + mlp_out)  # [batch, pos, d_model][m
[32m+[m[32m        elif self.cfg.parallel_attn_mlp:[m
[32m+[m[32m            # Dumb thing done by GPT-J, both MLP and Attn read from resid_pre and write to resid_post, no resid_mid used.[m
[32m+[m[32m            # In GPT-J, LN1 and LN2 are tied, in GPT-NeoX they aren't.[m
[32m+[m[32m            normalized_resid_pre_2 = self.ln2(resid_pre)[m
[32m+[m[32m            mlp_out = self.hook_mlp_out([m
[32m+[m[32m                self.mlp(normalized_resid_pre_2)[m
[32m+[m[32m            )  # [batch, pos, d_model][m
[32m+[m[32m            resid_post = self.hook_resid_post(resid_pre + attn_out + mlp_out)  # [batch, pos, d_model][m
[32m+[m[32m        else:[m
[32m+[m[32m            resid_post = self.hook_resid_post(resid_pre + attn_out)  # [batch, pos, d_model][m
[32m+[m[32m        return resid_post[m
\ No newline at end of file[m
[1mdiff --git a/easy_transformer/evals.py b/easy_transformer/evals.py[m
[1mnew file mode 100644[m
[1mindex 0000000..8684e74[m
[1m--- /dev/null[m
[1m+++ b/easy_transformer/evals.py[m
[36m@@ -0,0 +1,111 @@[m
[32m+[m[32m# %%[m
[32m+[m[32m"""[m[41m [m
[32m+[m[32mA file with some rough evals for models - I expect you to be likely better off using the HuggingFace evaluate library if you want to do anything properly, but this is here if you want it and want to eg cheaply and roughly compare models you've trained to baselines.[m
[32m+[m[32m"""[m
[32m+[m
[32m+[m[32mimport torch[m
[32m+[m[32mimport tqdm.auto as tqdm[m
[32m+[m[32mfrom datasets import load_dataset[m
[32m+[m[32mfrom easy_transformer import EasyTransformer, EasyTransformerConfig, utils[m
[32m+[m[32mfrom torch.utils.data import DataLoader[m
[32m+[m[32mimport einops[m
[32m+[m
[32m+[m[32m# %%[m
[32m+[m[32mdef make_wiki_data_loader(tokenizer, batch_size=8):[m
[32m+[m[32m    """[m[41m [m
[32m+[m[32m    Evaluate on Wikitext 2, a dump of Wikipedia articles. (Using the train set because it's larger, I don't really expect anyone to bother with quarantining the validation set nowadays.)[m
[32m+[m
[32m+[m[32m    Note there's likely to be dataset leakage into training data (though I believe GPT-2 was explicitly trained on non-Wikipedia data)[m
[32m+[m[32m    """[m
[32m+[m[32m    wiki_data = load_dataset("wikitext", "wikitext-2-v1", split="train")[m
[32m+[m[32m    print(len(wiki_data))[m
[32m+[m[32m    dataset = utils.tokenize_and_concatenate(wiki_data, tokenizer)[m
[32m+[m[32m    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)[m
[32m+[m[32m    return data_loader[m
[32m+[m
[32m+[m[32mdef make_owt_data_loader(tokenizer, batch_size=8):[m
[32m+[m[32m    """[m[41m [m
[32m+[m[32m    Evaluate on OpenWebText an open source replication of the GPT-2 training corpus (Reddit links with >3 karma)[m
[32m+[m
[32m+[m[32m    I think the Mistral models were trained on this dataset, so they get very good performance.[m
[32m+[m[32m    """[m
[32m+[m[32m    owt_data = load_dataset("stas/openwebtext-10k", split="train")[m
[32m+[m[32m    print(len(owt_data))[m
[32m+[m[32m    dataset = utils.tokenize_and_concatenate(owt_data, tokenizer)[m
[32m+[m[32m    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)[m
[32m+[m[32m    return data_loader[m
[32m+[m
[32m+[m[32mdef make_pile_data_loader(tokenizer, batch_size=8):[m
[32m+[m[32m    """[m[41m [m
[32m+[m[32m    Evaluate on OpenWebText an open source replication of the GPT-2 training corpus (Reddit links with >3 karma)[m
[32m+[m
[32m+[m[32m    I think the Mistral models were trained on this dataset, so they get very good performance.[m
[32m+[m[32m    """[m
[32m+[m[32m    pile_data = load_dataset("NeelNanda/pile-10k", split="train")[m
[32m+[m[32m    print(len(pile_data))[m
[32m+[m[32m    dataset = utils.tokenize_and_concatenate(pile_data, tokenizer)[m
[32m+[m[32m    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)[m
[32m+[m[32m    return data_loader[m
[32m+[m
[32m+[m[32mdef make_code_data_loader(tokenizer, batch_size=8):[m
[32m+[m[32m    """[m[41m [m
[32m+[m[32m    Evaluate on the CodeParrot dataset, a dump of Python code. All models seem to get significantly lower loss here (even non-code trained models like GPT-2), presumably code is much easier to predict than natural language?[m
[32m+[m[32m    """[m
[32m+[m[32m    code_data = load_dataset("codeparrot/codeparrot-valid-v2-near-dedup", split="train")[m
[32m+[m[32m    print(len(code_data))[m
[32m+[m[32m    dataset = utils.tokenize_and_concatenate(code_data, tokenizer, column_name="content")[m
[32m+[m[32m    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)[m
[32m+[m[32m    return data_loader[m
[32m+[m
[32m+[m[32mDATASET_NAMES = ['wiki', 'owt', 'pile', 'code'][m
[32m+[m[32mDATASET_LOADERS = [make_wiki_data_loader, make_owt_data_loader, make_pile_data_loader, make_code_data_loader][m
[32m+[m
[32m+[m[32m# %%[m
[32m+[m[32m@torch.inference_mode()[m
[32m+[m[32mdef evaluate_on_dataset(model, data_loader, truncate=100):[m
[32m+[m[32m    running_loss = 0[m
[32m+[m[32m    total = 0[m
[32m+[m[32m    for batch in tqdm.tqdm(data_loader):[m
[32m+[m[32m        loss = model(batch['tokens'].cuda(), return_type="loss").mean()[m
[32m+[m[32m        running_loss += loss.item()[m
[32m+[m[32m        total+=1[m
[32m+[m[32m        if total > truncate: break[m
[32m+[m[32m    return running_loss/total[m
[32m+[m
[32m+[m[32m# %%[m
[32m+[m[32m@torch.inference_mode()[m
[32m+[m[32mdef induction_loss(model, tokenizer=None, batch_size=4, subseq_len=384, prepend_bos=True):[m
[32m+[m[32m    """[m[41m [m
[32m+[m[32m    Generates a batch of random sequences repeated twice, and measures model performance on the second half. Tests whether a model has induction heads.[m
[32m+[m
[32m+[m[32m    By default, prepends a beginning of string token (prepend_bos flag), which is useful to give models a resting position, and sometimes models were trained with this.[m
[32m+[m[32m    """[m
[32m+[m[32m    # Make the repeated sequence[m
[32m+[m[32m    first_half_tokens = torch.randint(100, 20000, (batch_size, subseq_len)).cuda()[m
[32m+[m[32m    repeated_tokens = einops.repeat(first_half_tokens, "b p -> b (2 p)")[m
[32m+[m
[32m+[m[32m    # Prepend a Beginning Of String token[m
[32m+[m[32m    if prepend_bos:[m
[32m+[m[32m        if tokenizer is None:[m
[32m+[m[32m            tokenizer = model.tokenizer[m
[32m+[m[32m        repeated_tokens[:, 0] = tokenizer.bos_token_id[m
[32m+[m[32m    # Run the model, and extract the per token correct log prob[m
[32m+[m[32m    logits = model(repeated_tokens, return_type="logits")[m
[32m+[m[32m    correct_log_probs = utils.lm_cross_entropy_loss(logits, repeated_tokens, return_per_token=True)[m
[32m+[m[32m    # Take the loss over the second half of the sequence[m
[32m+[m[32m    return (correct_log_probs[:, subseq_len+1:].mean())[m
[32m+[m
[32m+[m[32m# %%[m
[32m+[m[32m@torch.inference_mode()[m
[32m+[m[32mdef evaluate(model, truncate=100, batch_size=8, tokenizer=None):[m
[32m+[m[32m    if tokenizer is None:[m
[32m+[m[32m        tokenizer = model.tokenizer[m
[32m+[m[32m    losses = {}[m
[32m+[m[32m    for data_name, data_loader_fn in zip(DATASET_NAMES, DATASET_LOADERS):[m
[32m+[m[32m        data_loader = data_loader_fn(tokenizer=tokenizer, batch_size=batch_size)[m
[32m+[m[32m        loss = evaluate_on_dataset(model, data_loader, truncate=truncate)[m
[32m+[m[32m        print(f"{data_name}: {loss}")[m
[32m+[m[32m        losses[f"{data_name}_loss"]=loss[m
[32m+[m[32m    return losses[m
[32m+[m
[32m+[m[32m# %%[m
\ No newline at end of file[m
[1mdiff --git a/easy_transformer/experiments.py b/easy_transformer/experiments.py[m
[1mindex 20af1d6..b66d131 100644[m
[1m--- a/easy_transformer/experiments.py[m
[1m+++ b/easy_transformer/experiments.py[m
[36m@@ -262,9 +262,6 @@[m [mclass EasyExperiment:[m
         self.metric = metric[m
         self.cfg = config.adapt_to_model(model)[m
         self.cfg.dataset = self.metric.dataset[m
[31m-        self.other_hooks = [][m
[31m-        # hooks that the model has added in ALL patching experiments[m
[31m-        # consists of (hook_name, hook) tuples[m
 [m
     def run_experiment(self):[m
         self.metric.set_baseline(self.model)[m
[36m@@ -302,13 +299,12 @@[m [mclass EasyExperiment:[m
         mean_metric = torch.zeros(self.metric.shape)[m
         self.model.reset_hooks()[m
         hk_name, hk = abl_hook[m
[31m-        handle = self.model.add_hook(hk_name, hk)[m
[32m+[m[32m        self.model.add_hook(hk_name, hk)[m
 [m
         # only useful if the computation are stochastic. On most case only one loop[m
         for it in range(self.cfg.nb_metric_iteration):[m
[31m-            self.update_setup(hk_name)  # also adds the other_hooks[m
[32m+[m[32m            self.update_setup(hk_name)[m
             mean_metric += self.metric.compute_metric(self.model)[m
[31m-[m
         return mean_metric / self.cfg.nb_metric_iteration[m
 [m
     def update_setup(self, hook_name):[m
[36m@@ -353,7 +349,6 @@[m [mclass EasyAblation(EasyExperiment):[m
             and (config.head_circuit in ["hook_attn_scores", "hook_attn"])[m
         )  # not implemented (surely not very useful)[m
         assert not (mean_by_groups and groups is None)[m
[31m-        assert not (mean_by_groups and config.abl_type not in ["mean", "custom"])[m
         self.semantic_indices = semantic_indices[m
 [m
         self.mean_by_groups = mean_by_groups[m
[36m@@ -407,32 +402,7 @@[m [mclass EasyAblation(EasyExperiment):[m
         self.act_cache = {}[m
         self.model.reset_hooks()[m
         self.model.cache_all(self.act_cache)[m
[31m-        toks = self.model.to_tokens(self.cfg.mean_dataset)[m
[31m-[m
[31m-        # this snippet as preprocessing for random ablation[m
[31m-        # max length of something that was tokenized[m
[31m-        max_len = toks.shape[1][m
[31m-        batch_size = toks.shape[0][m
[31m-        # find the indices that are padding[m
[31m-        are_padding = (toks == self.model.tokenizer.pad_token_id).float()[m
[31m-        # this calculates the index of the first token that's padding in each sequence[m
[31m-        self.first_pad_index = -torch.sum(are_padding, dim=1).long() + max_len[m
[31m-        # for each sequence position, get the places where[m
[31m-        # we can sample from (i.e. not padding)[m
[31m-        self.allowable_indices = [[] for _ in range(max_len)][m
[31m-        self.allowable_lengths = [][m
[31m-        for i in range(max_len):[m
[31m-            for j in range(len(self.cfg.mean_dataset)):[m
[31m-                if self.first_pad_index[j] > i:[m
[31m-                    self.allowable_indices[i].append(j)[m
[31m-            self.allowable_lengths.append(len(self.allowable_indices[i]))[m
[31m-            # pad out self.allowable_indices to be the same length[m
[31m-            self.allowable_indices[i] += [[m
[31m-                1e9 for _ in range(batch_size - self.allowable_lengths[i])[m
[31m-            ]  # 1e9 so will bug if we clip out of range[m
[31m-        # self.allowable_indices = torch.tensor(self.allowable_indices).long().T[m
[31m-        logits = self.model(toks)[m
[31m-[m
[32m+[m[32m        self.model(self.cfg.mean_dataset)[m
         self.mean_cache = {}[m
         for hk in self.act_cache.keys():[m
             if "blocks" in hk:  # TODO optimize to cache only the right activations[m
[36m@@ -459,12 +429,13 @@[m [mclass EasyAblation(EasyExperiment):[m
         mean = einops.repeat(mean, "... -> s ...", s=z.shape[0])[m
 [m
         if self.cfg.abl_type == "random":[m
[31m-            # presume that the thing here has size batch * seq_len * ...[m
[32m+[m
             mean = get_random_sample([m
[31m-                z.clone(),[m
[31m-                self.allowable_lengths,[m
[31m-                self.allowable_indices,[m
[31m-                self.first_pad_index,[m
[32m+[m[32m                z.clone().flatten(start_dim=0, end_dim=1),[m
[32m+[m[32m                ([m
[32m+[m[32m                    self.cfg.batch_size,[m
[32m+[m[32m                    self.cfg.max_seq_len,[m
[32m+[m[32m                ),[m
             )[m
 [m
         if self.mean_by_groups:[m
[36m@@ -530,14 +501,10 @@[m [mclass EasyPatching(EasyExperiment):[m
         if self.cfg.cache_act:[m
             self.get_all_act()[m
 [m
[31m-    def update_setup(self, hk_name):[m
[31m-        for other_hk_name, hk in self.other_hooks:[m
[31m-            self.model.add_hook(other_hk_name, hk)[m
[31m-[m
     def run_patching(self):[m
         return self.run_experiment()[m
 [m
[31m-    def get_hook(self, layer, head=None, target_module=None, manual_patch_fn=None):[m
[32m+[m[32m    def get_hook(self, layer, head=None, target_module=None, patch_fn=None):[m
         # If the target is a layer, head is None.[m
         hook_name, dim = self.get_target(layer, head, target_module=target_module)[m
         if self.cfg.cache_act:[m
[36m@@ -545,12 +512,10 @@[m [mclass EasyPatching(EasyExperiment):[m
         else:[m
             act = self.get_act(hook_name)[m
 [m
[31m-        if manual_patch_fn is None:[m
[32m+[m[32m        if patch_fn is None:[m
             patch_fn = self.cfg.patch_fn[m
[31m-        else:[m
[31m-            patch_fn = manual_patch_fn[m
 [m
[31m-        hook = get_act_hook(patch_fn, act, head, dim=dim)[m
[32m+[m[32m        hook = get_act_hook(self.cfg.patch_fn, act, head, dim=dim)[m
         return (hook_name, hook)[m
 [m
     def get_all_act(self):[m
[36m@@ -620,29 +585,7 @@[m [mdef get_act_hook(fn, alt_act=None, idx=None, dim=None, name=None, message=None):[m
     return custom_hook[m
 [m
 [m
[31m-def get_random_sample(z, allowable_lengths, allowable_indices, first_pad_index):[m
[31m-    """[m
[31m-    z has shape batch_size * seq_len * ...[m
[31m-    allowable_indices has size seq_len, and gives a list for each of the indices of which dataset examples don't have padding at that places[m
[31m-    """[m
[31m-[m
[31m-    batch_size = z.shape[0][m
[31m-    seq_len = z.shape[1][m
[31m-    indices = torch.Tensor([m
[31m-        np.random.randint(low=np.zeros((batch_size, 1)), high=allowable_lengths)[m
[31m-    ).long()  # crazy broadcasting[m
[31m-[m
[31m-    # I think index_select or something could do this more cleverly but ehh[m
[31m-    new_z = z.clone()[m
[31m-    for i in range(batch_size):[m
[31m-        for j in range(seq_len):[m
[31m-            if j >= first_pad_index[i]:[m
[31m-                continue[m
[31m-            new_z[i, j] = z[allowable_indices[j][indices[i, j]], j][m
[31m-    return new_z[m
[31m-[m
[31m-[m
[31m-def old_get_random_sample(activation_set, output_shape):[m
[32m+[m[32mdef get_random_sample(activation_set, output_shape):[m
     """activation_set: shape (N, ... ). Generate a tensor of shape (batch,seq_len,...) made of vectors sampled from activation_set"""[m
     N = activation_set.shape[0][m
     ori_shape = activation_set.shape[1:][m
[1mdiff --git a/easy_transformer/hook_points.py b/easy_transformer/hook_points.py[m
[1mindex 3eb93b4..879b27b 100644[m
[1m--- a/easy_transformer/hook_points.py[m
[1m+++ b/easy_transformer/hook_points.py[m
[36m@@ -1,6 +1,6 @@[m
 # Import stuff[m
 import logging[m
[31m-from typing import Callable[m
[32m+[m[32mfrom typing import Callable, Union, Optional, Sequence[m
 import torch[m
 import torch.nn as nn[m
 import torch.nn.functional as F[m
[36m@@ -32,6 +32,13 @@[m [mimport copy[m
 # import comet_ml[m
 import itertools[m
 [m
[32m+[m[32mfrom easy_transformer.activation_cache import ActivationCache[m
[32m+[m
[32m+[m[32m# %%[m
[32m+[m[32m# Define type aliases[m
[32m+[m[32mNamesFilter = Optional[Union[Callable[[str], bool], Sequence[str]]][m
[32m+[m
[32m+[m[32m# %%[m
 # A helper class to get access to intermediate activations (inspired by Garcon)[m
 # It's a dummy module that is the identity function by default[m
 # I can wrap any intermediate activation in a HookPoint and get a convenient[m
[36m@@ -53,21 +60,16 @@[m [mclass HookPoint(nn.Module):[m
         # which are the same for a HookPoint)[m
 [m
         if dir == "fwd":[m
[31m-[m
             def full_hook(module, module_input, module_output):[m
                 return hook(module_output, hook=self)[m
[31m-[m
             handle = self.register_forward_hook(full_hook)[m
             self.fwd_hooks.append(handle)[m
[31m-            return handle # WARNING: added by Arthur to do patching better[m
         elif dir == "bwd":[m
             # For a backwards hook, module_output is a tuple of (grad,) - I don't know why.[m
             def full_hook(module, module_input, module_output):[m
                 return hook(module_output[0], hook=self)[m
[31m-[m
             handle = self.register_full_backward_hook(full_hook)[m
             self.bwd_hooks.append(handle)[m
[31m-            return handle # WARNING: added by Arthur to do patching better[m
         else:[m
             raise ValueError(f"Invalid direction {dir}")[m
 [m
[36m@@ -97,14 +99,19 @@[m [mclass HookPoint(nn.Module):[m
         split_name = self.name.split(".")[m
         return int(split_name[1])[m
 [m
[31m-[m
[32m+[m[32m# %%[m
 class HookedRootModule(nn.Module):[m
[31m-    # A class building on nn.Module to interface nicely with HookPoints[m
[31m-    # Allows you to name each hook, remove hooks, cache every activation/gradient, etc[m
[32m+[m[32m    """[m
[32m+[m[32m    A class building on nn.Module to interface nicely with HookPoints[m
[32m+[m[32m    Adds various nice utilities, most notably run_with_hooks to run the model with temporary hooks, and run_with_cache to run the model on some input and return a cache of all activations[m
[32m+[m
[32m+[m[32m    WARNING: The main footgun with PyTorch hooking is that hooks are GLOBAL state. If you add a hook to the module, and then run it a bunch of times, the hooks persist. If you debug a broken hook and add the fixed version, the broken one is still there. To solve this, run_with_hooks will remove hooks at the start and end by default, and I recommend using reset_hooks liberally in your code.[m[41m [m
[32m+[m
[32m+[m[32m    The main time this goes wrong is when you want to use backward hooks (to cache or intervene on gradients). In this case, you need to keep the hooks around as global state until you've run loss.backward() (and so need to disable the reset_hooks_end flag on run_with_hooks)[m
[32m+[m[32m    """[m
     def __init__(self, *args):[m
         super().__init__()[m
         self.is_caching = False[m
[31m-[m
     def setup(self):[m
         # Setup function - this needs to be run in __init__ AFTER defining all[m
         # layers[m
[36m@@ -135,67 +142,17 @@[m [mclass HookedRootModule(nn.Module):[m
         self.remove_all_hook_fns(direction)[m
         self.is_caching = False[m
 [m
[31m-    def cache_all(self, cache, incl_bwd=False, device="cuda", remove_batch_dim=False):[m
[31m-        # Caches all activations wrapped in a HookPoint[m
[31m-        # Remove batch dim is a utility for single batch inputs that removes the batch[m
[31m-        # dimension from the cached activations - use ONLY for batches of size 1[m
[31m-        self.cache_some([m
[31m-            cache,[m
[31m-            lambda x: True,[m
[31m-            incl_bwd=incl_bwd,[m
[31m-            device=device,[m
[31m-            remove_batch_dim=remove_batch_dim,[m
[31m-        )[m
[31m-[m
[31m-    def cache_some([m
[31m-        self,[m
[31m-        cache,[m
[31m-        names: Callable[[str], bool],[m
[31m-        incl_bwd=False,[m
[31m-        device="cuda",[m
[31m-        remove_batch_dim=False,[m
[31m-    ):[m
[31m-        """Cache a list of hook provided by names, Boolean function on names"""[m
[31m-        self.is_caching = True[m
[31m-[m
[31m-        def save_hook(tensor, hook):[m
[31m-            if remove_batch_dim:[m
[31m-                cache[hook.name] = tensor.detach().to(device)[0][m
[31m-            else:[m
[31m-                cache[hook.name] = tensor.detach().to(device)[m
[31m-[m
[31m-        def save_hook_back(tensor, hook):[m
[31m-            if remove_batch_dim:[m
[31m-                cache[hook.name + "_grad"] = tensor[0].detach().to(device)[0][m
[31m-            else:[m
[31m-                cache[hook.name + "_grad"] = tensor[0].detach().to(device)[m
[31m-[m
[31m-        for name, hp in self.hook_dict.items():[m
[31m-            if names(name):[m
[31m-                hp.add_hook(save_hook, "fwd")[m
[31m-                if incl_bwd:[m
[31m-                    hp.add_hook(save_hook_back, "bwd")[m
[31m-[m
     def add_hook(self, name, hook, dir="fwd"):[m
         if type(name) == str:[m
[31m-            handle = self.mod_dict[name].add_hook(hook, dir=dir)[m
[31m-            return handle[m
[32m+[m[32m            self.mod_dict[name].add_hook(hook, dir=dir)[m
         else:[m
             # Otherwise, name is a Boolean function on names[m
[31m-            handles = [][m
             for hook_name, hp in self.hook_dict.items():[m
                 if name(hook_name):[m
[31m-                    handles.append(hp.add_hook(hook, dir=dir))[m
[31m-            return handles[m
[32m+[m[32m                    hp.add_hook(hook, dir=dir)[m
 [m
     def run_with_hooks([m
[31m-        self,[m
[31m-        *args,[m
[31m-        fwd_hooks=[],[m
[31m-        bwd_hooks=[],[m
[31m-        reset_hooks_start=True,[m
[31m-        reset_hooks_end=True,[m
[31m-        clear_contexts=False,[m
[32m+[m[32m        self, *model_args, fwd_hooks=[], bwd_hooks=[], reset_hooks_start=True, reset_hooks_end=True, clear_contexts=False, **model_kwargs[m
     ):[m
         """[m
         fwd_hooks: A list of (name, hook), where name is either the name of[m
[36m@@ -223,17 +180,125 @@[m [mclass HookedRootModule(nn.Module):[m
                         hp.add_hook(hook, dir="fwd")[m
         for name, hook in bwd_hooks:[m
             if type(name) == str:[m
[31m-                self.mod_dict[name].add_hook(hook, dir="fwd")[m
[32m+[m[32m                self.mod_dict[name].add_hook(hook, dir="bwd")[m
             else:[m
                 # Otherwise, name is a Boolean function on names[m
                 for hook_name, hp in self.hook_dict:[m
                     if name(hook_name):[m
                         hp.add_hook(hook, dir="bwd")[m
[31m-        out = self.forward(*args)[m
[32m+[m[32m        out = self.forward(*model_args, **model_kwargs)[m
         if reset_hooks_end:[m
             if len(bwd_hooks) > 0:[m
[31m-                logging.warning([m
[31m-                    "WARNING: Hooks were reset at the end of run_with_hooks while backward hooks were set. This removes the backward hooks before a backward pass can occur"[m
[31m-                )[m
[32m+[m[32m                logging.warning("WARNING: Hooks were reset at the end of run_with_hooks while backward hooks were set. This removes the backward hooks before a backward pass can occur")[m
             self.reset_hooks(clear_contexts)[m
         return out[m
[32m+[m
[32m+[m[32m    def add_caching_hooks([m
[32m+[m[32m        self,[m[41m [m
[32m+[m[32m        names_filter: NamesFilter=None,[m[41m [m
[32m+[m[32m        incl_bwd: bool=False,[m[41m [m
[32m+[m[32m        device=None,[m
[32m+[m[32m        remove_batch_dim: bool=False,[m
[32m+[m[32m        cache: Optional[dict]=None[m
[32m+[m[32m        ) -> dict:[m
[32m+[m[32m        """Adds hooks to the model to cache activations. Note: It does NOT actually run the model to get activations, that must be done separately.[m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            names_filter (NamesFilter, optional): Which activations to cache. Can be a list of strings (hook names) or a filter function mapping hook names to booleans. Defaults to lambda name: True.[m
[32m+[m[32m            incl_bwd (bool, optional): Whether to also do backwards hooks. Defaults to False.[m
[32m+[m[32m            device (_type_, optional): The device to store on. Defaults to CUDA if available else CPU.[m
[32m+[m[32m            remove_batch_dim (bool, optional): Whether to remove the batch dimension (only works for batch_size==1). Defaults to False.[m
[32m+[m[32m            cache (Optional[dict], optional): The cache to store activations in, a new dict is created by default. Defaults to None.[m
[32m+[m[41m        [m
[32m+[m[32m        Returns:[m
[32m+[m[32m            cache (dict): The cache where activations will be stored.[m
[32m+[m[32m        """[m
[32m+[m[32m        if remove_batch_dim:[m
[32m+[m[32m            logging.warning("Remove batch dim in caching hooks is deprecated. Use the Cache object or run_with_cache flags instead")[m
[32m+[m
[32m+[m[32m        if device is None:[m
[32m+[m[32m            device = "cuda" if torch.cuda.is_available() else "cpu"[m
[32m+[m[32m        if cache is None:[m
[32m+[m[32m            cache = {}[m
[32m+[m[41m        [m
[32m+[m[32m        if names_filter is None:[m
[32m+[m[32m            names_filter = lambda name: True[m
[32m+[m[32m        elif type(names_filter)==str:[m
[32m+[m[32m            names_filter = lambda name: name==names_filter[m
[32m+[m[32m        elif type(names_filter)==list:[m
[32m+[m[32m            names_filter = lambda name: name in names_filter[m
[32m+[m[41m        [m
[32m+[m[32m        self.is_caching = True[m
[32m+[m
[32m+[m[32m        def save_hook(tensor, hook):[m
[32m+[m[32m            if remove_batch_dim:[m
[32m+[m[32m                cache[hook.name] = tensor.detach().to(device)[0][m
[32m+[m[32m            else:[m
[32m+[m[32m                cache[hook.name] = tensor.detach().to(device)[m
[32m+[m
[32m+[m[32m        def save_hook_back(tensor, hook):[m
[32m+[m[32m            if remove_batch_dim:[m
[32m+[m[32m                cache[hook.name + "_grad"] = tensor[0].detach().to(device)[0][m
[32m+[m[32m            else:[m
[32m+[m[32m                cache[hook.name + "_grad"] = tensor[0].detach().to(device)[m
[32m+[m[41m        [m
[32m+[m[32m        for name, hp in self.hook_dict.items():[m
[32m+[m[32m            if names_filter(name):[m
[32m+[m[32m                hp.add_hook(save_hook, "fwd")[m
[32m+[m[32m                if incl_bwd:[m
[32m+[m[32m                    hp.add_hook(save_hook_back, "bwd")[m
[32m+[m[32m        return cache[m
[32m+[m[41m    [m
[32m+[m[32m    def run_with_cache([m
[32m+[m[32m        self,[m
[32m+[m[32m        *model_args,[m[41m [m
[32m+[m[32m        names_filter: NamesFilter=None,[m
[32m+[m[32m        device=None,[m
[32m+[m[32m        remove_batch_dim=False,[m
[32m+[m[32m        incl_bwd=False,[m
[32m+[m[32m        reset_hooks_end=True,[m
[32m+[m[32m        reset_hooks_start=True,[m
[32m+[m[32m        clear_contexts=False,[m
[32m+[m[32m        return_cache_object=True,[m
[32m+[m[32m        **model_kwargs):[m
[32m+[m[32m        """[m
[32m+[m[32m        Runs the model and returns model output and a Cache object[m
[32m+[m
[32m+[m[32m        model_args and model_kwargs - all positional arguments and keyword arguments not otherwise captured are input to the model[m
[32m+[m[32m        names_filter (None or str or [str] or fn:str->bool): a filter for which activations to cache. Defaults to None, which means cache everything.[m
[32m+[m[32m        device (str or torch.Device): The device to cache activations on, defaults to model device. Note that this must be set if the model does not have a model.cfg.device attribute. WARNING: Setting a different device than the one used by the model leads to significant performance degradation.[m
[32m+[m[32m        remove_batch_dim (bool): If True, will remove the batch dimension when caching. Only makes sense with batch_size=1 inputs.[m
[32m+[m[32m        incl_bwd (bool): If True, will call backward on the model output and also cache gradients. It is assumed that the model outputs a scalar, ie. return_type="loss", for predict the next token loss. Custom loss functions are not supported[m
[32m+[m[32m        reset_hooks_start (bool): If True, all prior hooks are removed at the start[m
[32m+[m[32m        reset_hooks_end (bool): If True, all hooks are removed at the end (ie,[m
[32m+[m[32m        including those added in this run)[m
[32m+[m[32m        clear_contexts (bool): If True, clears hook contexts whenever hooks are reset[m
[32m+[m[32m        return_cache_obj (bool): If True, returns an ActivationCache object, with many EasyTransformer specific methods. Otherwise returns a dictionary.[m
[32m+[m[32m        """[m
[32m+[m[32m        if reset_hooks_start:[m
[32m+[m[32m            self.reset_hooks(clear_contexts)[m
[32m+[m[32m        cache_dict = self.add_caching_hooks(names_filter, incl_bwd, device, remove_batch_dim)[m
[32m+[m[32m        model_out = self(*model_args, **model_kwargs)[m
[32m+[m
[32m+[m[32m        if incl_bwd:[m
[32m+[m[32m            model_out.backward()[m
[32m+[m[41m        [m
[32m+[m[32m        if return_cache_object:[m
[32m+[m[32m            cache = ActivationCache(cache_dict, self)[m
[32m+[m[32m        else:[m
[32m+[m[32m            cache = cache_dict[m
[32m+[m[41m        [m
[32m+[m[32m        if reset_hooks_end:[m
[32m+[m[32m            self.reset_hooks(clear_contexts)[m
[32m+[m[32m        return model_out, cache[m
[32m+[m
[32m+[m
[32m+[m[32m    def cache_all(self, cache, incl_bwd=False, device=None, remove_batch_dim=False):[m
[32m+[m[32m        logging.warning("cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache")[m
[32m+[m[32m        self.add_caching_hooks(names_filter=lambda name: True, cache=cache, incl_bwd=incl_bwd, device=device, remove_batch_dim=remove_batch_dim)[m
[32m+[m
[32m+[m[32m    def cache_some(self, cache, names: Callable[[str], bool], incl_bwd=False, device=None, remove_batch_dim=False):[m
[32m+[m[32m        """Cache a list of hook provided by names, Boolean function on names"""[m
[32m+[m[32m        logging.warning("cache_some is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache")[m
[32m+[m[32m        self.add_caching_hooks(names_filter=names, cache=cache, incl_bwd=incl_bwd, device=device, remove_batch_dim=remove_batch_dim)[m
[32m+[m[32m# %%[m
\ No newline at end of file[m
[1mdiff --git a/easy_transformer/train.py b/easy_transformer/train.py[m
[1mnew file mode 100644[m
[1mindex 0000000..e2df93f[m
[1m--- /dev/null[m
[1m+++ b/easy_transformer/train.py[m
[36m@@ -0,0 +1,156 @@[m
[32m+[m[32mfrom easy_transformer import EasyTransformer[m
[32m+[m[32mfrom easy_transformer import EasyTransformerConfig[m
[32m+[m[32mfrom dataclasses import dataclass[m
[32m+[m[32mfrom typing import Optional, Callable[m
[32m+[m[32mfrom torch.utils.data import Dataset, DataLoader[m
[32m+[m[32mimport torch.optim as optim[m
[32m+[m[32mimport wandb[m
[32m+[m[32mimport torch[m
[32m+[m[32mimport torch.nn as nn[m
[32m+[m[32mfrom tqdm.auto import tqdm[m
[32m+[m[32mfrom einops import rearrange[m
[32m+[m
[32m+[m
[32m+[m[32m@dataclass[m
[32m+[m[32mclass EasyTransformerTrainConfig:[m
[32m+[m[32m    """[m
[32m+[m[32m    Configuration class to store training hyperparameters for a training run of[m
[32m+[m[32m    an EasyTransformer model.[m
[32m+[m[32m    Args:[m
[32m+[m[32m        num_epochs (int): Number of epochs to train for[m
[32m+[m[32m        batch_size (int): Size of batches to use for training[m
[32m+[m[32m        lr (float): Learning rate to use for training[m
[32m+[m[32m        seed (int): Random seed to use for training[m
[32m+[m[32m        momentum (float): Momentum to use for training[m
[32m+[m[32m        max_grad_norm (float, *optional*): Maximum gradient norm to use for[m
[32m+[m[32m        weight_decay (float, *optional*): Weight decay to use for training[m
[32m+[m[32m            training[m
[32m+[m[32m        optimizer_name (str): The name of the optimizer to use[m
[32m+[m[32m        device (str, *optional*): Device to use for training[m
[32m+[m[32m        warmup_steps (int, *optional*): Number of warmup steps to use for training[m
[32m+[m[32m        save_every (int, *optional*): After how many batches should a checkpoint be saved[m
[32m+[m[32m        save_dir, (str, *optional*): Where to save checkpoints[m
[32m+[m[32m        wandb (bool): Whether to use Weights and Biases for logging[m
[32m+[m[32m        wandb_project (str, *optional*): Name of the Weights and Biases project to use[m
[32m+[m[32m        print_every (int, *optional*): Print the loss every n steps[m
[32m+[m[32m        max_steps (int, *optional*): Terminate the epoch after this many steps. Used for debugging.[m
[32m+[m[32m    """[m
[32m+[m
[32m+[m[32m    num_epochs: int[m
[32m+[m[32m    batch_size: int[m
[32m+[m[32m    lr: float = 1e-3[m
[32m+[m[32m    seed: int = 0[m
[32m+[m[32m    momentum: float = 0.0[m
[32m+[m[32m    max_grad_norm: Optional[float] = None[m
[32m+[m[32m    weight_decay: Optional[float] = None[m
[32m+[m[32m    optimizer_name: str = "Adam"[m
[32m+[m[32m    device: Optional[str] = None[m
[32m+[m[32m    warmup_steps: int = 0[m
[32m+[m[32m    save_every: Optional[int] = None[m
[32m+[m[32m    save_dir: Optional[str] = None[m
[32m+[m[32m    wandb: bool = False[m
[32m+[m[32m    wandb_project_name: Optional[str] = None[m
[32m+[m[32m    print_every: Optional[int] = 50[m
[32m+[m[32m    max_steps: Optional[int] = None[m
[32m+[m
[32m+[m
[32m+[m[32mdef train([m
[32m+[m[32m    model: EasyTransformer,[m
[32m+[m[32m    config: EasyTransformerTrainConfig,[m
[32m+[m[32m    dataset: Dataset,[m
[32m+[m[32m) -> EasyTransformer:[m
[32m+[m[32m    """[m
[32m+[m[32m    Trains an EasyTransformer model on an autoregressive language modeling task.[m
[32m+[m[32m    Args:[m
[32m+[m[32m        model: The model to train[m
[32m+[m[32m        config: The training configuration[m
[32m+[m[32m        dataset: The dataset to train on - this function assumes the dataset is[m
[32m+[m[32m            set up for autoregressive language modeling.[m
[32m+[m[32m    Returns:[m
[32m+[m[32m        The trained model[m
[32m+[m[32m    """[m
[32m+[m[32m    torch.manual_seed(config.seed)[m
[32m+[m[32m    model.train()[m
[32m+[m[32m    if config.wandb:[m
[32m+[m[32m        if config.wandb_project_name is None:[m
[32m+[m[32m            config.wandb_project_name = "easy-transformer"[m
[32m+[m[32m        wandb.init(project=config.wandb_project_name, config=vars(config))[m
[32m+[m
[32m+[m[32m    if config.device is None:[m
[32m+[m[32m        config.device = "cuda" if torch.cuda.is_available() else "cpu"[m
[32m+[m
[32m+[m[32m    if config.optimizer_name in ["Adam", "AdamW"]:[m
[32m+[m[32m        # Weight decay in Adam is implemented badly, so use AdamW instead (see PyTorch AdamW docs)[m
[32m+[m[32m        if config.weight_decay is not None:[m
[32m+[m[32m            optimizer = optim.AdamW([m
[32m+[m[32m                model.parameters(),[m
[32m+[m[32m                lr=config.lr,[m
[32m+[m[32m                weight_decay=config.weight_decay,[m
[32m+[m[32m            )[m
[32m+[m[32m        else:[m
[32m+[m[32m            optimizer = optim.Adam([m
[32m+[m[32m            model.parameters(),[m
[32m+[m[32m            lr=config.lr,[m
[32m+[m[32m            )[m
[32m+[m[32m    elif config.optimizer_name == "SGD":[m
[32m+[m[32m        optimizer = optim.SGD([m
[32m+[m[32m            model.parameters(),[m
[32m+[m[32m            lr=config.lr,[m
[32m+[m[32m            weight_decay=config.weight_decay[m
[32m+[m[32m            if config.weight_decay is not None[m
[32m+[m[32m            else 0.0,[m
[32m+[m[32m            momentum=config.momentum,[m
[32m+[m[32m        )[m
[32m+[m[32m    else:[m
[32m+[m[32m        raise ValueError(f"Optimizer {config.optimizer_name} not supported")[m
[32m+[m
[32m+[m[32m    scheduler = None[m
[32m+[m[32m    if config.warmup_steps > 0:[m
[32m+[m[32m        scheduler = optim.lr_scheduler.LambdaLR([m
[32m+[m[32m            optimizer,[m
[32m+[m[32m            lr_lambda=lambda step: min(1.0, step / config.warmup_steps),[m
[32m+[m[32m        )[m
[32m+[m
[32m+[m[32m    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)[m
[32m+[m
[32m+[m[32m    model.to(config.device)[m
[32m+[m
[32m+[m[32m    for epoch in tqdm(range(1, config.num_epochs + 1)):[m
[32m+[m[32m        samples = 0[m
[32m+[m[32m        for step, batch in tqdm(enumerate(dataloader)):[m
[32m+[m[32m            tokens = batch['tokens'].to(config.device)[m
[32m+[m[32m            loss = model(tokens, return_type='loss')[m
[32m+[m[32m            loss.backward()[m
[32m+[m[32m            if config.max_grad_norm is not None:[m
[32m+[m[32m                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)[m
[32m+[m[32m            optimizer.step()[m
[32m+[m[32m            if config.warmup_steps > 0:[m
[32m+[m[32m                assert scheduler is not None[m
[32m+[m[32m                scheduler.step()[m
[32m+[m[32m            optimizer.zero_grad()[m
[32m+[m
[32m+[m[32m            samples += tokens.shape[0][m
[32m+[m
[32m+[m[32m            if config.wandb:[m
[32m+[m[32m                wandb.log({"train_loss": loss.item(), "samples": samples, 'epoch': epoch})[m
[32m+[m[41m            [m
[32m+[m[32m            if ([m
[32m+[m[32m                config.print_every is not None[m
[32m+[m[32m                and step % config.print_every == 0[m
[32m+[m[32m            ):[m
[32m+[m[32m                print(f"Epoch {epoch} Samples {samples} Step {step} Loss {loss.item()}")[m
[32m+[m
[32m+[m[32m            if ([m
[32m+[m[32m                config.save_every is not None[m
[32m+[m[32m                and step % config.save_every == 0[m
[32m+[m[32m                and config.save_dir is not None[m
[32m+[m[32m            ):[m
[32m+[m[32m                torch.save(model.state_dict(), f"{config.save_dir}/model_{step}.pt")[m
[32m+[m[41m            [m
[32m+[m[32m            if ([m
[32m+[m[32m                config.max_steps is not None[m
[32m+[m[32m                and step >= config.max_steps[m
[32m+[m[32m            ):[m
[32m+[m[32m                break[m
[32m+[m
[32m+[m[32m    return model[m
[1mdiff --git a/easy_transformer/utils.py b/easy_transformer/utils.py[m
[1mindex 27745e6..f99f8a9 100644[m
[1m--- a/easy_transformer/utils.py[m
[1m+++ b/easy_transformer/utils.py[m
[36m@@ -3,7 +3,37 @@[m [mimport torch[m
 import torch.nn as nn[m
 import torch.nn.functional as F[m
 import gc[m
[32m+[m[32mimport datasets[m
[32m+[m[32mimport einops[m
[32m+[m[32mfrom transformers import AutoTokenizer[m
[32m+[m[32mimport random[m
[32m+[m[32mfrom typing import Optional, Union, Tuple, List[m
[32m+[m[32mimport transformers[m
[32m+[m[32mfrom huggingface_hub import hf_hub_download[m
[32m+[m[32mimport re[m
[32m+[m[32mfrom functools import lru_cache[m
 [m
[32m+[m[32mCACHE_DIR = transformers.TRANSFORMERS_CACHE[m
[32m+[m[32mimport json[m
[32m+[m
[32m+[m[32mdef download_file_from_hf(repo_name, file_name, subfolder=".", cache_dir=CACHE_DIR, force_is_torch=False):[m
[32m+[m[32m    """[m[41m [m
[32m+[m[32m    Helper function to download files from the HuggingFace Hub, from subfolder/file_name in repo_name, saving locally to cache_dir and returning the loaded file (if a json or Torch object) and the file path otherwise.[m
[32m+[m
[32m+[m[32m    If it's a Torch file without the ".pth" extension, set force_is_torch=True to load it as a Torch object.[m
[32m+[m[32m    """[m
[32m+[m[32m    file_path = hf_hub_download(repo_id=repo_name,[m
[32m+[m[32m                                                filename=file_name,[m[41m [m
[32m+[m[32m                                                subfolder=subfolder,[m[41m [m
[32m+[m[32m                                                cache_dir=cache_dir)[m
[32m+[m[32m    print(f"Saved at file_path: {file_path}")[m
[32m+[m[32m    if file_path.endswith(".pth") or force_is_torch:[m
[32m+[m[32m        return torch.load(file_path)[m
[32m+[m[32m    elif file_path.endswith(".json"):[m
[32m+[m[32m        return json.load(open(file_path, "r"))[m
[32m+[m[32m    else:[m
[32m+[m[32m        print("File type not supported:", file_path.split('.')[-1])[m
[32m+[m[32m        return file_path[m
 [m
 def get_sample_from_dataset(sequences, nb_sample=2, print_len=10):[m
     rd_idx = np.random.randint(0, len(sequences), 3)[m
[36m@@ -15,26 +45,9 @@[m [mdef print_gpu_mem(step_name=""):[m
         f"{step_name} ~ {np.round(torch.cuda.memory_allocated()/2e30, 2)} GiB allocated on GPU."[m
     )[m
 [m
[31m-[m
[31m-def get_corner(tensor, n=2):[m
[32m+[m[32mdef get_corner(tensor, n=3):[m
     # Prints the top left corner of the tensor[m
[31m-    if len(tensor.shape) == 0:[m
[31m-        return tensor[m
[31m-    elif len(tensor.shape) == 1:[m
[31m-        return tensor[:n][m
[31m-    elif len(tensor.shape) == 2:[m
[31m-        return tensor[:n, :n][m
[31m-    elif len(tensor.shape) == 3:[m
[31m-        return tensor[:n, :n, :n][m
[31m-    elif len(tensor.shape) == 4:[m
[31m-        return tensor[:n, :n, :n, :n][m
[31m-    elif len(tensor.shape) == 5:[m
[31m-        return tensor[:n, :n, :n, :n, :n][m
[31m-    elif len(tensor.shape) == 6:[m
[31m-        return tensor[:n, :n, :n, :n, :n, :n][m
[31m-    else:[m
[31m-        # I never need tensors of rank > 6[m
[31m-        raise ValueError(f"Tensor of shape {tensor.shape} is too big")[m
[32m+[m[32m    return tensor[tuple(slice(n) for _ in range(tensor.ndim))][m
 [m
 [m
 def to_numpy(tensor, flat=False):[m
[36m@@ -47,7 +60,40 @@[m [mdef to_numpy(tensor, flat=False):[m
     else:[m
         return tensor.detach().cpu().numpy()[m
 [m
[32m+[m[32mdef lm_cross_entropy_loss([m
[32m+[m[32m        logits: torch.Tensor, tokens: torch.Tensor, return_per_token: bool = False[m
[32m+[m[32m    ):[m
[32m+[m[32m    """Cross entropy loss for the language model, gives the loss for predicting the NEXT token.[m
[32m+[m
[32m+[m[32m    Args:[m
[32m+[m[32m        logits (torch.Tensor): Logits. Shape [batch, pos, d_vocab][m
[32m+[m[32m        tokens (torch.Tensor[int64]): Input tokens. Shape [batch, pos][m
[32m+[m[32m        return_per_token (bool, optional): Whether to return the log probs predicted for the correct token, or the loss (ie mean of the predicted log probs). Note that the returned array has shape [batch, seq-1] as we cannot predict the first token (alternately, we ignore the final logit). Defaults to False.[m
[32m+[m[32m    """[m
[32m+[m[32m    log_probs = F.log_softmax(logits, dim=-1)[m
[32m+[m[32m    # Use torch.gather to find the log probs of the correct tokens[m
[32m+[m[32m    # Offsets needed because we're predicting the NEXT token (this means the final logit is meaningless)[m
[32m+[m[32m    # None and [..., 0] needed because the tensor used in gather must have the same rank.[m
[32m+[m[32m    predicted_log_probs = log_probs[..., :-1, :].gather([m
[32m+[m[32m        dim=-1, index=tokens[..., 1:, None][m
[32m+[m[32m    )[..., 0][m
[32m+[m[32m    if return_per_token:[m
[32m+[m[32m        return -predicted_log_probs[m
[32m+[m[32m    else:[m
[32m+[m[32m        return -predicted_log_probs.mean()[m
 [m
[32m+[m[32mdef lm_accuracy(logits: torch.Tensor, tokens: torch.Tensor, return_per_token: bool = False):[m
[32m+[m[32m    """ Cross-Entropy Accuracy for Language Modelling. We measure the accuracy on the logits for predicting the NEXT token.[m
[32m+[m[41m    [m
[32m+[m[32m    If return_per_token is True, returns the boolean for top 1 accuracy for each token in the batch. Note that this has size [batch, seq_len-1], as we cannot predict the first token.[m[41m [m
[32m+[m[32m    """[m
[32m+[m[32m    top_prediction = logits.argmax(dim=-1)[m
[32m+[m[32m    correct_matches = top_prediction[:, :-1] == tokens[:, 1:][m
[32m+[m[32m    if return_per_token:[m
[32m+[m[32m        return correct_matches[m
[32m+[m[32m    else:[m
[32m+[m[32m        return correct_matches.sum()/correct_matches.numel()[m
[32m+[m[41m    [m
 def gelu_new(input):[m
     # Implementation of GeLU used by GPT2 - subtly different from PyTorch's[m
     return ([m
[36m@@ -61,6 +107,8 @@[m [mdef gelu_new(input):[m
         )[m
     )[m
 [m
[32m+[m[32mdef gelu_fast(input):[m
[32m+[m[32m    return 0.5 * input * (1.0 + torch.tanh(input * 0.7978845608 * (1.0 + 0.044715 * input * input)))[m
 [m
 def solu(input):[m
     """[m
[36m@@ -72,28 +120,433 @@[m [mdef solu(input):[m
     return input * F.softmax(input, dim=-1)[m
 [m
 [m
[31m-def reglu(input, gate):[m
[32m+[m[32mdef keep_single_column([m
[32m+[m[32m        dataset: datasets.arrow_dataset.Dataset,[m
[32m+[m[32m        col_name: str):[m
     """[m
[31m-    ReGLU activation function as described by[m
[31m-    https://arxiv.org/pdf/2002.05202.pdf.[m
[32m+[m[32m    Acts on a HuggingFace dataset to delete all columns apart from a single column name - useful when we want to tokenize and mix together different strings[m
     """[m
[31m-    return F.relu(gate) * input[m
[32m+[m[32m    for key in dataset.features:[m
[32m+[m[32m        if key != col_name:[m
[32m+[m[32m            dataset = dataset.remove_columns(key)[m
[32m+[m[32m    return dataset[m
[32m+[m
[32m+[m[32mdef tokenize_and_concatenate(dataset: datasets.arrow_dataset.Dataset,[m[41m [m
[32m+[m[32m                             tokenizer: AutoTokenizer,[m[41m [m
[32m+[m[32m                             streaming: bool=False,[m[41m [m
[32m+[m[32m                             max_length: int=1024,[m[41m [m
[32m+[m[32m                             column_name: str='text',[m[41m [m
[32m+[m[32m                             add_bos_token: bool=True,[m
[32m+[m[32m                             num_proc: int=10):[m
[32m+[m[32m    """Helper function to tokenizer and concatenate a dataset of text. This converts the text to tokens, concatenates them (separated by EOS tokens) and then reshapes them into a 2D array of shape (____, sequence_length), dropping the last batch. Tokenizers are much faster if parallelised, so we chop the string into 20, feed it into the tokenizer, in parallel with padding, then remove padding at the end.[m[41m [m
[32m+[m[41m    [m
[32m+[m[32m    This tokenization is useful for training language models, as it allows us to efficiently train on a large corpus of text of varying lengths (without, eg, a lot of truncation or padding). Further, for models with absolute positional encodings, this avoids privileging early tokens (eg, news articles often begin with CNN, and models may learn to use early positional encodings to predict these)[m
 [m
[32m+[m[32m    Args:[m
[32m+[m[32m        dataset (datasets.arrow_dataset.Dataset): The dataset to tokenize, assumed to be a HuggingFace text dataset.[m
[32m+[m[32m        tokenizer (AutoTokenizer): The tokenizer. Assumed to have a bos_token_id and an eos_token_id.[m
[32m+[m[32m        streaming (bool, optional): Whether the dataset is being streamed. If True, avoids using parallelism. Defaults to False.[m
[32m+[m[32m        max_length (int, optional): The length of the context window of the sequence. Defaults to 1024.[m
[32m+[m[32m        column_name (str, optional): The name of the text column in the dataset. Defaults to 'text'.[m
[32m+[m[32m        add_bos_token (bool, optional): . Defaults to True.[m
 [m
[31m-def geglu(input, gate, use_gelu_new=False):[m
[32m+[m[32m    Returns:[m
[32m+[m[32m        datasets.arrow_dataset.Dataset: Returns the tokenized dataset, as a dataset of tensors, with a single column called "tokens"[m
[32m+[m[41m    [m
[32m+[m[32m    Note: There is a bug when inputting very small datasets (eg, <1 batch per process) where it just outputs nothing. I'm not super sure why[m
     """[m
[31m-    GeGLU activation function as described by[m
[31m-    https://arxiv.org/pdf/2002.05202.pdf.[m
[32m+[m[32m    dataset = keep_single_column(dataset, column_name)[m
[32m+[m[32m    if tokenizer.pad_token is None:[m
[32m+[m[32m        # We add a padding token, purely to implement the tokenizer. This will be removed before inputting tokens to the model, so we do not need to increment d_vocab in the model.[m
[32m+[m[32m        tokenizer.add_special_tokens({'pad_token': "<PAD>"})[m
[32m+[m[32m    # Define the length to chop things up into - leaving space for a bos_token if required[m
[32m+[m[32m    if add_bos_token:[m
[32m+[m[32m        seq_len = max_length - 1[m
[32m+[m[32m    else:[m
[32m+[m[32m        seq_len = max_length[m
[32m+[m[41m    [m
[32m+[m[32m    def tokenize_function(examples):[m
[32m+[m[32m        text = examples[column_name][m
[32m+[m[32m        # Concatenate it all into an enormous string, separated by eos_tokens[m
[32m+[m[32m        full_text = tokenizer.eos_token.join(text)[m
[32m+[m[32m        # Divide into 20 chunks of ~ equal length[m
[32m+[m[32m        num_chunks = 20[m
[32m+[m[32m        chunk_length = (len(full_text)-1)//num_chunks + 1[m
[32m+[m[32m        chunks = [full_text[i*chunk_length:(i+1)*chunk_length] for i in range(num_chunks)][m
[32m+[m[32m        # Tokenize the chunks in parallel. Uses NumPy because HuggingFace map doesn't want tensors returned[m
[32m+[m[32m        tokens = tokenizer(chunks, return_tensors='np', padding=True)['input_ids'].flatten()[m
[32m+[m[32m        # Drop padding tokens[m
[32m+[m[32m        tokens = tokens[tokens!=tokenizer.pad_token_id][m
[32m+[m[32m        num_tokens = len(tokens)[m
[32m+[m[32m        num_batches = num_tokens//(seq_len)[m
[32m+[m[32m        # Drop the final tokens if not enough to make a full sequence[m
[32m+[m[32m        tokens = tokens[:seq_len*num_batches][m
[32m+[m[32m        tokens = einops.rearrange(tokens, '(batch seq) -> batch seq', batch=num_batches, seq=seq_len)[m
[32m+[m[32m        if add_bos_token:[m
[32m+[m[32m            prefix = np.full((num_batches, 1), tokenizer.bos_token_id)[m
[32m+[m[32m            tokens = np.concatenate([prefix, tokens], axis=1)[m
[32m+[m[32m        return {'tokens':tokens}[m
[32m+[m[32m    tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=(num_proc if not streaming else None), remove_columns=[column_name])[m
[32m+[m[32m    tokenized_dataset.set_format(type='torch', columns=['tokens'])[m
[32m+[m[32m    return tokenized_dataset[m
[32m+[m[32m"""[m[41m [m
[32m+[m[32mTest ^[m
[32m+[m
[32m+[m[32mdata = datasets.Dataset.from_dict({"text":[str(i) for i in range(1000)]})[m
[32m+[m[32mtokenizer = AutoTokenizer.from_pretrained("NeelNanda/gpt-neox-tokenizer-digits")[m
[32m+[m[32mprint(data)[m
[32m+[m[32mtokenize_and_concatenate(data, tokenizer, streaming=False, column_name="text")[m
[32m+[m[32m"""[m
[32m+[m
[32m+[m[32mdef set_seed_everywhere(seed):[m
[32m+[m[32m    torch.manual_seed(seed)[m
[32m+[m[32m    random.seed(seed)[m
[32m+[m[32m    np.random.seed(seed)[m
[32m+[m
[32m+[m
[32m+[m[32mdef sample_logits([m
[32m+[m[32m    final_logits: torch.Tensor,[m[41m [m
[32m+[m[32m    top_k: Optional[int] = None,[m[41m [m
[32m+[m[32m    top_p: Optional[int] = None,[m[41m [m
[32m+[m[32m    temperature: float = 1.0,[m[41m [m
[32m+[m[32m    freq_penalty: float = 0.0,[m
[32m+[m[32m    tokens: Optional[torch.Tensor] = None):[m
[32m+[m[32m    """[m[41m [m
[32m+[m[32m    Sample from the logits, in order to generate text[m
[32m+[m
[32m+[m[32m    final_logits has shape [batch, vocab_size][m
[32m+[m[32m    We divide the logits by temperature before softmaxing and sampling - high temperature = more uniform, low = more argmaxy. Temp = 0.0 is greedy sampling[m
[32m+[m[32m    We apply top_k and top_p filtering to the logits, to encourage diversity. top_k = 10 means we only sample from the 10 most likely tokens. top_p = 0.9 means we only sample from the top 90% of tokens, and then renormalise the distribution. top_k and top_p are mutually exclusive. By default we apply neither and just sample from the full distribution.[m
[32m+[m
[32m+[m[32m    Frequency penalty is a penalty on the probability of a token, proportional to the number of times it has been generated so far. This encourages the model to generate new tokens, rather than repeating itself. It is a hyperparameter, and should be tuned. It is applied to the logits before sampling. If this is non-zero it is required to input the input_tokens[m
[32m+[m
[32m+[m[32m    #! TODO: Finish testing all the edge cases here. Useful testing code:[m
[32m+[m[32m    logits = torch.randn(4)[m
[32m+[m[32m    print(logits)[m
[32m+[m[32m    np.unique(np.array([sample_logits(logits, top_k=2).item() for i in range(1000)]), return_counts=True)[m
     """[m
[31m-    if use_gelu_new:[m
[31m-        return gelu_new(gate) * input[m
[32m+[m[32m    if temperature == 0.0:[m
[32m+[m[32m        # Greedy sampling[m
[32m+[m[32m        return final_logits.argmax(dim=-1)[m
     else:[m
[31m-        return F.gelu(gate) * input[m
[32m+[m[32m        # Sample from the distribution[m
 [m
[32m+[m[32m        final_logits = final_logits / temperature[m
[32m+[m[32m        if freq_penalty > 0:[m
[32m+[m[32m            assert tokens is not None, "Must provide input_tokens if applying a frequency penalty"[m
[32m+[m[32m            for batch_index in range(final_logits.shape[0]):[m
[32m+[m[32m                # torch.bincount returns a tensor of length d_vocab, with the number of occurences of each token in the tokens.[m
[32m+[m[32m                final_logits[batch_index] = final_logits[batch_index] - freq_penalty * torch.bincount([m
[32m+[m[32m                    tokens[batch_index], minlength=final_logits.shape[-1][m
[32m+[m[32m                )[m
[32m+[m[32m        if top_k is not None:[m
[32m+[m[32m            assert top_k > 0, "top_k has to be greater than 0"[m
[32m+[m[32m            top_logits, top_idx = final_logits.topk(top_k, dim=-1)[m
[32m+[m[32m            indices_to_remove = final_logits < top_logits[..., -1].unsqueeze(-1)[m
[32m+[m[32m            final_logits = final_logits.masked_fill(indices_to_remove, -float("inf"))[m
[32m+[m[32m        elif top_p is not None:[m
[32m+[m[32m            assert 1.0 >= top_p > 0.0, "top_p has to be in [0, 1)"[m
[32m+[m[32m            sorted_logits, sorted_indices = torch.sort(final_logits, descending=True)[m
[32m+[m[32m            cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)[m
[32m+[m[32m            # We round up - we want prob >= top_p not <top_p[m
[32m+[m[32m            sorted_indices_to_remove = cumulative_probs > top_p[m
[32m+[m[32m            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[[m
[32m+[m[32m                ..., :-1[m
[32m+[m[32m            ].clone()[m
[32m+[m[32m            sorted_indices_to_remove[..., 0] = 0[m
[32m+[m[32m            indices_to_remove = sorted_indices_to_remove.scatter([m
[32m+[m[32m                -1, sorted_indices, sorted_indices_to_remove[m
[32m+[m[32m            )[m
[32m+[m[32m            final_logits = final_logits.masked_fill(indices_to_remove, -float("inf"))[m
[32m+[m[32m        return torch.distributions.categorical.Categorical(logits=final_logits).sample()[m
[32m+[m
[32m+[m[32m# %%[m
[32m+[m[32m# Type alias[m
[32m+[m[32mSliceInput =  Optional[Union[int, Tuple[int, int], Tuple[int, int, int], List[int], torch.Tensor]][m
[32m+[m[32mclass Slice:[m
[32m+[m[32m    """[m
[32m+[m[32m    We use a custom slice syntax because Python/Torch's don't let us reduce the number of dimensions:[m
[32m+[m[41m    [m
[32m+[m[32m    Note that slicing with input_slice=None means do nothing, NOT add an extra dimension (use unsqueeze for that)[m
 [m
[31m-def swiglu(input, gate):[m
[32m+[m[32m    There are several modes:[m
[32m+[m[32m    int - just index with that integer (decreases number of dimensions)[m
[32m+[m[32m    slice - Input is a tuple converted to a slice ((k,) means :k, (k, m) means m:k, (k, m, n) means m:k:n)[m
[32m+[m[32m    array - Input is a list or tensor or numpy array, converted to a numpy array, and we take the stack of values at those indices[m
[32m+[m[32m    identity - Input is None, leave it unchanged.[m
[32m+[m
[32m+[m[32m    Examples for dim=0:[m
[32m+[m[32m    if input_slice=0, tensor -> tensor[0][m
[32m+[m[32m    elif input_slice = (1, 5), tensor -> tensor[1:5][m
[32m+[m[32m    elif input_slice = (1, 5, 2), tensor -> tensor[1:5:2] (ie indexing with [1, 3])[m
[32m+[m[32m    elif input_slice = [1, 4, 5], tensor -> tensor[[1, 4, 5]] (ie changing the first axis to have length 3, and taking the indices 1, 4, 5 out).[m
[32m+[m[32m    elif input_slice is a Tensor, same as list - Tensor is assumed to be a 1D list of indices.[m
     """[m
[31m-    SwiGLU activation function as described by[m
[31m-    https://arxiv.org/pdf/2002.05202.pdf.[m
[32m+[m[32m    def __init__([m
[32m+[m[32m        self,[m[41m [m
[32m+[m[32m        input_slice: SliceInput=None,[m
[32m+[m[32m        ):[m
[32m+[m[32m        if type(input_slice)==tuple:[m
[32m+[m[32m            input_slice = slice(*input_slice)[m
[32m+[m[32m            self.slice = input_slice[m
[32m+[m[32m            self.mode="slice"[m
[32m+[m[32m        elif type(input_slice)==int:[m
[32m+[m[32m            self.slice = input_slice[m
[32m+[m[32m            self.mode="int"[m
[32m+[m[32m        elif type(input_slice)==slice:[m
[32m+[m[32m            self.slice = input_slice[m
[32m+[m[32m            self.mode="slice"[m
[32m+[m[32m        elif type(input_slice)==list or type(input_slice)==torch.Tensor or type(input_slice)==np.ndarray:[m
[32m+[m[32m            self.slice = to_numpy(input_slice)[m
[32m+[m[32m            self.mode="array"[m
[32m+[m[32m        elif input_slice is None:[m
[32m+[m[32m            self.slice = slice(None)[m
[32m+[m[32m            self.mode="identity"[m
[32m+[m[32m        else:[m
[32m+[m[32m            raise ValueError(f"Invalid input_slice {input_slice}")[m
[32m+[m[41m    [m
[32m+[m[32m    def apply(self, tensor, dim=0):[m
[32m+[m[32m        """[m
[32m+[m[32m        Takes in a tensor and a slice, and applies the slice to the given dimension (supports positive and negative dimension syntax). Returns the sliced tensor.[m[41m [m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        ndim = tensor.ndim[m
[32m+[m[32m        slices = [slice(None)] * ndim[m
[32m+[m[32m        slices[dim] = self.slice[m
[32m+[m[32m        return tensor[tuple(slices)][m
[32m+[m[41m        [m
[32m+[m[32m    def indices(self, max_ctx=None):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        Returns the indices when this slice is applied to an axis of size max_ctx. Returns them as a numpy array, for integer slicing it is eg array([4])[m
[32m+[m[32m        """[m
[32m+[m[32m        if self.mode == "int":[m
[32m+[m[32m            return np.array([self.slice])[m
[32m+[m[32m        else:[m
[32m+[m[32m            return np.arange(max_ctx)[self.slice][m
[32m+[m[41m    [m
[32m+[m[32m    def __repr__(self):[m
[32m+[m[32m        return f"Slice: {self.slice} Mode: {self.mode} "[m
[32m+[m
[32m+[m
[32m+[m[32m# def apply_slice_to_dim([m
[32m+[m[32m#     tensor: torch.Tensor,[m
[32m+[m[32m#     input_slice: Union[Slice, SliceInput],[m
[32m+[m[32m#     dim: int=0,[m
[32m+[m[32m#     ):[m
[32m+[m[32m#     """Takes in a tensor and a slice, and applies the slice to the given dimension (supports positive and negative dimension syntax). Returns the sliced tensor.[m[41m [m
[32m+[m
[32m+[m[32m#     Note that slicing with input_slice=None means do nothing, NOT add an extra dimension (use unsqueeze for that)[m
[32m+[m[41m    [m
[32m+[m[32m#     We use a custom slice syntax because Python/Torch's don't let us reduce the number of dimensions:[m
[32m+[m
[32m+[m[32m#     Examples for dim=0:[m
[32m+[m[32m#     if input_slice=0, tensor -> tensor[0][m
[32m+[m[32m#     elif input_slice = (1, 5), tensor -> tensor[1:5][m
[32m+[m[32m#     elif input_slice = (1, 5, 2), tensor -> tensor[1:5:2] (ie indexing with [1, 3])[m
[32m+[m[32m#     elif input_slice = [1, 4, 5], tensor -> tensor[[1, 4, 5]] (ie changing the first axis to have length 3, and taking the indices 1, 4, 5 out).[m
[32m+[m[32m#     elif input_slice is a Tensor, same as list - Tensor is assumed to be a 1D list of indices.[m
[32m+[m[32m#     """[m
[32m+[m[32m#     ndim = tensor.ndim[m
[32m+[m[32m#     slices = [slice(None)] * ndim[m
[32m+[m[32m#     if isinstance(input_slice, tuple):[m
[32m+[m[32m#         input_slice = slice(*input_slice)[m
[32m+[m[32m#     elif input_slice is None:[m
[32m+[m[32m#         input_slice = slice(None)[m
[32m+[m[32m#     slices[dim] = input_slice[m
[32m+[m[32m#     return tensor[tuple(slices)][m
[32m+[m[32m# %%[m
[32m+[m
[32m+[m[32mdef act_name([m
[32m+[m[32m    name: str,[m
[32m+[m[32m    layer: Optional[int]=None,[m
[32m+[m[32m    layer_type: Optional[str]=None,[m
[32m+[m[32m    ):[m
[32m+[m[32m    """[m[41m [m
[32m+[m[32m    Helper function to convert shorthand to an activation name. Pretty hacky, intended to be useful for short feedback loop hacking stuff together, more so than writing good, readable code. But it is deterministic![m
[32m+[m
[32m+[m[32m    eg:[m
[32m+[m[32m    act_name('k', 6, 'a')=='blocks.6.attn.hook_k'[m
[32m+[m[32m    act_name('pre', 2)=='blocks.2.mlp.hook_pre'[m
[32m+[m[32m    act_name('embed')=='hook_embed'[m
[32m+[m[32m    act_name('normalized', 27, 'ln2')=='blocks.27.ln2.hook_normalized'[m
[32m+[m[32m    act_name('k6')=='blocks.6.attn.hook_k'[m
[32m+[m[32m    act_name('scale4ln1')=='blocks.4.ln1.hook_scale'[m
[32m+[m[32m    act_name('pre5')=='blocks.5.mlp.hook_pre'[m
     """[m
[31m-    return F.silu(gate) * input[m
[32m+[m[32m    match = re.match(r"([a-z]+)(\d+)([a-z]?.*)", name)[m
[32m+[m[32m    if match is not None:[m
[32m+[m[32m        name, layer, layer_type = match.groups(0)[m
[32m+[m
[32m+[m[32m    layer_type_dict = {'a':'attn', 'm':'mlp', 'b':'', 'block':'', 'blocks':'', 'attention':'attn'}[m
[32m+[m[32m    act_name = ""[m
[32m+[m[32m    if layer is not None:[m
[32m+[m[32m        act_name += f"blocks.{layer}."[m
[32m+[m[32m    if name in ['k', 'v', 'q', 'result', 'attn', 'attn_scores']:[m
[32m+[m[32m        layer_type='attn'[m
[32m+[m[32m    elif name in ['pre', 'post', 'mid']:[m
[32m+[m[32m        layer_type='mlp'[m
[32m+[m[32m    elif layer_type in layer_type_dict:[m
[32m+[m[32m        layer_type = layer_type_dict[layer_type][m
[32m+[m[41m    [m
[32m+[m[32m    if layer_type:[m
[32m+[m[32m        act_name += f"{layer_type}."[m
[32m+[m[32m    act_name += f"hook_{name}"[m
[32m+[m[32m    return act_name[m
[32m+[m[32m# %%[m
[32m+[m[32mdef transpose(tensor):[m
[32m+[m[32m    """[m[41m [m
[32m+[m[32m    Utility to swap the last two dimensions of a tensor, regardless of the number of leading dimensions[m
[32m+[m[32m    """[m
[32m+[m[32m    return tensor.transpose(-1, -2)[m
[32m+[m
[32m+[m[32mclass FactoredMatrix:[m
[32m+[m[32m    """[m[41m [m
[32m+[m[32m    Class to represent low rank factored matrices, where the matrix is represented as a product of two matrices. Has utilities for efficient calculation of eigenvalues, norm and SVD.[m[41m [m
[32m+[m[32m    """[m
[32m+[m[32m    def __init__(self, A, B):[m
[32m+[m[32m        self.A = A[m
[32m+[m[32m        self.B = B[m
[32m+[m[32m        assert self.A.size(-1)==self.B.size(-2), f"Factored matrix must match on inner dimension, shapes were a: {self.A.shape}, b:{self.B.shape}"[m
[32m+[m[32m        self.ldim = self.A.size(-2)[m
[32m+[m[32m        self.rdim = self.B.size(-1)[m
[32m+[m[32m        self.mdim = self.B.size(-2)[m
[32m+[m[32m        self.has_leading_dims = (self.A.ndim>2) or (self.B.ndim>2)[m
[32m+[m[32m        self.shape = torch.broadcast_shapes(self.A.shape[:-2], self.B.shape[:-2]) + (self.ldim, self.rdim)[m
[32m+[m[41m        [m
[32m+[m
[32m+[m[32m    def __matmul__(self, other):[m
[32m+[m[32m        if isinstance(other, torch.Tensor):[m
[32m+[m[32m            if other.ndim < 2:[m
[32m+[m[32m                # It's a vector, so we collapse the factorisation and just return a vector[m
[32m+[m[32m                # Squeezing/Unsqueezing is to preserve broadcasting working nicely[m
[32m+[m[32m                return (self.A @ (self.B @ other.unsqueeze(-1))).squeeze(-1)[m
[32m+[m[32m            else:[m
[32m+[m[32m                assert other.size(-2)==self.rdim, f"Right matrix must match on inner dimension, shapes were self: {self.shape}, other:{other.shape}"[m
[32m+[m[32m                if self.rdim > self.mdim:[m
[32m+[m[32m                    return FactoredMatrix(self.A, self.B @ other)[m
[32m+[m[32m                else:[m
[32m+[m[32m                    return FactoredMatrix(self.AB, other)[m
[32m+[m[32m        elif isinstance(other, FactoredMatrix):[m
[32m+[m[32m            return (self @ other.A) @ other.B[m
[32m+[m[41m    [m
[32m+[m[32m    def __rmatmul__(self, other):[m
[32m+[m[32m        if isinstance(other, torch.Tensor):[m
[32m+[m[32m            assert other.size(-1)==self.ldim, f"Left matrix must match on inner dimension, shapes were self: {self.shape}, other:{other.shape}"[m
[32m+[m[32m            if other.ndim < 2:[m
[32m+[m[32m                # It's a vector, so we collapse the factorisation and just return a vector[m
[32m+[m[32m                return ((other.unsqueeze(-2) @ self.A) @ self.B).squeeze(-1)[m
[32m+[m[32m            elif self.ldim > self.mdim:[m
[32m+[m[32m                return FactoredMatrix(other @ self.A, self.B)[m
[32m+[m[32m            else:[m
[32m+[m[32m                return FactoredMatrix(other, self.AB)[m
[32m+[m[32m        elif isinstance(other, FactoredMatrix):[m
[32m+[m[32m            return other.A @ (other.B @ self)[m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    def AB(self):[m
[32m+[m[32m        """ The product matrix - expensive to compute, and can consume a lot of GPU memory"""[m
[32m+[m[32m        return self.A @ self.B[m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    def BA(self):[m
[32m+[m[32m        """ The reverse product. Only makes sense when ldim==rdim"""[m
[32m+[m[32m        assert self.rdim==self.ldim, f"Can only take ba if ldim==rdim, shapes were self: {self.shape}"[m
[32m+[m[32m        return self.B @ self.A[m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    def T(self):[m
[32m+[m[32m        return FactoredMatrix(self.B.transpose(-2, -1), self.A.transpose(-2, -1))[m
[32m+[m[41m    [m
[32m+[m[32m    @lru_cache(maxsize=None)[m
[32m+[m[32m    def svd(self):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        Efficient algorithm for finding Singular Value Decomposition, a tuple (U, S, Vh) for matrix M st S is a vector and U, Vh are orthogonal matrices, and U @ S.diag() @ Vh == M[m
[32m+[m[32m        """[m
[32m+[m[32m        Ua, Sa, Vha = torch.svd(self.A)[m
[32m+[m[32m        Ub, Sb, Vhb = torch.svd(self.B)[m
[32m+[m[32m        middle = Sa[..., :, None] * transpose(Vha) @ Ub * Sb[..., None, :][m
[32m+[m[32m        Um, Sm, Vhm = torch.svd(middle)[m
[32m+[m[32m        U = Ua @ Um[m
[32m+[m[32m        Vh = Vhb @ Vhm[m
[32m+[m[32m        S = Sm[m
[32m+[m[32m        return U, S, Vh[m[41m [m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    def U(self):[m
[32m+[m[32m        return self.svd()[0][m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    def S(self):[m
[32m+[m[32m        return self.svd()[1][m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    def Vh(self):[m
[32m+[m[32m        return self.svd()[2][m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    def eigenvalues(self):[m
[32m+[m[32m        """ Eigenvalues of AB are the same as for BA (apart from trailing zeros), because if BAv=kv ABAv = A(BAv)=kAv, so Av is an eigenvector of AB with eigenvalue k. """[m
[32m+[m[32m        return torch.linalg.eig(self.BA).eigenvalues[m
[32m+[m[41m    [m
[32m+[m[32m    def __getitem__(self, idx):[m
[32m+[m[32m        """Indexing - assumed to only apply to the leading dimensions."""[m
[32m+[m[41m        [m
[32m+[m[32m        return FactoredMatrix(self.A[idx], self.B[idx])[m
[32m+[m[41m    [m
[32m+[m[32m    def norm(self):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        Frobenius norm is sqrt(sum of squared singular values)[m
[32m+[m[32m        """[m
[32m+[m[32m        return self.S.pow(2).sum(-1).sqrt()[m
[32m+[m[41m    [m
[32m+[m[32m    def __repr__(self):[m
[32m+[m[32m        return f"FactoredMatrix: Shape({self.shape}), Hidden Dim({self.mdim}), Norm({self.norm()})"[m
[32m+[m[41m    [m
[32m+[m[32m    def make_even(self):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        Returns the factored form of (U @ S.sqrt().diag(), S.sqrt().diag() @ Vh) where U, S, Vh are the SVD of the matrix. This is an equivalent factorisation, but more even - each half has half the singular values, and orthogonal rows/cols[m
[32m+[m[32m        """[m
[32m+[m[32m        return FactoredMatrix(self.U * self.S.sqrt()[..., None, :], self.S.sqrt()[..., :, None] * transpose(self.Vh))[m
[32m+[m[41m    [m
[32m+[m[32m    def get_corner(self, k=3):[m
[32m+[m[32m        return get_corner(self.A[..., :k, :] @ self.B[..., :, :k], k)[m
[32m+[m[41m    [m
[32m+[m[32m    @property[m
[32m+[m[32m    def ndim(self):[m
[32m+[m[32m        return len(self.shape)[m
[32m+[m[41m    [m
[32m+[m[32m    def collapse_l(self):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        Collapses the left side of the factorization by removing the orthogonal factor (given by self.U). Returns a (..., mdim, rdim) tensor[m
[32m+[m[32m        """[m
[32m+[m[32m        return self.S[..., :, None]*transpose(self.Vh)[m
[32m+[m[41m    [m
[32m+[m[32m    def collapse_r(self):[m
[32m+[m[32m        """[m[41m [m
[32m+[m[32m        Analogous to collapse_l, returns a (..., ldim, mdim) tensor[m
[32m+[m[32m        """[m
[32m+[m[32m        return self.U * self.S[..., None, :][m
[32m+[m[41m    [m
[32m+[m[32m    def unsqueeze(self, k):[m
[32m+[m[32m        return FactoredMatrix(self.A.unsqueeze(k), self.B.unsqueeze(k))[m
[32m+[m
[32m+[m[32mdef composition_scores(left: FactoredMatrix, right: FactoredMatrix, broadcast_dims=True):[m
[32m+[m[32m    """[m
[32m+[m[32m    See `EasyTransformer.all_composition_scores` for documentation[m
[32m+[m[32m    """[m
[32m+[m[32m    if broadcast_dims:[m
[32m+[m[32m        r_leading = right.ndim-2[m
[32m+[m[32m        l_leading = left.ndim-2[m
[32m+[m[32m        for i in range(l_leading):[m
[32m+[m[32m            right = right.unsqueeze(i)[m
[32m+[m[32m        for i in range(r_leading):[m
[32m+[m[32m            left = left.unsqueeze(i+l_leading)[m
[32m+[m[32m    assert left.rdim==right.ldim, f"Composition scores require left.rdim==right.ldim, shapes were left: {left.shape}, right:{right.shape}"[m
[32m+[m
[32m+[m[32m    right = right.collapse_r()[m
[32m+[m[32m    left = left.collapse_l()[m
[32m+[m[32m    r_norms = right.norm(dim=[-2, -1])[m
[32m+[m[32m    l_norms = left.norm(dim=[-2, -1])[m
[32m+[m[32m    comp_norms = (left @ right).norm(dim=[-2, -1])[m
[32m+[m[32m    return comp_norms/r_norms/l_norms[m
[32m+[m
[32m+[m[32m# %%[m
[1mdiff --git a/easy_transformer/weight_conversion.py b/easy_transformer/weight_conversion.py[m
[1mnew file mode 100644[m
[1mindex 0000000..7572d4b[m
[1m--- /dev/null[m
[1m+++ b/easy_transformer/weight_conversion.py[m
[36m@@ -0,0 +1,359 @@[m
[32m+[m[32mfrom easy_transformer import EasyTransformerConfig[m
[32m+[m[32mimport einops[m
[32m+[m[32mimport torch[m
[32m+[m
[32m+[m[32mVALID_PRETRAINED_MODEL_NAMES = set([m
[32m+[m[32m    [[m
[32m+[m[32m        "gpt2", # Alias for GPT-2 Small[m
[32m+[m[32m        "gpt2-small",[m
[32m+[m[32m        "gpt2-medium",[m
[32m+[m[32m        "gpt2-large",[m
[32m+[m[32m        "gpt2-xl",[m
[32m+[m[32m        "facebook/opt-125m",[m
[32m+[m[32m        "facebook/opt-1.3b",[m
[32m+[m[32m        "facebook/opt-2.7b",[m
[32m+[m[32m        "facebook/opt-6.7b",[m
[32m+[m[32m        "facebook/opt-13b",[m
[32m+[m[32m        "facebook/opt-30b",[m
[32m+[m[32m        "facebook/opt-66b",[m
[32m+[m[32m        "EleutherAI/gpt-neo-125M",[m
[32m+[m[32m        "EleutherAI/gpt-neo-1.3B",[m
[32m+[m[32m        "EleutherAI/gpt-neo-2.7B",[m
[32m+[m[32m        "stanford-gpt2-small-A",[m
[32m+[m[32m        "stanford-gpt2-small-B",[m
[32m+[m[32m        "stanford-gpt2-small-C",[m
[32m+[m[32m        "stanford-gpt2-small-D",[m
[32m+[m[32m        "stanford-gpt2-small-E",[m
[32m+[m[32m        "stanford-gpt2-medium-A",[m
[32m+[m[32m        "stanford-gpt2-medium-B",[m
[32m+[m[32m        "stanford-gpt2-medium-C",[m
[32m+[m[32m        "stanford-gpt2-medium-D",[m
[32m+[m[32m        "stanford-gpt2-medium-E",[m
[32m+[m[32m        "solu-1l-old",[m
[32m+[m[32m        "solu-2l-old",[m
[32m+[m[32m        "solu-4l-old",[m
[32m+[m[32m        "solu-6l-old",[m
[32m+[m[32m        "solu-8l-old",[m
[32m+[m[32m        "solu-10l-old",[m
[32m+[m[32m        "solu-1l-c4-code",[m
[32m+[m[32m        "solu-2l-c4-code",[m
[32m+[m[32m        "solu-3l-c4-code",[m
[32m+[m[32m        "solu-4l-c4-code",[m
[32m+[m[32m        "attn-only-2l-induction-demo",[m
[32m+[m[32m        "EleutherAI/gpt-j-6B",[m
[32m+[m[32m        "EleutherAI/gpt-neox-20b",[m
[32m+[m[32m        "EleutherAI/pythia-19m",[m
[32m+[m[32m        "EleutherAI/pythia-125m",[m
[32m+[m[32m        "EleutherAI/pythia-350m",[m
[32m+[m[32m        "EleutherAI/pythia-800m",[m
[32m+[m[32m        "EleutherAI/pythia-1.3b",[m
[32m+[m[32m        "EleutherAI/pythia-6.7b",[m
[32m+[m[32m        "EleutherAI/pythia-13b",[m
[32m+[m[32m        "EleutherAI/pythia-125m-deduped",[m
[32m+[m[32m        "EleutherAI/pythia-800m-deduped",[m
[32m+[m[32m        "EleutherAI/pythia-1.3b-deduped",[m
[32m+[m[32m        "EleutherAI/pythia-6.7b-deduped",[m
[32m+[m[32m        "EleutherAI/pythia-13b-deduped",[m
[32m+[m[32m    ][m
[32m+[m[32m)[m
[32m+[m
[32m+[m[32m# Maps things to their official HuggingFace name[m
[32m+[m[32mPRETRAINED_MODEL_NAMES_DICT = {[m
[32m+[m[32m    "stanford-gpt2-small-A": "stanford-crfm/alias-gpt2-small-x21",[m
[32m+[m[32m    "stanford-gpt2-small-B": "stanford-crfm/battlestar-gpt2-small-x49",[m
[32m+[m[32m    "stanford-gpt2-small-C": "stanford-crfm/caprica-gpt2-small-x81",[m
[32m+[m[32m    "stanford-gpt2-small-D": "stanford-crfm/darkmatter-gpt2-small-x343",[m
[32m+[m[32m    "stanford-gpt2-small-E": "stanford-crfm/expanse-gpt2-small-x777",[m
[32m+[m[32m    "stanford-gpt2-medium-A": "stanford-crfm/arwen-gpt2-medium-x21",[m
[32m+[m[32m    "stanford-gpt2-medium-B": "stanford-crfm/beren-gpt2-medium-x49",[m
[32m+[m[32m    "stanford-gpt2-medium-C": "stanford-crfm/celebrimbor-gpt2-medium-x81",[m
[32m+[m[32m    "stanford-gpt2-medium-D": "stanford-crfm/durin-gpt2-medium-x343",[m
[32m+[m[32m    "stanford-gpt2-medium-E": "stanford-crfm/eowyn-gpt2-medium-x777",[m
[32m+[m[32m    "gpt2-small": "gpt2",[m
[32m+[m[32m}[m
[32m+[m[32m# The steps for which there are checkpoints in the stanford crfm models - provided as reference[m
[32m+[m[32mSTANFORD_CRFM_CHECKPOINTS = ([m
[32m+[m[32m    list(range(0, 100, 10))[m
[32m+[m[32m    + list(range(100, 2000, 50))[m
[32m+[m[32m    + list(range(2000, 20000, 100))[m
[32m+[m[32m    + list(range(20000, 400000 + 1, 1000))[m
[32m+[m[32m)[m
[32m+[m
[32m+[m[32mdef convert_gpt2_weights(gpt2, cfg: EasyTransformerConfig):[m
[32m+[m[32m    state_dict = {}[m
[32m+[m
[32m+[m[32m    state_dict["embed.W_E"] = gpt2.transformer.wte.weight[m
[32m+[m[32m    state_dict["pos_embed.W_pos"] = gpt2.transformer.wpe.weight[m
[32m+[m
[32m+[m[32m    for l in range(cfg.n_layers):[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln1.w"] = gpt2.transformer.h[l].ln_1.weight[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln1.b"] = gpt2.transformer.h[l].ln_1.bias[m
[32m+[m[41m        [m
[32m+[m[32m        # In GPT-2, q,k,v are produced by one big linear map, whose output is[m
[32m+[m[32m        # concat([q, k, v])[m
[32m+[m[32m        W = gpt2.transformer.h[l].attn.c_attn.weight[m
[32m+[m[32m        W_Q, W_K, W_V = torch.tensor_split(W, 3, dim=1)[m
[32m+[m[32m        W_Q = einops.rearrange(W_Q, "m (i h)->i m h", i=cfg.n_heads)[m
[32m+[m[32m        W_K = einops.rearrange(W_K, "m (i h)->i m h", i=cfg.n_heads)[m
[32m+[m[32m        W_V = einops.rearrange(W_V, "m (i h)->i m h", i=cfg.n_heads)[m
[32m+[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_K"] = W_K[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_V"] = W_V[m
[32m+[m
[32m+[m[32m        qkv_bias = gpt2.transformer.h[l].attn.c_attn.bias[m
[32m+[m[32m        qkv_bias = einops.rearrange([m
[32m+[m[32m            qkv_bias,[m
[32m+[m[32m            "(qkv index head)->qkv index head",[m
[32m+[m[32m            qkv=3,[m
[32m+[m[32m            index=cfg.n_heads,[m
[32m+[m[32m            head=cfg.d_head,[m
[32m+[m[32m        )[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_Q"] = qkv_bias[0][m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_K"] = qkv_bias[1][m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_V"] = qkv_bias[2][m
[32m+[m
[32m+[m[32m        W_O = gpt2.transformer.h[l].attn.c_proj.weight[m
[32m+[m[32m        W_O = einops.rearrange(W_O, "(i h) m->i h m", i=cfg.n_heads)[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_O"] = W_O[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_O"] = gpt2.transformer.h[l].attn.c_proj.bias[m
[32m+[m
[32m+[m[41m        [m
[32m+[m[32m        state_dict[f"blocks.{l}.ln2.w"] = gpt2.transformer.h[l].ln_2.weight[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln2.b"] = gpt2.transformer.h[l].ln_2.bias[m
[32m+[m[41m        [m
[32m+[m[32m        W_in = gpt2.transformer.h[l].mlp.c_fc.weight[m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.W_in"] = W_in[m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.b_in"] = gpt2.transformer.h[l].mlp.c_fc.bias[m
[32m+[m[41m        [m
[32m+[m[32m        W_out = gpt2.transformer.h[l].mlp.c_proj.weight[m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.W_out"] = W_out[m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.b_out"] = gpt2.transformer.h[l].mlp.c_proj.bias[m
[32m+[m[32m    state_dict[f"unembed.W_U"] = gpt2.lm_head.weight.T[m
[32m+[m[41m    [m
[32m+[m[32m    state_dict["ln_final.w"] = gpt2.transformer.ln_f.weight[m
[32m+[m[32m    state_dict["ln_final.b"] = gpt2.transformer.ln_f.bias[m
[32m+[m[32m    return state_dict[m
[32m+[m
[32m+[m[32mdef convert_neo_weights(neo, cfg: EasyTransformerConfig):[m
[32m+[m[32m    state_dict = {}[m
[32m+[m
[32m+[m[32m    state_dict["embed.W_E"] = neo.transformer.wte.weight[m
[32m+[m[32m    state_dict["pos_embed.W_pos"] = neo.transformer.wpe.weight[m
[32m+[m
[32m+[m[32m    for l in range(cfg.n_layers):[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln1.w"] = neo.transformer.h[l].ln_1.weight[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln1.b"] = neo.transformer.h[l].ln_1.bias[m
[32m+[m[41m        [m
[32m+[m[32m        W_Q = neo.transformer.h[l].attn.attention.q_proj.weight[m
[32m+[m[32m        W_K = neo.transformer.h[l].attn.attention.k_proj.weight[m
[32m+[m[32m        W_V = neo.transformer.h[l].attn.attention.v_proj.weight[m
[32m+[m[32m        W_Q = einops.rearrange(W_Q, "(i h) m->i m h", i=cfg.n_heads)[m
[32m+[m[32m        W_K = einops.rearrange(W_K, "(i h) m->i m h", i=cfg.n_heads)[m
[32m+[m[32m        W_V = einops.rearrange(W_V, "(i h) m->i m h", i=cfg.n_heads)[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_K"] = W_K[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_V"] = W_V[m
[32m+[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_Q"] = torch.zeros(cfg.n_heads, cfg.d_head)[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_K"] = torch.zeros(cfg.n_heads, cfg.d_head)[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_V"] = torch.zeros(cfg.n_heads, cfg.d_head)[m
[32m+[m
[32m+[m[32m        W_O = neo.transformer.h[l].attn.attention.out_proj.weight[m
[32m+[m[32m        W_O = einops.rearrange(W_O, "m (i h)->i h m", i=cfg.n_heads)[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_O"] = W_O[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_O"] = neo.transformer.h[[m
[32m+[m[32m            l[m
[32m+[m[32m        ].attn.attention.out_proj.bias[m
[32m+[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln2.w"] = neo.transformer.h[l].ln_2.weight[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln2.b"] = neo.transformer.h[l].ln_2.bias[m
[32m+[m[41m        [m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.W_in"] = neo.transformer.h[l].mlp.c_fc.weight.T[m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.b_in"] = neo.transformer.h[l].mlp.c_fc.bias[m
[32m+[m[41m        [m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.W_out"] = neo.transformer.h[l].mlp.c_proj.weight.T[m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.b_out"] = neo.transformer.h[l].mlp.c_proj.bias[m
[32m+[m[32m    state_dict["ln_final.w"] = neo.transformer.ln_f.weight[m
[32m+[m[32m    state_dict["ln_final.b"] = neo.transformer.ln_f.bias[m
[32m+[m[41m    [m
[32m+[m[32m    state_dict["unembed.W_U"] = neo.lm_head.weight.T[m
[32m+[m[32m    state_dict["unembed.b_U"] = torch.zeros(cfg.d_vocab)[m
[32m+[m[32m    return state_dict[m
[32m+[m
[32m+[m[32mdef convert_gptj_weights(gptj, cfg: EasyTransformerConfig):[m
[32m+[m[32m    state_dict = {}[m
[32m+[m
[32m+[m[32m    state_dict["embed.W_E"] = gptj.transformer.wte.weight[m
[32m+[m
[32m+[m[32m    for l in range(cfg.n_layers):[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln1.w"] = gptj.transformer.h[l].ln_1.weight[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln1.b"] = gptj.transformer.h[l].ln_1.bias[m
[32m+[m[41m        [m
[32m+[m[32m        W_Q = gptj.transformer.h[l].attn.q_proj.weight[m
[32m+[m[32m        W_K = gptj.transformer.h[l].attn.k_proj.weight[m
[32m+[m[32m        W_V = gptj.transformer.h[l].attn.v_proj.weight[m
[32m+[m[32m        W_Q = einops.rearrange(W_Q, "(i h) m->i m h", i=cfg.n_heads)[m
[32m+[m[32m        W_K = einops.rearrange(W_K, "(i h) m->i m h", i=cfg.n_heads)[m
[32m+[m[32m        W_V = einops.rearrange(W_V, "(i h) m->i m h", i=cfg.n_heads)[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_K"] = W_K[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_V"] = W_V[m
[32m+[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_Q"] = torch.zeros(cfg.n_heads, cfg.d_head)[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_K"] = torch.zeros(cfg.n_heads, cfg.d_head)[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_V"] = torch.zeros(cfg.n_heads, cfg.d_head)[m
[32m+[m
[32m+[m[32m        W_O = gptj.transformer.h[l].attn.out_proj.weight[m
[32m+[m[32m        W_O = einops.rearrange(W_O, "m (i h)->i h m", i=cfg.n_heads)[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_O"] = W_O[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_O"] = torch.zeros(cfg.d_model)[m
[32m+[m[41m        [m
[32m+[m[32m        # Layer Norm 1 and 2 are tied.[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln2.w"] = state_dict[f"blocks.{l}.ln1.w"][m
[32m+[m[32m        state_dict[f"blocks.{l}.ln2.b"] = state_dict[f"blocks.{l}.ln1.b"][m
[32m+[m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.W_in"] = gptj.transformer.h[l].mlp.fc_in.weight.T[m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.b_in"] = gptj.transformer.h[l].mlp.fc_in.bias[m
[32m+[m[41m        [m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.W_out"] = gptj.transformer.h[l].mlp.fc_out.weight.T[m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.b_out"] = gptj.transformer.h[l].mlp.fc_out.bias[m
[32m+[m[32m    state_dict["ln_final.w"] = gptj.transformer.ln_f.weight[m
[32m+[m[32m    state_dict["ln_final.b"] = gptj.transformer.ln_f.bias[m
[32m+[m[41m    [m
[32m+[m[32m    state_dict["unembed.W_U"] = gptj.lm_head.weight.T[m
[32m+[m[32m    # Contains a bias, for some reason?[m
[32m+[m[32m    state_dict["unembed.b_U"] = gptj.lm_head.bias[m
[32m+[m[32m    return state_dict[m
[32m+[m
[32m+[m[32mdef convert_neox_weights(neox, cfg: EasyTransformerConfig):[m
[32m+[m[32m    state_dict = {}[m
[32m+[m
[32m+[m[32m    state_dict["embed.W_E"] = neox.gpt_neox.embed_in.weight[m
[32m+[m
[32m+[m[32m    for l in range(cfg.n_layers):[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln1.w"] = neox.gpt_neox.layers[l].input_layernorm.weight[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln1.b"] = neox.gpt_neox.layers[l].input_layernorm.bias[m
[32m+[m
[32m+[m[32m        # For some inexplicable reason, NeoX both uses the concatenated QKV matmul of GPT-2 (afaict this has a neglible performance impact) AND has the flattened axis in the DIFFERENT order of (head_index qkv d_head) - this took me an hour to debug...[m
[32m+[m[32m        W = neox.gpt_neox.layers[l].attention.query_key_value.weight[m
[32m+[m[32m        W = einops.rearrange(W, "(i qkv h) m->qkv i m h", i=cfg.n_heads, qkv=3)[m
[32m+[m
[32m+[m[32m        # Fold in layer norm weights[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_Q"] = W[0][m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_K"] = W[1][m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_V"] = W[2][m
[32m+[m
[32m+[m[32m        qkv_bias = neox.gpt_neox.layers[l].attention.query_key_value.bias[m
[32m+[m[32m        qkv_bias = einops.rearrange([m
[32m+[m[32m            qkv_bias,[m
[32m+[m[32m            "(index qkv head)->qkv index head",[m
[32m+[m[32m            qkv=3,[m
[32m+[m[32m            index=cfg.n_heads,[m
[32m+[m[32m            head=cfg.d_head,[m
[32m+[m[32m        )[m
[32m+[m[32m        # Fold in layer norm biases[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_Q"] = qkv_bias[0][m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_K"] = qkv_bias[1][m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_V"] = qkv_bias[2][m
[32m+[m[41m        [m
[32m+[m[32m        W_O = neox.gpt_neox.layers[l].attention.dense.weight[m
[32m+[m[32m        W_O = einops.rearrange(W_O, "m (i h)->i h m", i=cfg.n_heads)[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_O"] = W_O[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_O"] = neox.gpt_neox.layers[l].attention.dense.bias[m
[32m+[m[41m        [m
[32m+[m[32m        state_dict[f"blocks.{l}.ln2.w"] = neox.gpt_neox.layers[l].post_attention_layernorm.weight[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln2.b"] = neox.gpt_neox.layers[l].post_attention_layernorm.bias[m
[32m+[m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.W_in"] = neox.gpt_neox.layers[l].mlp.dense_h_to_4h.weight.T[m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.b_in"] = neox.gpt_neox.layers[l].mlp.dense_h_to_4h.bias[m
[32m+[m[41m        [m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.W_out"] = neox.gpt_neox.layers[l].mlp.dense_4h_to_h.weight.T[m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.b_out"] = neox.gpt_neox.layers[l].mlp.dense_4h_to_h.bias[m
[32m+[m[32m    state_dict["ln_final.w"] = neox.gpt_neox.final_layer_norm.weight[m
[32m+[m[32m    state_dict["ln_final.b"] = neox.gpt_neox.final_layer_norm.bias[m
[32m+[m[41m    [m
[32m+[m[32m    state_dict["unembed.W_U"] = neox.embed_out.weight.T[m
[32m+[m[32m    state_dict["unembed.b_U"] = torch.zeros(cfg.d_vocab)[m
[32m+[m[32m    return state_dict[m
[32m+[m
[32m+[m[32mdef convert_opt_weights(opt, cfg: EasyTransformerConfig):[m
[32m+[m[32m    state_dict = {}[m
[32m+[m
[32m+[m[32m    state_dict["embed.W_E"] = opt.model.decoder.embed_tokens.weight[m
[32m+[m[32m    state_dict["pos_embed.W_pos"] = opt.model.decoder.embed_positions.weight[2:, :][m
[32m+[m
[32m+[m[32m    for l in range(cfg.n_layers):[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln1.w"] = opt.model.decoder.layers[l].self_attn_layer_norm.weight[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln1.b"] = opt.model.decoder.layers[l].self_attn_layer_norm.bias[m
[32m+[m[41m        [m
[32m+[m[32m        W_Q = opt.model.decoder.layers[l].self_attn.q_proj.weight[m
[32m+[m[32m        W_K = opt.model.decoder.layers[l].self_attn.k_proj.weight[m
[32m+[m[32m        W_V = opt.model.decoder.layers[l].self_attn.v_proj.weight[m
[32m+[m[32m        W_Q = einops.rearrange([m
[32m+[m[32m            W_Q,[m
[32m+[m[32m            "(index d_head) d_model->index d_model d_head",[m
[32m+[m[32m            index=cfg.n_heads,[m
[32m+[m[32m        )[m
[32m+[m[32m        W_K = einops.rearrange([m
[32m+[m[32m            W_K,[m
[32m+[m[32m            "(index d_head) d_model->index d_model d_head",[m
[32m+[m[32m            index=cfg.n_heads,[m
[32m+[m[32m        )[m
[32m+[m[32m        W_V = einops.rearrange([m
[32m+[m[32m            W_V,[m
[32m+[m[32m            "(index d_head) d_model->index d_model d_head",[m
[32m+[m[32m            index=cfg.n_heads,[m
[32m+[m[32m        )[m
[32m+[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_K"] = W_K[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_V"] = W_V[m
[32m+[m
[32m+[m[32m        q_bias = einops.rearrange([m
[32m+[m[32m            opt.model.decoder.layers[l].self_attn.q_proj.bias,[m
[32m+[m[32m            "(head_index d_head)->head_index d_head",[m
[32m+[m[32m            head_index=cfg.n_heads,[m
[32m+[m[32m            d_head=cfg.d_head,[m
[32m+[m[32m        )[m
[32m+[m[32m        k_bias = einops.rearrange([m
[32m+[m[32m            opt.model.decoder.layers[l].self_attn.k_proj.bias,[m
[32m+[m[32m            "(head_index d_head)->head_index d_head",[m
[32m+[m[32m            head_index=cfg.n_heads,[m
[32m+[m[32m            d_head=cfg.d_head,[m
[32m+[m[32m        )[m
[32m+[m[32m        v_bias = einops.rearrange([m
[32m+[m[32m            opt.model.decoder.layers[l].self_attn.v_proj.bias,[m
[32m+[m[32m            "(head_index d_head)->head_index d_head",[m
[32m+[m[32m            head_index=cfg.n_heads,[m
[32m+[m[32m            d_head=cfg.d_head,[m
[32m+[m[32m        )[m
[32m+[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_Q"] = q_bias[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_K"] = k_bias[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_V"] = v_bias[m
[32m+[m
[32m+[m[32m        W_O = opt.model.decoder.layers[l].self_attn.out_proj.weight[m
[32m+[m[32m        W_O = einops.rearrange([m
[32m+[m[32m            W_O,[m
[32m+[m[32m            "d_model (index d_head)->index d_head d_model",[m
[32m+[m[32m            index=cfg.n_heads,[m
[32m+[m[32m        )[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.W_O"] = W_O[m
[32m+[m[32m        state_dict[f"blocks.{l}.attn.b_O"] = opt.model.decoder.layers[[m
[32m+[m[32m            l[m
[32m+[m[32m        ].self_attn.out_proj.bias[m
[32m+[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln2.w"] = opt.model.decoder.layers[l].final_layer_norm.weight[m
[32m+[m[32m        state_dict[f"blocks.{l}.ln2.b"] = opt.model.decoder.layers[l].final_layer_norm.bias[m
[32m+[m[41m        [m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.W_in"] = opt.model.decoder.layers[l].fc1.weight.T[m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.W_out"] = opt.model.decoder.layers[l].fc2.weight.T[m
[32m+[m[41m        [m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.b_in"] = opt.model.decoder.layers[l].fc1.bias[m
[32m+[m[32m        state_dict[f"blocks.{l}.mlp.b_out"] = opt.model.decoder.layers[l].fc2.bias[m
[32m+[m[32m    state_dict[f"ln_final.w"] = opt.model.decoder.final_layer_norm.weight[m
[32m+[m[32m    state_dict[f"ln_final.b"] = opt.model.decoder.final_layer_norm.bias[m
[32m+[m[32m    state_dict["unembed.W_U"] = opt.lm_head.weight.T[m
[32m+[m[32m    state_dict["unembed.b_U"] = torch.zeros(cfg.d_vocab)[m
[32m+[m[32m    return state_dict[m
\ No newline at end of file[m
[1mdiff --git a/further_comments.md b/further_comments.md[m
[1mnew file mode 100644[m
[1mindex 0000000..4aea4b3[m
[1m--- /dev/null[m
[1m+++ b/further_comments.md[m
[36m@@ -0,0 +1,47 @@[m
[32m+[m[32m# Further Details on Config Options[m
[32m+[m[32m    ## Shortformer Attention (`positional_embeddings_type == "shortformer"`)[m
[32m+[m[32m        Shortformer style models are a variant on GPT-2 style positional embeddings, which do not add positional embeddings into the residual stream but instead add it in to the queries and keys immediately before multiplying by W_Q and W_K, and NOT having it around for the values or MLPs. It's otherwise the same - the positional embeddings are absolute, and are learned. The positional embeddings are added to the residual stream in the standard way, and then the queries and keys are calculated as W_Q(res_stream + pos_embed) and W_K(res_stream + pos_embed). The values and MLPs are calculated as W_V(res_stream) and W_MLP(res_stream). This is the same as GPT-2 style positional embeddings, except that the positional embeddings are not added to the residual stream, but instead are only added to the queries and keys. This is a variant on the Shortformer model from the paper [Shortformer: The Benefits of Shorter Sequences in Language Modeling](https://arxiv.org/abs/2101.08692).[m
[32m+[m
[32m+[m[32m        The original intention was to use this to do more efficient caching: caching is hard with absolute positional embeddings, since you can't translate the context window without recomputing the entire thing, but easier if the prior values and residual stream terms are the same. I've mostly implemented it because it makes it easier for models to form induction heads. I'm not entirely sure why, though hypothesise that it's because there's two ways for induction heads to form with positional embeddings in the residual stream and only one with shortformer style positional embeddings.[m
[32m+[m[41m    [m
[32m+[m[32m    ## What is LayerNorm Folding? (`fold_ln`)[m
[32m+[m[32m        [LayerNorm](https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1) is a common regularisation technique used in transformers. Annoyingly, unlike eg BatchNorm, it can't be turned off at inference time, it's a meaningful change to the mathematical function implemented by the transformer. From an interpretability perspective, this is a headache! And it's easy to shoot yourself in the foot by naively ignoring it - eg, making the mistake of saying neuron_pre = resid_mid @ W_in, rather than LayerNorm(resid_mid) @ W_in. This mistake is an OK approximation, but by folding in the LayerNorm we can do much better![m
[32m+[m
[32m+[m[32m        TLDR: If we have LayerNorm (weights w_ln and b_ln) followed by a linear layer (W+b), we can reduce the LayerNorm to LayerNormPre (just centering & normalising) and follow it by a linear layer with `W_eff = w[:, None] * W` (element-wise multiplication) and `b_eff = b + b_ln @ W`. This is computationally equivalent, and it never makes sense to think of W and w_ln as separate objects, so EasyTransformer handles it for you when loading pre-trained weights - set fold_ln = False when loading a state dict if you want to turn this off[m
[32m+[m[41m        [m
[32m+[m[32m        Mathematically, LayerNorm is the following:[m
[32m+[m[32m        x1 = x0 - x0.mean()[m
[32m+[m[32m        x2 = x1 / ((x1**2).mean()).sqrt()[m
[32m+[m[32m        x3 = x2 * w[m
[32m+[m[32m        x4 = x3 + b[m
[32m+[m[41m        [m
[32m+[m[32m        Apart from dividing by the norm, these are all pretty straightforwards operations from a linear algebra perspective. And from an interpretability perspective, if anything is linear, it's really easy and you can mostly ignore it (everything breaks up into sums, you can freely change basis, don't need to track interference between terms, etc) - the hard part is engaging with non-linearities![m
[32m+[m[41m        [m
[32m+[m[32m        A key thing to bear in mind is that EVERY time we read from the residual stream, we apply a LayerNorm - this gives us a lot of leverage to reason about it![m
[32m+[m[41m        [m
[32m+[m[32m        So let's translate this into linear algebra notation.[m
[32m+[m[32m        x0 is a vector in R^n[m
[32m+[m[41m        [m
[32m+[m[32m        x1 = x0 - x0.mean()[m[41m [m
[32m+[m[32m            = x0 - (x0.mean()) * ones (broadcasting, ones=torch.ones(n))[m
[32m+[m[32m            = (x0 @ ones/sqrt(n)) * ones/sqrt(n). ones has norm sqrt(n), so ones/sqrt(n) is the unit vector in the diagonal direction. We're just projecting x0 onto this (fixed) vector and subtracting that value off. Alternately, we're projecting onto the n-1 dimensional subspace orthogonal to ones.[m
[32m+[m[41m            [m
[32m+[m[32m            Since LayerNorm is applied EVERY time we read from the stream, the model just never uses the ones direction of the residual stream, so it's essentially just decreasing d_model by one. We can simulate this by just centering all matrices writing to the residual stream.[m
[32m+[m[32m            Why is removing this dimension useful? I have no idea! I'm not convinced it is...[m
[32m+[m[41m        [m
[32m+[m[32m        x2 = x1 / ((x1**2).mean()).sqrt() (Ignoring eps)[m
[32m+[m[32m           = (x1 / x1.norm()) * sqrt(n)[m
[32m+[m[32m           This is a projection onto the unit sphere (well, sphere of radius sqrt(n) - the norm of ones). This is fundamentally non-linear, eg doubling the input keeps the output exactly the same.[m[41m  [m
[32m+[m[32m           This is by far the most irritating part of LayerNorm. I THINK it's mostly useful for numerical stability reasons and not used to do useful computation by the model, but I could easily be wrong! And interpreting a circuit containing LayerNorm sounds like a nightmare...[m
[32m+[m[32m           In practice, you can mostly get aware with ignore this and treating the scaling factor as a constant, since it does apply across the entire residual stream for each token - this makes it a "global" property of the model's calculation, so for any specific question it hopefully doesn't matter that much. But when you're considering a sufficiently important circuit that it's a good fraction of the norm of the residual stream, it's probably worth thinking about.[m
[32m+[m[41m        [m
[32m+[m[32m        x3 = x2 * w[m
[32m+[m[32m           = x2 @ W_ln (W_ln is a diagonal matrix with the weights of the LayerNorm - this is equivalent to element-wise multiplication)[m
[32m+[m[32m           This is really easy to deal with - we're about to be input to a linear layer, and can say (x2 @ W_ln) @ W = x2 @ (W_ln @ W) = x2 @ W_eff - we can just fold the LayerNorm weights into the linear layer weights.[m
[32m+[m[41m        [m
[32m+[m[32m        x4 = x3 + b is similarly easy - x4 @ W + B = x2 @ W_eff + B_eff, where W_eff = W_ln @ W and B_eff = B + b @ W[m
[32m+[m[41m        [m
[32m+[m[32m        This function is calculating W_eff and B_eff for each layer reading from the residual stream and replacing W and B with those[m
[32m+[m[41m        [m
[32m+[m[41m        [m
[32m+[m[32m        See this for more: https://transformer-circuits.pub/2021/framework/index.html#:~:text=Handling%20Layer%20Normalization[m
\ No newline at end of file[m
[1mdiff --git a/ioi_circuit_extraction.py b/ioi_circuit_extraction.py[m
[1mindex 45c3c30..88fc837 100644[m
[1m--- a/ioi_circuit_extraction.py[m
[1m+++ b/ioi_circuit_extraction.py[m
[36m@@ -307,7 +307,7 @@[m [mdef do_circuit_extraction([m
     mean_dataset=None,[m
     model=None,[m
     metric=None,[m
[31m-    exclude_heads=[],[m
[32m+[m[32m    excluded=[],  # tuple of (layer, head) or (layer, None for MLPs)[m
     return_hooks=False,[m
     hooks_dict=False,[m
 ):[m
[36m@@ -362,7 +362,7 @@[m [mdef do_circuit_extraction([m
     heads_keys.sort(key=lambda x: (x[0], x[1]))[m
 [m
     for layer, head in heads_keys:[m
[31m-        if (layer, head) in exclude_heads:[m
[32m+[m[32m        if (layer, head) in excluded:[m
             continue[m
         assert (layer, head) not in hooks, ((layer, head), "already in hooks")[m
         hooks[(layer, head)] = abl.get_hook(layer, head)[m
[36m@@ -381,7 +381,3 @@[m [mdef do_circuit_extraction([m
         for hook in hooks.values():[m
             model.add_hook(*hook)[m
         return model, abl[m
[31m-[m
[31m-[m
[31m-if __name__ == "__main__":[m
[31m-    print(CIRCUIT)[m
[1mdiff --git a/ioi_dataset.py b/ioi_dataset.py[m
[1mindex 138b0f4..3797a2e 100644[m
[1m--- a/ioi_dataset.py[m
[1m+++ b/ioi_dataset.py[m
[36m@@ -1,4 +1,5 @@[m
 import io[m
[32m+[m[32mfrom logging import warning[m
 from typing import Union, List[m
 from site import PREFIXES[m
 import warnings[m
[36m@@ -145,6 +146,24 @@[m [mBABA_TEMPLATES = [[m
     "Friends [B] and [A] found a [OBJECT] at the [PLACE]. [B] gave it to [A]",[m
 ][m
 [m
[32m+[m[32mBABA_LONG_TEMPLATES = [[m
[32m+[m[32m    "Then in the morning, [B] and [A] went to the [PLACE]. [B] gave a [OBJECT] to [A]",[m
[32m+[m[32m    "Then in the morning, [B] and [A] had a lot of fun at the [PLACE]. [B] gave a [OBJECT] to [A]",[m
[32m+[m[32m    "Then in the morning, [B] and [A] were working at the [PLACE]. [B] decided to give a [OBJECT] to [A]",[m
[32m+[m[32m    "Then in the morning, [B] and [A] were thinking about going to the [PLACE]. [B] wanted to give a [OBJECT] to [A]",[m
[32m+[m[32m    "Then in the morning, [B] and [A] had a long argument, and afterwards [B] said to [A]",[m
[32m+[m[32m    "After taking a long break [B] and [A] went to the [PLACE], [B] gave a [OBJECT] to [A]",[m
[32m+[m[32m    "When soon afterwards [B] and [A] got a [OBJECT] at the [PLACE], [B] decided to give it to [A]",[m
[32m+[m[32m    "When soon afterwards [B] and [A] got a [OBJECT] at the [PLACE], [B] decided to give the [OBJECT] to [A]",[m
[32m+[m[32m    "While spending time together [B] and [A] were working at the [PLACE], [B] gave a [OBJECT] to [A]",[m
[32m+[m[32m    "While spending time together [B] and [A] were commuting to the [PLACE], [B] gave a [OBJECT] to [A]",[m
[32m+[m[32m    "After the lunch in the afternoon, [B] and [A] went to the [PLACE]. [B] gave a [OBJECT] to [A]",[m
[32m+[m[32m    "Afterwards, while spending time together [B] and [A] went to the [PLACE]. [B] gave a [OBJECT] to [A]",[m
[32m+[m[32m    "Then in the morning afterwards, [B] and [A] had a long argument. Afterwards [B] said to [A]",[m
[32m+[m[32m    "The local big [PLACE] [B] and [A] went to had a [OBJECT]. [B] gave it to [A]",[m
[32m+[m[32m    "Friends separated at birth [B] and [A] found a [OBJECT] at the [PLACE]. [B] gave it to [A]",[m
[32m+[m[32m][m
[32m+[m
 BABA_LATE_IOS = [[m
     "Then, [B] and [A] went to the [PLACE]. [B] gave a [OBJECT] to [A]",[m
     "Then, [B] and [A] had a lot of fun at the [PLACE]. [B] gave a [OBJECT] to [A]",[m
[36m@@ -457,7 +476,7 @@[m [mdef gen_flipped_prompts(prompts, names, flip=("S2", "IO")):[m
 # *Tok Idxs Methods[m
 [m
 [m
[31m-def get_name_idxs(prompts, tokenizer, idx_types=["IO", "S", "S2"]):[m
[32m+[m[32mdef get_name_idxs(prompts, tokenizer, idx_types=["IO", "S", "S2"], prepend_bos=False):[m
     name_idx_dict = dict((idx_type, []) for idx_type in idx_types)[m
     double_s2 = False[m
     for prompt in prompts:[m
[36m@@ -481,65 +500,47 @@[m [mdef get_name_idxs(prompts, tokenizer, idx_types=["IO", "S", "S2"]):[m
     if double_s2:[m
         warnings.warn("S2 index has been computed as the same for S and S2")[m
 [m
[31m-    return [torch.tensor(name_idx_dict[idx_type]) for idx_type in idx_types][m
[31m-[m
[31m-[m
[31m-def get_end_idxs(prompts, tokenizer, name_tok_len=1, has_start_padding=False):[m
[31m-    toks = torch.Tensor([m
[31m-        tokenizer([prompt["text"] for prompt in prompts], padding=True).input_ids[m
[31m-    ).type(torch.int)[m
[31m-    relevant_idx = int([m
[31m-        has_start_padding[m
[31m-    )  # if end of texting then ignore the first 50256[m
[31m-    end_idxs = torch.tensor([m
[31m-        [[m
[31m-            (toks[i] == 50256).nonzero()[relevant_idx][0].item()[m
[31m-            if 50256 in toks[i][1:][m
[31m-            else toks.shape[1][m
[31m-            for i in range(toks.shape[0])[m
[31m-        ][m
[31m-    )[m
[31m-    end_idxs = end_idxs - 1 - name_tok_len  # YOURE LOOKING AT TO NOT FINAL IO TOKEN[m
[31m-    return end_idxs[m
[31m-[m
[32m+[m[32m    return [[m
[32m+[m[32m        int(prepend_bos) + torch.tensor(name_idx_dict[idx_type])[m
[32m+[m[32m        for idx_type in idx_types[m
[32m+[m[32m    ][m
 [m
[31m-def get_rand_idxs(end_idxs, exclude):[m
[31m-    rand_idxs = [][m
[31m-    for i in range(len(end_idxs)):[m
[31m-        idx = np.random.randint(end_idxs[i])[m
[31m-        while idx in torch.vstack(exclude)[:, i]:[m
[31m-            idx = np.random.randint(end_idxs[i])[m
[31m-        rand_idxs.append(idx)[m
[31m-    return rand_idxs[m
 [m
[32m+[m[32mdef get_end_idxs(prompts, tokenizer, name_tok_len=1, prepend_bos=False, toks=None):[m
[32m+[m[32m    # toks = torch.Tensor(tokenizer([prompt["text"] for prompt in prompts], padding=True).input_ids).type(torch.int)[m
[32m+[m[32m    relevant_idx = int(prepend_bos)[m
[32m+[m[32m    # if the sentence begins with an end token[m
[32m+[m[32m    # AND the model pads at the end with the same end token,[m
[32m+[m[32m    # then we need make special arrangements[m
[32m+[m
[32m+[m[32m    pad_token_id = tokenizer.pad_token_id[m
[32m+[m
[32m+[m[32m    end_idxs_raw = [][m
[32m+[m[32m    for i in range(toks.shape[0]):[m
[32m+[m[32m        if pad_token_id not in toks[i][1:]:[m
[32m+[m[32m            end_idxs_raw.append(toks.shape[1])[m
[32m+[m[32m            continue[m
[32m+[m[32m        nonzers = (toks[i] == pad_token_id).nonzero()[m
[32m+[m[32m        try:[m
[32m+[m[32m            nonzers = nonzers[relevant_idx][m
[32m+[m[32m        except:[m
[32m+[m[32m            print(toks[i])[m
[32m+[m[32m            print(nonzers)[m
[32m+[m[32m            print(relevant_idx)[m
[32m+[m[32m            print(i)[m
[32m+[m[32m            raise ValueError("Something went wrong")[m
[32m+[m[32m        nonzers = nonzers[0][m
[32m+[m[32m        nonzers = nonzers.item()[m
[32m+[m[32m        end_idxs_raw.append(nonzers)[m
[32m+[m[32m    end_idxs = torch.tensor(end_idxs_raw)[m
[32m+[m[32m    end_idxs = end_idxs - 1 - name_tok_len[m
[32m+[m
[32m+[m[32m    for i in range(toks.shape[0]):[m
[32m+[m[32m        assert toks[i][end_idxs[i] + 1] != 0 and ([m
[32m+[m[32m            toks.shape[1] == end_idxs[i] + 2 or toks[i][end_idxs[i] + 2] == pad_token_id[m
[32m+[m[32m        ), (toks[i], end_idxs[i], toks[i].shape, "the END idxs aren't properly formatted")[m
 [m
[31m-def get_word_idxs(prompts, word_list, tokenizer):[m
[31m-    """Get the index of the words in word_list in the prompts. Exactly one of the word_list word has to be present in each prompt"""[m
[31m-    idxs = [][m
[31m-    tokenized_words = [[m
[31m-        tokenizer.decode(tokenizer(word)["input_ids"][0]) for word in word_list[m
[31m-    ][m
[31m-    for pr_idx, prompt in enumerate(prompts):[m
[31m-        toks = [[m
[31m-            tokenizer.decode(t)[m
[31m-            for t in tokenizer(prompt["text"], return_tensors="pt", padding=True)[[m
[31m-                "input_ids"[m
[31m-            ][0][m
[31m-        ][m
[31m-        idx = None[m
[31m-        for i, w_tok in enumerate(tokenized_words):[m
[31m-            if word_list[i] in prompt["text"]:[m
[31m-                try:[m
[31m-                    idx = toks.index(w_tok)[m
[31m-                    if toks.count(w_tok) > 1:[m
[31m-                        idx = len(toks) - toks[::-1].index(w_tok) - 1[m
[31m-                except:[m
[31m-                    idx = toks.index(w_tok)[m
[31m-                    # raise ValueError(toks, w_tok, prompt["text"])[m
[31m-        if idx is None:[m
[31m-            raise ValueError(f"Word {word_list} and {i} not found {prompt}")[m
[31m-        idxs.append(idx)[m
[31m-    return torch.tensor(idxs)[m
[32m+[m[32m    return end_idxs[m
 [m
 [m
 import torch[m
[36m@@ -555,27 +556,21 @@[m [mALL_SEM = [[m
 ]  # , "verb", "starts", "S-1", "punct"] # Kevin's antic averages[m
 [m
 [m
[31m-def get_idx_dict(ioi_prompts, tokenizer, has_start_padding=False):[m
[31m-    ([m
[31m-        IO_idxs,[m
[31m-        S_idxs,[m
[31m-        S2_idxs,[m
[31m-    ) = get_name_idxs(ioi_prompts, tokenizer, idx_types=["IO", "S", "S2"])[m
[32m+[m[32mdef get_idx_dict(ioi_prompts, tokenizer, prepend_bos=False, toks=None):[m
[32m+[m[32m    (IO_idxs, S_idxs, S2_idxs,) = get_name_idxs([m
[32m+[m[32m        ioi_prompts,[m
[32m+[m[32m        tokenizer,[m
[32m+[m[32m        idx_types=["IO", "S", "S2"],[m
[32m+[m[32m        prepend_bos=prepend_bos,[m
[32m+[m[32m    )[m
 [m
     end_idxs = get_end_idxs([m
[31m-        ioi_prompts, tokenizer, name_tok_len=1, has_start_padding=has_start_padding[m
[32m+[m[32m        ioi_prompts,[m
[32m+[m[32m        tokenizer,[m
[32m+[m[32m        name_tok_len=1,[m
[32m+[m[32m        prepend_bos=prepend_bos,[m
[32m+[m[32m        toks=toks,[m
     )[m
[31m-    rand_idxs = get_rand_idxs(end_idxs, exclude=[IO_idxs, S_idxs, S2_idxs])[m
[31m-    punc_idxs = None[m
[31m-    warnings.warn("Punctuation not implemented")[m
[31m-    try:[m
[31m-        punc_idxs = get_word_idxs([m
[31m-            ioi_prompts, [",", "."], tokenizer[m
[31m-        )  # if there is "," and '.' in the prompt, only the '.' index will be kept.[m
[31m-    except:[m
[31m-        warnings.warn("Punctuation not implemented")[m
[31m-    verb_idxs = get_word_idxs(ioi_prompts, VERBS, tokenizer)[m
[31m-    # and_idxs = get_word_idxs(ioi_prompts, [" and"], tokenizer)[m
 [m
     return {[m
         "IO": IO_idxs,[m
[36m@@ -585,15 +580,13 @@[m [mdef get_idx_dict(ioi_prompts, tokenizer, has_start_padding=False):[m
         "S-1": S_idxs - 1,[m
         "S+1": S_idxs + 1,[m
         "S2": S2_idxs,[m
[31m-        "end": end_idxs,  # the " to" token, the last one.[m
[31m-        "rand": rand_idxs,  # random index at each[m
[31m-        "punct": punc_idxs,[m
[31m-        "verb": verb_idxs,[m
[31m-        # "and": and_idxs,[m
[31m-        "starts": torch.zeros_like(verb_idxs),[m
[32m+[m[32m        "end": end_idxs,[m
[32m+[m[32m        "starts": torch.zeros_like(end_idxs),[m
     }[m
 [m
 [m
[32m+[m[32m# Some functions for experiments on Pointer Arithmetic[m
[32m+[m
 PREFIXES = [[m
     "             Afterwards,",[m
     "            Two friends met at a bar. Then,",[m
[36m@@ -651,6 +644,7 @@[m [mclass IOIDataset:[m
         prefixes=None,[m
         nb_templates=None,[m
         ioi_prompts_for_word_idxs=None,[m
[32m+[m[32m        prepend_bos=False,[m
         manual_word_idx=None,[m
     ):[m
         """[m
[36m@@ -659,6 +653,14 @@[m [mclass IOIDataset:[m
             (example use case: making a ABCA dataset)[m
         """[m
 [m
[32m+[m[32m        if not ([m
[32m+[m[32m            N == 1[m
[32m+[m[32m            or prepend_bos == False[m
[32m+[m[32m            or tokenizer.bos_token_id == tokenizer.eos_token_id[m
[32m+[m[32m        ):[m
[32m+[m[32m            warnings.warn([m
[32m+[m[32m                "Probably word_idx will be calculated incorrectly due to this formatting"[m
[32m+[m[32m            )[m
         assert not (symmetric and prompt_type == "ABC")[m
         assert ([m
             (prompts is not None) or (not symmetric) or (N % 2 == 0)[m
[36m@@ -745,17 +747,23 @@[m [mclass IOIDataset:[m
                 self.templates_by_prompt.append("BABA")[m
 [m
         # print(self.ioi_prompts, "that's that")[m
[31m-        self.toks = torch.Tensor([m
[31m-            self.tokenizer([m
[31m-                [prompt["text"] for prompt in self.ioi_prompts], padding=True[m
[31m-            ).input_ids[m
[31m-        ).type(torch.int)[m
[32m+[m[32m        texts = [[m
[32m+[m[32m            (self.tokenizer.bos_token if prepend_bos else "") + prompt["text"][m
[32m+[m[32m            for prompt in self.ioi_prompts[m
[32m+[m[32m        ][m
[32m+[m[32m        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type([m
[32m+[m[32m            torch.int[m
[32m+[m[32m        )[m
 [m
         if ioi_prompts_for_word_idxs is None:[m
             ioi_prompts_for_word_idxs = self.ioi_prompts[m
         self.word_idx = get_idx_dict([m
[31m-            ioi_prompts_for_word_idxs, self.tokenizer, self.toks[0][0] == 50256[m
[32m+[m[32m            ioi_prompts_for_word_idxs,[m
[32m+[m[32m            self.tokenizer,[m
[32m+[m[32m            prepend_bos=prepend_bos,[m
[32m+[m[32m            toks=self.toks,[m
         )[m
[32m+[m[32m        self.prepend_bos = prepend_bos[m
         if manual_word_idx is not None:[m
             self.word_idx = manual_word_idx[m
 [m
[36m@@ -859,6 +867,7 @@[m [mclass IOIDataset:[m
             prompts=flipped_prompts,[m
             prefixes=self.prefixes,[m
             ioi_prompts_for_word_idxs=flipped_prompts if flip[0] == "RAND" else None,[m
[32m+[m[32m            prepend_bos=self.prepend_bos,[m
             manual_word_idx=manual_word_idx,[m
         )[m
         return flipped_ioi_dataset[m
[36m@@ -884,6 +893,7 @@[m [mclass IOIDataset:[m
             tokenizer=self.tokenizer,[m
             prompts=sliced_prompts,[m
             prefixes=self.prefixes,[m
[32m+[m[32m            prepend_bos=self.prepend_bos,[m
         )[m
         return sliced_dataset[m
 [m
[1mdiff --git a/ioi_minimality.py b/ioi_minimality.py[m
[1mindex 6bcd449..c3cc966 100644[m
[1m--- a/ioi_minimality.py[m
[1m+++ b/ioi_minimality.py[m
[36m@@ -138,7 +138,7 @@[m [mmodel, _ = do_circuit_extraction([m
     mlps_to_remove={},[m
     ioi_dataset=ioi_dataset,[m
     mean_dataset=mean_dataset,[m
[31m-    exclude_heads=naive_heads,[m
[32m+[m[32m    excluded=naive_heads,[m
 )[m
 [m
 circuit_baseline_metric = metric(model, ioi_dataset)[m
[1mdiff --git a/ioi_notebook.py b/ioi_notebook.py[m
[1mindex ae492b4..ba53319 100644[m
[1m--- a/ioi_notebook.py[m
[1m+++ b/ioi_notebook.py[m
[36m@@ -19,12 +19,14 @@[m
 [m
 # %% [markdown][m
 # ## Imports[m
[32m+[m[32mfrom copy import deepcopy[m
 import os[m
 import torch[m
[32m+[m
 if os.environ["USER"] in ["exx", "arthur"]:  # so Arthur can safely use octobox[m
     os.environ["CUDA_VISIBLE_DEVICES"] = "2"[m
 assert torch.cuda.device_count() == 1[m
[31m-from easy_transformer.EasyTransformer import MODEL_NAMES_DICT, LayerNormPre[m
[32m+[m[32mfrom easy_transformer.EasyTransformer import LayerNormPre[m
 from tqdm import tqdm[m
 import pandas as pd[m
 import torch[m
[36m@@ -65,6 +67,7 @@[m [mimport plotly.express as px[m
 import plotly.io as pio[m
 from plotly.subplots import make_subplots[m
 import plotly.graph_objects as go[m
[32m+[m[32mimport warnings[m
 import plotly[m
 from sklearn.linear_model import LinearRegression[m
 from transformers import AutoModelForCausalLM, AutoTokenizer[m
[36m@@ -79,7 +82,7 @@[m [mfrom datasets import load_dataset[m
 from IPython import get_ipython[m
 import matplotlib.pyplot as plt[m
 import random as rd[m
[31m-[m
[32m+[m[32mfrom copy import deepcopy[m
 from ioi_dataset import ([m
     IOIDataset,[m
     NOUNS_DICT,[m
[36m@@ -90,6 +93,8 @@[m [mfrom ioi_dataset import ([m
     ABBA_TEMPLATES,[m
 )[m
 from ioi_utils import ([m
[32m+[m[32m    max_2d,[m
[32m+[m[32m    CLASS_COLORS,[m
     all_subsets,[m
     clear_gpu_mem,[m
     show_tokens,[m
[36m@@ -97,6 +102,7 @@[m [mfrom ioi_utils import ([m
     show_attention_patterns,[m
     safe_del,[m
 )[m
[32m+[m[32mfrom random import randint as ri[m
 from ioi_circuit_extraction import ([m
     do_circuit_extraction,[m
     gen_prompt_uniform,[m
[36m@@ -111,6 +117,8 @@[m [mfrom ioi_circuit_extraction import ([m
     CIRCUIT,[m
     ARTHUR_CIRCUIT,[m
 )[m
[32m+[m[32mfrom ioi_utils import logit_diff, probs[m
[32m+[m[32mfrom ioi_utils import get_top_tokens_and_probs as g[m
 [m
 ipython = get_ipython()[m
 if ipython is not None:[m
[36m@@ -119,28 +127,697 @@[m [mif ipython is not None:[m
 #%% [markdown][m
 # # <h1><b>Setup</b></h1>[m
 # Import model and dataset[m
[31m-#%% # plot writing in the IO - S direction[m
[31m-model_name = "gpt2"  # Here we used gpt-2 small ("gpt2")[m
[31m-print_gpu_mem("About to load model")[m
[31m-model = EasyTransformer([m
[31m-    model_name, use_attn_result=True[m
[31m-)  # use_attn_result adds a hook blocks.{lay}.attn.hook_result that is before adding the biais of the attention layer[m
[32m+[m[32m#%% Plot writing in the IO - S direction[m
[32m+[m[32mif False:[m
[32m+[m[32m    model = EasyTransformer([m
[32m+[m[32m        model_name, use_attn_result=True[m
[32m+[m[32m    )  # use_attn_result adds a hook blocks.{lay}.attn.hook_result that is before adding the biais of the attention layer[m
[32m+[m[32mif True:[m
[32m+[m[32m    model = EasyTransformer.from_pretrained("gpt2").cuda()[m
[32m+[m[32m    # model = EasyTransformer.from_pretrained("EleutherAI/gpt-neo-125M").cuda()[m
[32m+[m[32m    # model = EasyTransformer.from_pretrained("gpt2-medium").cuda()[m
[32m+[m[32m    # model = EasyTransformer.from_pretrained([m
[32m+[m[32m    #     f"stanford-gpt2-small-{letter}"[m
[32m+[m[32m    # ).cuda()  # to E![m
[32m+[m[32m    model.set_use_attn_result(True)[m
[32m+[m
 device = "cuda"[m
 if torch.cuda.is_available():[m
     model.to(device)[m
 print_gpu_mem("Gpt2 loaded")[m
[32m+[m[32mN = 100[m
[32m+[m[32mioi_dataset = IOIDataset([m
[32m+[m[32m    prompt_type="mixed",[m
[32m+[m[32m    N=N,[m
[32m+[m[32m    tokenizer=model.tokenizer,[m
[32m+[m[32m    prepend_bos=True,[m
[32m+[m[32m    has_start_padding_and_start_is_end=True,[m
[32m+[m[32m)[m
[32m+[m[32mall_diff_dataset = ([m
[32m+[m[32m    ioi_dataset.gen_flipped_prompts(("IO", "RAND"))[m
[32m+[m[32m    .gen_flipped_prompts(("S", "RAND"))[m
[32m+[m[32m    .gen_flipped_prompts(("S1", "RAND"), manual_word_idx=ioi_dataset.word_idx)[m
[32m+[m[32m)[m
[32m+[m[32mwarnings.warn("Edit the last two here")[m
[32m+[m
[32m+[m[32m##%% [markdown] wait what about without start garbage?[m
[32m+[m
[32m+[m[32mioi_dataset_2 = IOIDataset([m
[32m+[m[32m    prompt_type="mixed",[m
[32m+[m[32m    N=N,[m
[32m+[m[32m    tokenizer=model.tokenizer,[m
[32m+[m[32m    prepend_bos=False,[m
[32m+[m[32m    has_start_padding_and_start_is_end=False,[m
[32m+[m[32m    prompts=ioi_dataset.ioi_prompts,[m
[32m+[m[32m)[m
[32m+[m[32mall_diff_dataset_2 = ([m
[32m+[m[32m    ioi_dataset_2.gen_flipped_prompts(("IO", "RAND"))[m
[32m+[m[32m    .gen_flipped_prompts(("S", "RAND"))[m
[32m+[m[32m    .gen_flipped_prompts(("S1", "RAND"), manual_word_idx=ioi_dataset.word_idx)[m
[32m+[m[32m)[m
[32m+[m[32m##%%[m
[32m+[m[32mall_diff_dataset, all_diff_dataset_2 = all_diff_dataset_2, all_diff_dataset[m
[32m+[m[32mioi_dataset, ioi_dataset_2 = ioi_dataset_2, ioi_dataset[m
[32m+[m
[32m+[m[32m##%% [markdown] test to see if the word_idx is legit[m
[32m+[m[32mfor new_N in range(1, 3):[m
[32m+[m[32m    # d = IOIDataset(prompt_type="mixed", N=new_N, tokenizer=model.tokenizer, prepend_bos=True, has_start_padding_and_start_is_end=True)[m
[32m+[m[32m    d = ioi_dataset[m
[32m+[m[32m    print(f"new_N={new_N}")[m
[32m+[m[32m    for i in range(new_N):[m
[32m+[m[32m        for key in d.word_idx.keys():[m
[32m+[m[32m            print([m
[32m+[m[32m                f"key={key} {int(d.word_idx[key][i])} {d.tokenizer.decode(d.toks[i][d.word_idx[key][i]])}"[m
[32m+[m[32m            )[m
[32m+[m[32mprint("Seem fine?")[m
[32m+[m[32m##%%[m
[32m+[m[32mtotd = 0[m
[32m+[m[32mcp = 0[m
[32m+[m[32mmodel.reset_hooks()[m
[32m+[m[32mfor d in [ioi_dataset]:[m
[32m+[m[32m    # for i in range(dataset.N):[m
[32m+[m[32m    # d = ioi_dataset[i:i+1][m
[32m+[m[32m    circuit_logit_diff = logit_diff(model, d)[m
[32m+[m[32m    totd += circuit_logit_diff[m
[32m+[m[32m    circuit_probs = probs(model, d)[m
[32m+[m[32m    probs2 = probs(model, d, type="s")[m
[32m+[m[32m    cp += circuit_probs[m
[32m+[m[32m    print(f"{circuit_logit_diff} {probs2} {circuit_probs}")[m
[32m+[m[32m##%% [markdown] look at logit writing for the GPT NEO[m
[32m+[m
[32m+[m
[32m+[m[32mdef patch_positions(z, source_act, hook, positions=["end"]):[m
[32m+[m[32m    for pos in positions:[m
[32m+[m[32m        z[torch.arange(ioi_dataset.N), ioi_dataset.word_idx[pos]] = source_act[[m
[32m+[m[32m            torch.arange(ioi_dataset.N), ioi_dataset.word_idx[pos][m
[32m+[m[32m        ][m
[32m+[m[32m    return z[m
[32m+[m
[32m+[m
[32m+[m[32mdef patch_all(z, source_act, hook):[m
[32m+[m[32m    return source_act[m
[32m+[m
[32m+[m
[32m+[m[32mdef direct_patch_and_freeze([m
[32m+[m[32m    model,[m
[32m+[m[32m    source_dataset,[m
[32m+[m[32m    target_dataset,[m
[32m+[m[32m    ioi_dataset,[m
[32m+[m[32m    sender_heads,[m
[32m+[m[32m    receiver_hooks,[m
[32m+[m[32m    max_layer,[m
[32m+[m[32m    positions=["end"],[m
[32m+[m[32m    verbose=False,[m
[32m+[m[32m    return_hooks=False,[m
[32m+[m[32m    extra_hooks=[],  # when we call reset hooks, we may want to add some extra hooks after this, add these here[m
[32m+[m[32m    freeze_mlps=False,  # recall in IOI paper we consider these "vital model components"[m
[32m+[m[32m):[m
[32m+[m[32m    """[m
[32m+[m[32m    Patch in the effect of `sender_heads` on `receiver_hooks` only[m
[32m+[m[32m    (though MLPs are "ignored", so are slight confounders)[m
[32m+[m
[32m+[m[32m    If max_layer < model.cfg.n_layers, then let some part of the model do computations (not frozen)[m
[32m+[m[32m    """[m
[32m+[m
[32m+[m[32m    sender_hooks = [][m
[32m+[m
[32m+[m[32m    for layer, head_idx in sender_heads:[m
[32m+[m[32m        if head_idx is None:[m
[32m+[m[32m            sender_hooks.append((f"blocks.{layer}.hook_mlp_out", None))[m
[32m+[m
[32m+[m[32m        else:[m
[32m+[m[32m            sender_hooks.append((f"blocks.{layer}.attn.hook_result", head_idx))[m
[32m+[m
[32m+[m[32m    sender_hook_names = [x[0] for x in sender_hooks][m
[32m+[m[32m    receiver_hook_names = [x[0] for x in receiver_hooks][m
[32m+[m
[32m+[m[32m    sender_cache = {}[m
[32m+[m[32m    model.reset_hooks()[m
[32m+[m[32m    for hook in extra_hooks:[m
[32m+[m[32m        model.add_hook(*hook)[m
[32m+[m[32m    model.cache_some(sender_cache, lambda x: x in sender_hook_names)[m
[32m+[m[32m    # print(f"{sender_hook_names}")[m
[32m+[m[32m    source_logits = model([m
[32m+[m[32m        source_dataset.toks.long()[m
[32m+[m[32m    )  # this should see what the logits are when i) main heads are ablated + ii) we're also ablating (lay, head_idx)[m
[32m+[m
[32m+[m[32m    target_cache = {}[m
[32m+[m[32m    model.reset_hooks()[m
[32m+[m[32m    for hook in extra_hooks:[m
[32m+[m[32m        model.add_hook(*hook)[m
[32m+[m[32m    model.cache_all(target_cache)[m
[32m+[m[32m    target_logits = model(target_dataset.toks.long())[m
[32m+[m
[32m+[m[32m    # for all the Q, K, V things[m
[32m+[m[32m    model.reset_hooks()[m
[32m+[m[32m    for layer in range(max_layer):[m
[32m+[m[32m        for head_idx in range(model.cfg.n_heads):[m
[32m+[m[32m            for hook_template in [[m
[32m+[m[32m                "blocks.{}.attn.hook_q",[m
[32m+[m[32m                "blocks.{}.attn.hook_k",[m
[32m+[m[32m                "blocks.{}.attn.hook_v",[m
[32m+[m[32m            ]:[m
[32m+[m[32m                hook_name = hook_template.format(layer)[m
[32m+[m
[32m+[m[32m                if hook_name in receiver_hook_names:[m
[32m+[m[32m                    continue[m
[32m+[m
[32m+[m[32m                hook = get_act_hook([m
[32m+[m[32m                    patch_all,[m
[32m+[m[32m                    alt_act=target_cache[hook_name],[m
[32m+[m[32m                    idx=head_idx,[m
[32m+[m[32m                    dim=2 if head_idx is not None else None,[m
[32m+[m[32m                    name=hook_name,[m
[32m+[m[32m                )[m
[32m+[m[32m                model.add_hook(hook_name, hook)[m
[32m+[m
[32m+[m[32m        if freeze_mlps:[m
[32m+[m[32m            hook_name = f"blocks.{layer}.hook_mlp_out"[m
[32m+[m[32m            hook = get_act_hook([m
[32m+[m[32m                patch_all,[m
[32m+[m[32m                alt_act=target_cache[hook_name],[m
[32m+[m[32m                idx=None,[m
[32m+[m[32m                dim=None,[m
[32m+[m[32m                name=hook_name,[m
[32m+[m[32m            )[m
[32m+[m[32m            model.add_hook(hook_name, hook)[m
[32m+[m
[32m+[m[32m    for hook in extra_hooks:[m
[32m+[m[32m        # ughhh, think that this is what we want, this should override the QKV above[m
[32m+[m[32m        model.add_hook(*hook)[m
[32m+[m
[32m+[m[32m    # we can override the hooks above for the sender heads, though[m
[32m+[m[32m    for hook_name, head_idx in sender_hooks:[m
[32m+[m[32m        assert not torch.allclose(sender_cache[hook_name], target_cache[hook_name]), ([m
[32m+[m[32m            hook_name,[m
[32m+[m[32m            head_idx,[m
[32m+[m[32m        )[m
[32m+[m[32m        hook = get_act_hook([m
[32m+[m[32m            partial(patch_positions, positions=positions),[m
[32m+[m[32m            alt_act=sender_cache[hook_name],[m
[32m+[m[32m            idx=head_idx,[m
[32m+[m[32m            dim=2 if head_idx is not None else None,[m
[32m+[m[32m            name=hook_name,[m
[32m+[m[32m        )[m
[32m+[m[32m        model.add_hook(hook_name, hook)[m
[32m+[m
[32m+[m[32m    # measure the receiver heads' values[m
[32m+[m[32m    receiver_cache = {}[m
[32m+[m[32m    model.cache_some(receiver_cache, lambda x: x in receiver_hook_names)[m
[32m+[m[32m    receiver_logits = model(target_dataset.toks.long())[m
[32m+[m
[32m+[m[32m    # patch these values in[m
[32m+[m[32m    model.reset_hooks()[m
[32m+[m[32m    for hook in extra_hooks:[m
[32m+[m[32m        model.add_hook([m
[32m+[m[32m            *hook[m
[32m+[m[32m        )  # ehh probably doesn't actually matter cos end thing hooked[m
[32m+[m
[32m+[m[32m    hooks = [][m
[32m+[m[32m    for hook_name, head_idx in receiver_hooks:[m
[32m+[m[32m        hook = get_act_hook([m
[32m+[m[32m            partial(patch_positions, positions=positions),[m
[32m+[m[32m            alt_act=receiver_cache[hook_name],[m
[32m+[m[32m            idx=head_idx,[m
[32m+[m[32m            dim=2 if head_idx is not None else None,[m
[32m+[m[32m            name=hook_name,[m
[32m+[m[32m        )[m
[32m+[m[32m        hooks.append((hook_name, hook))[m
[32m+[m
[32m+[m[32m    if return_hooks:[m
[32m+[m[32m        return hooks[m
[32m+[m[32m    else:[m
[32m+[m[32m        for hook_name, hook in hooks:[m
[32m+[m[32m            model.add_hook(hook_name, hook)[m
[32m+[m[32m        return model[m
[32m+[m
[32m+[m
[32m+[m[32m#%% [markdown] first patch-and-freeze experiments[m
[32m+[m[32m# TODO why are there effects that come AFTER the patching?? it's before 36 mins in voko I think[m
[32m+[m
[32m+[m[32mdataset_names = [[m
[32m+[m[32m    # "ioi_dataset",[m
[32m+[m[32m    # "abca_dataset",[m
[32m+[m[32m    # "dcc_dataset",[m
[32m+[m[32m    # "acca_dataset",[m
[32m+[m[32m    # "acba_dataset",[m
[32m+[m[32m    "all_diff_dataset",[m
[32m+[m[32m    # "totally_diff_dataset",[m
[32m+[m[32m][m
[32m+[m
[32m+[m[32m# ([(19, 1),[m
[32m+[m[32m#   (12, 3),[m
[32m+[m[32m#   (13, 4),[m
[32m+[m[32m#   (13, 13),[m
[32m+[m[32m#   (15, 8),[m
[32m+[m[32m#   (16, 0),[m
[32m+[m[32m#   (15, 14),[m
[32m+[m
[32m+[m[32m# knockout some heads !!![m
[32m+[m[32mexclude_heads = [[m
[32m+[m[32m    (layer, head_idx)[m
[32m+[m[32m    for layer in range(model.cfg.n_layers)[m
[32m+[m[32m    for head_idx in range(model.cfg.n_heads)[m
[32m+[m[32m][m
[32m+[m
[32m+[m[32mall_top_heads = {[m
[32m+[m[32m    "stanford-gpt2-small-A": [(10, 4), (10, 10), (10, 11)],[m
[32m+[m[32m    "gpt2": [(9, 9), (9, 6), (10, 0)],[m
[32m+[m[32m    "elutherAI": [(9, 4), (11, 4), (11, 2)],[m
[32m+[m[32m    "gpt2-medium": [(19, 1), (12, 3), (13, 4)],[m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32mtop_heads = all_top_heads.get(model.cfg.model_name, [])[m
[32m+[m
[32m+[m[32mfor head in top_heads:[m
[32m+[m[32m    exclude_heads.remove(head)[m
[32m+[m
[32m+[m[32mthe_extra_hooks = [][m
[32m+[m[32mall_results = [][m
[32m+[m[32mall_mlp_results = [][m
[32m+[m
[32m+[m[32mfor use_extra_hooks in [False, True]:[m
[32m+[m[32m    if use_extra_hooks:[m
[32m+[m
[32m+[m[32m        # BLAH1[m
[32m+[m[32m        # top_heads=all_top_heads.get(model.cfg.model_name, [])[m
[32m+[m
[32m+[m[32m        # for head in top_heads:[m
[32m+[m[32m        #     exclude_heads.remove(head)[m
[32m+[m[32m        # the_extra_hooks = do_circuit_extraction([m
[32m+[m[32m        #     model=model,[m
[32m+[m[32m        #     heads_to_keep={},[m
[32m+[m[32m        #     mlps_to_remove={},[m
[32m+[m[32m        #     ioi_dataset=ioi_dataset,[m
[32m+[m[32m        #     mean_dataset=all_diff_dataset,[m
[32m+[m[32m        #     excluded=exclude_heads,[m
[32m+[m[32m        #     return_hooks=True,[m
[32m+[m[32m        # )[m
[32m+[m[32m        # model.reset_hooks()[m
[32m+[m
[32m+[m[32m        # all_results = [][m
[32m+[m[32m        # all_mlp_results = [][m
[32m+[m[32m        # BLAH2[m
[32m+[m
[32m+[m[32m        exclude_heads = [[m
[32m+[m[32m            (layer, head_idx)[m
[32m+[m[32m            for layer in range(model.cfg.n_layers)[m
[32m+[m[32m            for head_idx in range(model.cfg.n_heads)[m
[32m+[m[32m        ][m
[32m+[m[32m        max_heads = max_2d(-all_results[0], k=3)[0][m
[32m+[m[32m        for head in max_heads:[m
[32m+[m[32m            exclude_heads.remove(head)[m
[32m+[m[32m        extra_hooks = do_circuit_extraction([m
[32m+[m[32m            model=model,[m
[32m+[m[32m            heads_to_keep={},[m
[32m+[m[32m            mlps_to_remove={},[m
[32m+[m[32m            ioi_dataset=ioi_dataset,[m
[32m+[m[32m            mean_dataset=all_diff_dataset,[m
[32m+[m[32m            excluded=exclude_heads,[m
[32m+[m[32m            return_hooks=True,[m
[32m+[m[32m        )[m
[32m+[m[32m        assert len(extra_hooks) == 3, extra_hooks[m
[32m+[m[32m        model.reset_hooks()[m
[32m+[m
[32m+[m[32m    else:[m
[32m+[m[32m        extra_hooks = [][m
[32m+[m
[32m+[m[32m    model.reset_hooks()[m
[32m+[m[32m    for extra_hook in extra_hooks:[m
[32m+[m[32m        model.add_hook(*extra_hook)[m
[32m+[m[32m    default_logit_diff = logit_diff(model, ioi_dataset)[m
[32m+[m
[32m+[m[32m    results = torch.zeros(size=(model.cfg.n_layers, model.cfg.n_heads))[m
[32m+[m[32m    mlp_results = torch.zeros(size=(model.cfg.n_layers, 1))[m
[32m+[m[32m    for source_layer in tqdm(range(model.cfg.n_layers)):[m
[32m+[m[32m        for source_head_idx in [None] + list(range(model.cfg.n_heads)):[m
[32m+[m[32m            model.reset_hooks()[m
[32m+[m[32m            receiver_hooks = [][m
[32m+[m[32m            # for layer, head_idx in circuit["name mover"]:[m
[32m+[m[32m            # receiver_hooks.append((f"blocks.{layer}.attn.hook_q", head_idx))[m
[32m+[m[32m            # receiver_hooks.append((f"blocks.{layer}.attn.hook_v", head_idx))[m
[32m+[m[32m            # receiver_hooks.append((f"blocks.{layer}.attn.hook_k", head_idx))[m
[32m+[m[32m            receiver_hooks.append([m
[32m+[m[32m                (f"blocks.{model.cfg.n_layers-1}.hook_resid_post", None)[m
[32m+[m[32m            )[m
[32m+[m
[32m+[m[32m            model = direct_patch_and_freeze([m
[32m+[m[32m                model=model,[m
[32m+[m[32m                source_dataset=all_diff_dataset,[m
[32m+[m[32m                target_dataset=ioi_dataset,[m
[32m+[m[32m                ioi_dataset=ioi_dataset,[m
[32m+[m[32m                sender_heads=[(source_layer, source_head_idx)],[m
[32m+[m[32m                receiver_hooks=receiver_hooks,[m
[32m+[m[32m                max_layer=model.cfg.n_heads,[m
[32m+[m[32m                positions=["end"],[m
[32m+[m[32m                verbose=False,[m
[32m+[m[32m                return_hooks=False,[m
[32m+[m[32m                freeze_mlps=False,[m
[32m+[m[32m                extra_hooks=extra_hooks,[m
[32m+[m[32m            )[m
[32m+[m[32m            cur_logit_diff = logit_diff(model, ioi_dataset)[m
[32m+[m
[32m+[m[32m            if source_head_idx is None:[m
[32m+[m[32m                mlp_results[source_layer] = cur_logit_diff - default_logit_diff[m
[32m+[m[32m            else:[m
[32m+[m[32m                results[source_layer][source_head_idx] = ([m
[32m+[m[32m                    cur_logit_diff - default_logit_diff[m
[32m+[m[32m                )[m
[32m+[m
[32m+[m[32m            if ([m
[32m+[m[32m                source_layer == model.cfg.n_layers - 1[m
[32m+[m[32m                and source_head_idx == model.cfg.n_heads - 1[m
[32m+[m[32m            ):[m
[32m+[m[32m                # show attention head results[m
[32m+[m[32m                fname = f"svgs/patch_and_freeze_{ctime()}_{ri(2134, 123759)}"[m
[32m+[m[32m                fig = show_pp([m
[32m+[m[32m                    results.T,[m
[32m+[m[32m                    title=f"{fname} patching NMs",[m
[32m+[m[32m                    return_fig=True,[m
[32m+[m[32m                    show_fig=False,[m
[32m+[m[32m                )[m
[32m+[m
[32m+[m[32m                fig.write_image([m
[32m+[m[32m                    f"svgs/patch_and_freezes/to_duplicate_token_K_{use_extra_hooks}.png"[m
[32m+[m[32m                )[m
[32m+[m
[32m+[m[32m                fig.write_image(fname + ".png")[m
[32m+[m[32m                fig.write_image(fname + ".svg")[m
[32m+[m[32m                fig.show()[m
[32m+[m
[32m+[m[32m                # # show mlp results # mlps are (hopefully not anymore???) fucked[m
[32m+[m[32m                fname = f"svgs/patch_and_freeze_mlp_{ctime()}_{ri(2134, 123759)}"[m
[32m+[m[32m                fig = show_pp([m
[32m+[m[32m                    mlp_results.T,[m
[32m+[m[32m                    title="Direct effect of MLPs on Logit Difference",[m
[32m+[m[32m                    return_fig=True,[m
[32m+[m[32m                    show_fig=False,[m
[32m+[m[32m                )[m
[32m+[m[32m                fig.write_image(fname + ".png")[m
[32m+[m[32m                fig.write_image(fname + ".svg")[m
[32m+[m[32m                fig.show()[m
[32m+[m[32m                all_results.append(results.clone())[m
[32m+[m[32m                all_mlp_results.append(mlp_results.clone())[m
[32m+[m[32m    #%% [markdown] plotting (your downfalls!) # weird ass indentation[m
[32m+[m[32m    cc = deepcopy(CLASS_COLORS)[m
[32m+[m[32m    circuit = deepcopy(CIRCUIT)[m
[32m+[m
[32m+[m[32m    def what_class(layer, head, circuit):[m
[32m+[m[32m        for circuit_class in circuit:[m
[32m+[m[32m            if (layer, head) in max_heads:[m
[32m+[m[32m                return "name mover"  # circuit[circuit_class]:[m
[32m+[m[32m            return "duplicate token"[m
[32m+[m
[32m+[m[32m    # plot the most important heads by[m
[32m+[m
[32m+[m[32m    k = 15[m
[32m+[m[32m    top_heads = max_2d([m
[32m+[m[32m        torch.abs(all_results[0]), k=k[m
[32m+[m[32m    )[  # backup results or initial results[m
[32m+[m[32m        0[m
[32m+[m[32m    ]  # initial results is the patch with no KOs; direct effect on logits[m
[32m+[m
[32m+[m[32m    exclude_heads = [][m
[32m+[m[32m    exclude_heads = [[m
[32m+[m[32m        (layer_idx, head)[m
[32m+[m[32m        for layer_idx in range(model.cfg.n_layers)[m
[32m+[m[32m        for head in range(model.cfg.n_heads)[m
[32m+[m[32m        if what_class(layer_idx, head, circuit=circuit)[m
[32m+[m[32m        not in ["name mover", "negative", "s2 inhibition"][m
[32m+[m[32m    ][m
[32m+[m
[32m+[m[32m    fig = go.Figure()[m
[32m+[m
[32m+[m[32m    for name, result in zip([m
[32m+[m[32m        ["Left: WT", "Right: KO of most important NMs"],[m
[32m+[m[32m        [all_results[0], all_results[1]],[m
[32m+[m[32m    ):[m
[32m+[m[32m        heights = [[m
[32m+[m[32m            -result[layer][head][m
[32m+[m[32m            for layer, head in top_heads[m
[32m+[m[32m            # if (layer, head) not in exclude_heads[m
[32m+[m[32m        ][m
[32m+[m[32m        colors = [[m
[32m+[m[32m            cc[what_class(layer, head_idx, circuit=circuit)][m
[32m+[m[32m            for layer, head_idx in top_heads[m
[32m+[m[32m            # if (layer, head_idx) not in exclude_heads[m
[32m+[m[32m        ][m
[32m+[m
[32m+[m[32m        # plot a bar chart[m
[32m+[m[32m        fig.add_trace([m
[32m+[m[32m            go.Bar([m
[32m+[m[32m                x=[str(x) for x in top_heads],  # if x not in exclude_heads],[m
[32m+[m[32m                y=heights,[m
[32m+[m[32m                orientation="v",[m
[32m+[m[32m                marker_color=colors,[m
[32m+[m[32m                name=name,[m
[32m+[m[32m            )[m
[32m+[m[32m        )[m
[32m+[m
[32m+[m[32m    # no legend[m
[32m+[m[32m    fig.update_layout(showlegend=False)[m
[32m+[m
[32m+[m[32m    # stack them[m
[32m+[m[32m    # set y axis range to [-1, 1][m
[32m+[m[32m    fig.update_yaxes(range=[-3, 3])[m
[32m+[m[32m    fname = f"Contribution to logit difference (left: wild type, right: after top three name mover knockout)"[m
[32m+[m
[32m+[m[32m    # update title[m
[32m+[m[32m    fig.update_layout(title=fname)[m
[32m+[m[32m    fig.write_image(f"svgs/{fname}.svg")[m
[32m+[m[32m    fig.show()[m
[32m+[m
[32m+[m[32m#%% [markdown] do the scatter plot, but now with direct effect on logit diff[m
[32m+[m[32mreceiver_hooks = [(f"blocks.{model.cfg.n_layers-1}.hook_resid_post", None)][m
[32m+[m[32mlayer = 9[m
[32m+[m[32mhead_idx = 9[m
[32m+[m[32mmodel.reset_hooks()[m
[32m+[m[32mhooks_database = [[]][m
[32m+[m[32mAVERAGE_OVER = 100[m
[32m+[m
[32m+[m[32mfor _ in tqdm(range(AVERAGE_OVER)):[m
[32m+[m
[32m+[m[32m    cur_all_diff_dataset = ([m
[32m+[m[32m        ioi_dataset.gen_flipped_prompts(("IO", "RAND"))[m
[32m+[m[32m        .gen_flipped_prompts(("S", "RAND"))[m
[32m+[m[32m        .gen_flipped_prompts(("S1", "RAND"), manual_word_idx=ioi_dataset.word_idx)[m
[32m+[m[32m    )[m
[32m+[m
[32m+[m[32m    hooks_database.append([m
[32m+[m[32m        direct_patch_and_freeze([m
[32m+[m[32m            model=model,[m
[32m+[m[32m            source_dataset=cur_all_diff_dataset,[m
[32m+[m[32m            target_dataset=ioi_dataset,[m
[32m+[m[32m            ioi_dataset=ioi_dataset,[m
[32m+[m[32m            sender_heads=[(layer, head_idx)],[m
[32m+[m[32m            receiver_hooks=receiver_hooks,[m
[32m+[m[32m            max_layer=12,[m
[32m+[m[32m            positions=["end"],[m
[32m+[m[32m            verbose=False,[m
[32m+[m[32m            return_hooks=True,[m
[32m+[m[32m        )[m
[32m+[m[32m    )[m
[32m+[m
[32m+[m[32mattention_hook_name = f"blocks.{layer}.attn.hook_attn"[m
[32m+[m[32m# (batch, head, from, to)[m
[32m+[m
[32m+[m[32mys = [][m
[32m+[m
[32m+[m[32mall_io_attentions = [][m
[32m+[m[32mall_s_attentions = [][m
[32m+[m[32mall_io_logits = [][m
[32m+[m[32mall_s_logits = [][m
[32m+[m
[32m+[m[32mfor hooks in tqdm(hooks_database):[m
[32m+[m[32m    model.reset_hooks()[m
[32m+[m[32m    for hook in hooks:[m
[32m+[m[32m        model.add_hook(*hook)[m
[32m+[m
[32m+[m[32m    cache = {}[m
[32m+[m[32m    model.cache_some(cache, lambda name: name == attention_hook_name)[m
[32m+[m
[32m+[m[32m    io_logits, s_logits = logit_diff(model, ioi_dataset, all=True, both=True)[m
[32m+[m[32m    io_logits = io_logits.detach().cpu()[m
[32m+[m[32m    s_logits = s_logits.detach().cpu()[m
[32m+[m
[32m+[m[32m    all_io_logits.append(io_logits)[m
[32m+[m[32m    all_s_logits.append(s_logits)[m
[32m+[m
[32m+[m[32m    for token, all_attentions in zip([m
[32m+[m[32m        ["IO", "S"], [all_io_attentions, all_s_attentions][m
[32m+[m[32m    ):[m
[32m+[m[32m        attention = ([m
[32m+[m[32m            cache[attention_hook_name][[m
[32m+[m[32m                torch.arange(ioi_dataset.N),[m
[32m+[m[32m                head_idx,[m
[32m+[m[32m                ioi_dataset.word_idx["end"],[m
[32m+[m[32m                ioi_dataset.word_idx[token],[m
[32m+[m[32m            ][m
[32m+[m[32m            .detach()[m
[32m+[m[32m            .cpu()[m
[32m+[m[32m        )[m
[32m+[m[32m        all_attentions.append(attention.clone())[m
[32m+[m
[32m+[m[32mfor i in range(2, AVERAGE_OVER + 1):[m
[32m+[m[32m    all_io_attentions[1] += all_io_attentions[i][m
[32m+[m[32m    all_s_attentions[1] += all_s_attentions[i][m
[32m+[m[32m    all_io_logits[1] += all_io_logits[i][m
[32m+[m[32m    all_s_logits[1] += all_s_logits[i][m
[32m+[m
[32m+[m[32mall_io_attentions[1] /= AVERAGE_OVER[m
[32m+[m[32mall_s_attentions[1] /= AVERAGE_OVER[m
[32m+[m[32mall_io_logits[1] /= AVERAGE_OVER[m
[32m+[m[32mall_s_logits[1] /= AVERAGE_OVER[m
[32m+[m
[32m+[m[32m# df.append([prob, dot, tok_type, prompt["text"]])[m
[32m+[m[32m# # most of the pandas stuff is intuitive, no need to deeply understand[m
[32m+[m[32m# viz_df = pd.DataFrame([m
[32m+[m[32m#     df, columns=[f"Attn Prob on Name", f"Dot w Name Embed", "Name Type", "text"][m
[32m+[m[32m# )[m
[32m+[m[32m# fig = px.scatter([m
[32m+[m[32m#     viz_df,[m
[32m+[m[32m#     x=f"Attn Prob on Name",[m
[32m+[m[32m#     y=f"Dot w Name Embed",[m
[32m+[m[32m#     color="Name Type",[m
[32m+[m[32m#     hover_data=["text"],[m
[32m+[m[32m#     color_discrete_sequence=["rgb(114,255,100)", "rgb(201,165,247)"],[m
[32m+[m[32m#     title=f"How Strong {layer_no}.{head_no} Writes in the Name Embed Direction Relative to Attn Prob",[m
[32m+[m[32m# )[m
[32m+[m
[32m+[m[32mdf = pd.concat([m
[32m+[m[32m    [[m
[32m+[m[32m        pd.DataFrame([m
[32m+[m[32m            {[m
[32m+[m[32m                "attention": all_s_attentions[0],[m
[32m+[m[32m                "change": -(all_s_logits[1] - all_s_logits[0]),[m
[32m+[m[32m                "token": "S",[m
[32m+[m[32m                "text": ioi_dataset.text_prompts,[m
[32m+[m[32m                # "change": (all_io_logits[0] - all_s_logits[0])[m
[32m+[m[32m                # - (all_io_logits[1] - all_s_logits[1]),[m
[32m+[m[32m            }[m
[32m+[m[32m        ),[m
[32m+[m[32m        pd.DataFrame([m
[32m+[m[32m            {[m
[32m+[m[32m                "attention": all_io_attentions[0],[m
[32m+[m[32m                "change": -(all_io_logits[1] - all_io_logits[0]),[m
[32m+[m[32m                "token": "IO",[m
[32m+[m[32m                "text": ioi_dataset.text_prompts,[m
[32m+[m[32m            }[m
[32m+[m[32m        ),[m
[32m+[m[32m    ][m
[32m+[m[32m)[m
[32m+[m
[32m+[m
[32m+[m[32mfig = px.scatter([m
[32m+[m[32m    df,[m
[32m+[m[32m    x=f"attention",[m
[32m+[m[32m    y=f"change",[m
[32m+[m[32m    color="token",[m
[32m+[m[32m    hover_data=["text"],[m
[32m+[m[32m    color_discrete_sequence=["rgb(114,255,100)", "rgb(201,165,247)"],[m
[32m+[m[32m    title=f"How {layer}.{head_idx} affects logits (change after patch-and-freeze)",[m
[32m+[m[32m)[m
[32m+[m
[32m+[m[32m# update y axis label[m
[32m+[m[32my_label = "Change in logits"[m
[32m+[m[32mfig.update_yaxes(title_text=y_label)[m
[32m+[m[32mfig.update_xaxes(title_text="Attention on token")[m
[32m+[m
[32m+[m[32mfig.write_image(f"svgs/attention_scatter_{y_label}_{ctime()}.svg")[m
[32m+[m[32mfig.write_image(f"svgs/attention_scatter_{y_label}_{ctime()}.png")[m
[32m+[m[32mfig.show()[m
[32m+[m
[32m+[m[32m# select where token == "IO" in df[m
[32m+[m[32mxs = df[df["token"] == "IO"]["attention"][m
[32m+[m[32mys = df[df["token"] == "IO"]["change"][m
[32m+[m
[32m+[m[32m# correlation coefficient[m
[32m+[m[32mprint(f"{np.corrcoef(xs, ys)[0, 1]}")[m
[32m+[m[32m#%%[m
[32m+[m[32mys = [][m
[32m+[m[32maverage_attention = {}[m
[32m+[m
[32m+[m[32mfor idx, dataset in enumerate([ioi_dataset]):[m
[32m+[m[32m    fig = go.Figure()[m
[32m+[m[32m    print(idx, ["ioi", "abca"][idx])[m
[32m+[m[32m    heads = [(10, 10)][m
[32m+[m[32m    heads_raw = (10, 10)[m
[32m+[m[32m    average_attention[heads_raw] = {}[m
[32m+[m[32m    cur_ys = [][m
[32m+[m[32m    cur_stds = [][m
[32m+[m[32m    att = torch.zeros(size=(dataset.N, dataset.max_len, dataset.max_len))[m
[32m+[m[32m    for head in tqdm(heads):[m
[32m+[m[32m        att += show_attention_patterns([m
[32m+[m[32m            model, [head], dataset, return_mtx=True, mode="scores"[m
[32m+[m[32m        )[m
[32m+[m[32m    att /= len(heads)[m
[32m+[m
[32m+[m[32m    vals = att[torch.arange(dataset.N), ioi_dataset.word_idx["end"][: dataset.N], :][m
[32m+[m[32m    evals = torch.exp(vals)[m
[32m+[m[32m    val_sum = torch.sum(evals, dim=1)[m
[32m+[m[32m    assert val_sum.shape == (dataset.N,), val_sum.shape[m
[32m+[m[32m    print(f"{heads} {val_sum.mean()}")[m
[32m+[m
[32m+[m[32m    for key in ioi_dataset.word_idx.keys():[m
[32m+[m[32m        end_to_s2 = att[[m
[32m+[m[32m            torch.arange(dataset.N),[m
[32m+[m[32m            ioi_dataset.word_idx["end"][: dataset.N],[m
[32m+[m[32m            ioi_dataset.word_idx[key][: dataset.N],[m
[32m+[m[32m        ][m
[32m+[m[32m        # ABCA dataset calculates S2 in trash way... so we use the IOI dataset indices[m
[32m+[m[32m        cur_ys.append(end_to_s2.mean().item())[m
[32m+[m[32m        cur_stds.append(end_to_s2.std().item())[m
[32m+[m[32m        average_attention[heads_raw][key] = end_to_s2.mean().item()[m
[32m+[m[32m    fig.add_trace([m
[32m+[m[32m        go.Bar([m
[32m+[m[32m            x=list(ioi_dataset.word_idx.keys()),[m
[32m+[m[32m            y=cur_ys,[m
[32m+[m[32m            error_y=dict(type="data", array=cur_stds),[m
[32m+[m[32m            name=str(heads_raw),[m
[32m+[m[32m        )[m
[32m+[m[32m    )  # ["IOI", "ABCA"][idx]))[m
[32m+[m
[32m+[m[32m    fig.update_layout(title_text="Attention scores; from END to S2")[m
[32m+[m[32m    fig.show()[m
[32m+[m[32m#%% [markdown back to old ioi_experiments stuff][m
[32m+[m[32m# text = ioi_dataset.text_prompts[0][m
[32m+[m[32m# probs, tokens = g(model, text)[m
 # %% [markdown][m
 # Each prompts is a dictionnary containing 'IO', 'S' and the "text", the sentence that will be given to the model.[m
 # The prompt type can be "ABBA", "BABA" or "mixed" (half of the previous two) depending on the pattern you want to study[m
 # %%[m
 # IOI Dataset initialisation[m
 N = 100[m
[31m-ioi_dataset_baba = IOIDataset(prompt_type="BABA", N=N, tokenizer=model.tokenizer)[m
[31m-ioi_dataset_abba = IOIDataset(prompt_type="ABBA", N=N, tokenizer=model.tokenizer)[m
[31m-ioi_dataset = IOIDataset(prompt_type="mixed", N=N, tokenizer=model.tokenizer)[m
[31m-abca_dataset = ioi_dataset.gen_flipped_prompts(("S2", "RAND"))  # we flip the second b for a random c[m
[32m+[m[32mioi_dataset_baba = IOIDataset([m
[32m+[m[32m    prompt_type="BABA",[m
[32m+[m[32m    N=N,[m
[32m+[m[32m    tokenizer=model.tokenizer,[m
[32m+[m[32m    prepend_bos=True,[m
[32m+[m[32m    has_start_padding_and_start_is_end=True,[m
[32m+[m[32m)[m
[32m+[m[32mioi_dataset_abba = IOIDataset([m
[32m+[m[32m    prompt_type="ABBA",[m
[32m+[m[32m    N=N,[m
[32m+[m[32m    tokenizer=model.tokenizer,[m
[32m+[m[32m    prepend_bos=True,[m
[32m+[m[32m    has_start_padding_and_start_is_end=True,[m
[32m+[m[32m)[m
[32m+[m[32mioi_dataset = IOIDataset([m
[32m+[m[32m    prompt_type="mixed",[m
[32m+[m[32m    N=N,[m
[32m+[m[32m    tokenizer=model.tokenizer,[m
[32m+[m[32m    prepend_bos=True,[m
[32m+[m[32m    has_start_padding_and_start_is_end=True,[m
[32m+[m[32m)[m
[32m+[m[32mabca_dataset = ioi_dataset.gen_flipped_prompts([m
[32m+[m[32m    ("S2", "RAND")[m
[32m+[m[32m)  # we flip the second b for a random c[m
 pprint(abca_dataset.text_prompts[:5])[m
[31m-[m
[32m+[m[32mall_diff_dataset = ([m
[32m+[m[32m    ioi_dataset.gen_flipped_prompts(("IO", "RAND"))[m
[32m+[m[32m    .gen_flipped_prompts(("S", "RAND"))[m
[32m+[m[32m    .gen_flipped_prompts(("S1", "RAND"), manual_word_idx=ioi_dataset.word_idx)[m
[32m+[m[32m)[m
 abca_dataset_abba = ioi_dataset_abba.gen_flipped_prompts(("S2", "RAND"))[m
 abca_dataset_baba = ioi_dataset_baba.gen_flipped_prompts(("S2", "RAND"))[m
 [m
[36m@@ -161,12 +838,19 @@[m [mdef logit_diff(model, ioi_dataset, all=False):[m
     if all:[m
         return IO_logits - S_logits[m
     return (IO_logits - S_logits).mean().detach().cpu()[m
[32m+[m
[32m+[m
 # %% [markdown][m
 # ioi_dataset `ioi_dataset.word_idx` contains the indices of certains special words in each prompt. Example on the prompt 0[m
 # %%[m
 [(k, int(ioi_dataset.word_idx[k][0])) for k in ioi_dataset.word_idx.keys()][m
 # %%[m
[31m-[(i, t) for (i, t) in enumerate(show_tokens(ioi_dataset.ioi_prompts[0]["text"], model, return_list=True))][m
[32m+[m[32m[[m
[32m+[m[32m    (i, t)[m
[32m+[m[32m    for (i, t) in enumerate([m
[32m+[m[32m        show_tokens(ioi_dataset.ioi_prompts[0]["text"], model, return_list=True)[m
[32m+[m[32m    )[m
[32m+[m[32m][m
 # %% [markdown][m
 # The `ioi_dataset` can also generate a copy of itself where some names have been flipped by a random name that is unrelated to the context with `gen_flipped_prompts`. This will be useful for patching experiments.[m
 # %%[m
[36m@@ -179,7 +863,11 @@[m [mpprint(flipped.ioi_prompts[:5])[m
 # %%[m
 webtext = load_dataset("stas/openwebtext-10k")[m
 owb_seqs = [[m
[31m-    "".join(show_tokens(webtext["train"]["text"][i][:2000], model, return_list=True)[: ioi_dataset.max_len])[m
[32m+[m[32m    "".join([m
[32m+[m[32m        show_tokens(webtext["train"]["text"][i][:2000], model, return_list=True)[[m
[32m+[m[32m            : ioi_dataset.max_len[m
[32m+[m[32m        ][m
[32m+[m[32m    )[m
     for i in range(ioi_dataset.N)[m
 ][m
 # %% [markdown][m
[36m@@ -189,11 +877,15 @@[m [mowb_seqs = [[m
 # %% [markdown][m
 # The first series of experiment: we define our metric, here, how much the logit for IO is bigger than S, we ablate part of the network and see what matters. Globally, it shows that the behavior is distributed accross many parts of the network, we cannot draw much conclusion from this alone.[m
 # %%[m
[31m-def mean_at_end(z, mean, hook):  # to ablate at particular indices, we have to define a custom ablation function[m
[31m-    z[torch.arange(len(ioi_dataset.ioi_prompts)), ioi_dataset.word_idx["end"], :] = mean[[m
[32m+[m[32mdef mean_at_end([m
[32m+[m[32m    z, mean, hook[m
[32m+[m[32m):  # to ablate at particular indices, we have to define a custom ablation function[m
[32m+[m[32m    z[[m
         torch.arange(len(ioi_dataset.ioi_prompts)), ioi_dataset.word_idx["end"], :[m
[31m-    ][m
[32m+[m[32m    ] = mean[torch.arange(len(ioi_dataset.ioi_prompts)), ioi_dataset.word_idx["end"], :][m
     return z[m
[32m+[m
[32m+[m
 # %%[m
 metric = ExperimentMetric(metric=logit_diff, dataset=ioi_dataset, relative_metric=True)[m
 config_mlp = AblationConfig([m
[36m@@ -230,7 +922,9 @@[m [mfig = px.imshow([m
     color_continuous_scale="RdBu",[m
 )[m
 [m
[31m-fig.update_layout(yaxis=dict(tickmode="array", tickvals=[0, 1], ticktext=["mlp", "attention layer"]))[m
[32m+[m[32mfig.update_layout([m
[32m+[m[32m    yaxis=dict(tickmode="array", tickvals=[0, 1], ticktext=["mlp", "attention layer"])[m
[32m+[m[32m)[m
 fig.show()[m
 #%% Mean everywhere[m
 config_mlp = AblationConfig([m
[36m@@ -265,7 +959,9 @@[m [mfig = px.imshow([m
     color_continuous_scale="RdBu",[m
 )[m
 [m
[31m-fig.update_layout(yaxis=dict(tickmode="array", tickvals=[0, 1], ticktext=["mlp", "attention layer"]))[m
[32m+[m[32mfig.update_layout([m
[32m+[m[32m    yaxis=dict(tickmode="array", tickvals=[0, 1], ticktext=["mlp", "attention layer"])[m
[32m+[m[32m)[m
 fig.show()[m
 # %% random ablation[m
 metric = ExperimentMetric(metric=logit_diff, dataset=ioi_dataset, relative_metric=True)[m
[36m@@ -305,7 +1001,9 @@[m [mfig = px.imshow([m
     color_continuous_scale="RdBu",[m
 )[m
 [m
[31m-fig.update_layout(yaxis=dict(tickmode="array", tickvals=[0, 1], ticktext=["mlp", "attention layer"]))[m
[32m+[m[32mfig.update_layout([m
[32m+[m[32m    yaxis=dict(tickmode="array", tickvals=[0, 1], ticktext=["mlp", "attention layer"])[m
[32m+[m[32m)[m
 fig.show()[m
 # %% ABC-mean ablation of mlps[m
 metric = ExperimentMetric(metric=logit_diff, dataset=ioi_dataset, relative_metric=True)[m
[36m@@ -475,6 +1173,7 @@[m [mdef layer_norm(x, cfg=MODEL_CFG):[m
 def pytorch_layer_norm(x, eps=MODEL_EPS):[m
     return torch.nn.LayerNorm(normalized_shape=x.shape[-1], eps=eps)(x)[m
 [m
[32m+[m
 m = torch.randn(2, 3, 4)[m
 assert torch.allclose(layer_norm(m), pytorch_layer_norm(m))[m
 [m
[36m@@ -511,10 +1210,18 @@[m [mdef writing_direction_heatmap([m
     logit_diffs = logit_diff(model, ioi_dataset, all=True).cpu()[m
 [m
     for i in tqdm(range(ioi_dataset.N)):[m
[31m-        io_tok = ioi_dataset.toks[i][ioi_dataset.word_idx["IO"][i].item()][m
[31m-        s_tok = ioi_dataset.toks[i][ioi_dataset.word_idx["S"][i].item()][m
[31m-        io_dir = model_unembed[io_tok][m
[31m-        s_dir = model_unembed[s_tok][m
[32m+[m[32m        toks = ioi_dataset[i : i + 1].toks.long()[m
[32m+[m[32m        # print(toks.shape)[m
[32m+[m[32m        io_tok = toks[0][ioi_dataset.word_idx["IO"][i].item()][m
[32m+[m[32m        s_tok = toks[0][ioi_dataset.word_idx["S"][i].item()][m
[32m+[m[32m        io_dir = model_unembed[:, io_tok][m
[32m+[m[32m        try:[m
[32m+[m[32m            s_dir = model_unembed[:, s_tok][m
[32m+[m[32m        except:[m
[32m+[m[32m            # print(f"Error with {i}th prompt")[m
[32m+[m[32m            print(model_unembed.shape)[m
[32m+[m[32m            # print()[m
[32m+[m[32m            assert False[m
         unembed_bias_io = unembed_bias[io_tok][m
         unembed_bias_s = unembed_bias[s_tok][m
         if dir_mode == "IO - S":[m
[36m@@ -527,14 +1234,24 @@[m [mdef writing_direction_heatmap([m
             raise NotImplementedError()[m
         dire.to("cuda")[m
         cache = {}[m
[31m-        model.cache_all(cache, device="cuda")  # TODO maybe speed up by only caching relevant things[m
[31m-        logits = model(ioi_dataset.text_prompts[i])[m
[31m-[m
[31m-        res_stream_sum = torch.zeros(size=(d_model,), device="cuda") # cuda implem to speed up things[m
[32m+[m[32m        model.cache_all([m
[32m+[m[32m            cache, device="cuda"[m
[32m+[m[32m        )  # TODO maybe speed up by only caching relevant things[m
[32m+[m[32m        toks = ioi_dataset[i : i + 1].toks.long()[m
[32m+[m[32m        print(toks)[m
[32m+[m[32m        logits = model(toks)  # text_prompts[i])[m
[32m+[m[32m        #  print(f"{cache.keys()}")[m
[32m+[m
[32m+[m[32m        res_stream_sum = torch.zeros([m
[32m+[m[32m            size=(d_model,), device="cuda"[m
[32m+[m[32m        )  # cuda implem to speed up things[m
         res_stream_sum += cache["blocks.0.hook_resid_pre"][0, -2, :]  # .detach().cpu()[m
         # the pos and token embeddings[m
 [m
[31m-        layer_norm_div = get_layer_norm_div(cache["blocks.11.hook_resid_post"][0, -2, :])[m
[32m+[m[32m        warnings.warn("Set this to the last layer of model!!!")[m
[32m+[m[32m        layer_norm_div = get_layer_norm_div([m
[32m+[m[32m            cache[f"blocks.{model.cfg.n_layers - 1}.hook_resid_post"][0, -2, :][m
[32m+[m[32m        )[m
 [m
         for lay in range(n_layers):[m
             cur_attn = ([m
[36m@@ -564,27 +1281,51 @@[m [mdef writing_direction_heatmap([m
             cur_attn /= layer_norm_div  # ... and then apply the layer norm division[m
             cur_mlp /= layer_norm_div[m
 [m
[31m-            attn_vals[:n_heads, lay] += torch.einsum("ha,a->h", cur_attn.cpu(), dire.cpu())[m
[32m+[m[32m            attn_vals[:n_heads, lay] += torch.einsum([m
[32m+[m[32m                "ha,a->h", cur_attn.cpu(), dire.cpu()[m
[32m+[m[32m            )[m
             mlp_vals[lay] = torch.einsum("a,a->", cur_mlp.cpu(), dire.cpu())[m
 [m
         res_stream_sum -= res_stream_sum.mean()[m
[31m-        res_stream_sum = layer_norm(res_stream_sum.unsqueeze(0).unsqueeze(0)).squeeze(0).squeeze(0)[m
[31m-        cur_writing = torch.einsum("a,a->", res_stream_sum, dire.to("cuda")) + unembed_bias_io - unembed_bias_s[m
[32m+[m[32m        res_stream_sum = ([m
[32m+[m[32m            layer_norm(res_stream_sum.unsqueeze(0).unsqueeze(0)).squeeze(0).squeeze(0)[m
[32m+[m[32m        )[m
[32m+[m[32m        cur_writing = ([m
[32m+[m[32m            torch.einsum("a,a->", res_stream_sum, dire.to("cuda"))[m
[32m+[m[32m            + unembed_bias_io[m
[32m+[m[32m            - unembed_bias_s[m
[32m+[m[32m        )[m
 [m
         assert i == 11 or torch.allclose(  # ??? and it's way off, too[m
             cur_writing,[m
             logit_diffs[i],[m
[31m-            rtol=1e-2,[m
[31m-            atol=1e-2,[m
[32m+[m[32m            rtol=1e-3,[m
[32m+[m[32m            atol=1e-3,[m
         ), f"{i} {cur_writing} {logit_diffs[i]}"[m
 [m
     attn_vals /= ioi_dataset.N[m
     mlp_vals /= ioi_dataset.N[m
     all_figs = [][m
     if "attn" in show:[m
[31m-        all_figs.append(show_pp(attn_vals, xlabel="head no", ylabel="layer no", title=title, return_fig=True))[m
[32m+[m[32m        all_figs.append([m
[32m+[m[32m            show_pp([m
[32m+[m[32m                attn_vals,[m
[32m+[m[32m                xlabel="head no",[m
[32m+[m[32m                ylabel="layer no",[m
[32m+[m[32m                title=title,[m
[32m+[m[32m                return_fig=True,[m
[32m+[m[32m            )[m
[32m+[m[32m        )[m
     if "mlp" in show:[m
[31m-        all_figs.append(show_pp(mlp_vals.unsqueeze(0).T, xlabel="", ylabel="layer no", title=title, return_fig=True))[m
[32m+[m[32m        all_figs.append([m
[32m+[m[32m            show_pp([m
[32m+[m[32m                mlp_vals.unsqueeze(0).T,[m
[32m+[m[32m                xlabel="",[m
[32m+[m[32m                ylabel="layer no",[m
[32m+[m[32m                title=title,[m
[32m+[m[32m                return_fig=True,[m
[32m+[m[32m            )[m
[32m+[m[32m        )[m
     if return_figs and return_vals:[m
         return all_figs, attn_vals, mlp_vals[m
     if return_vals:[m
[36m@@ -597,7 +1338,7 @@[m [mtorch.cuda.empty_cache()[m
 [m
 all_figs, attn_vals, mlp_vals = writing_direction_heatmap([m
     model,[m
[31m-    ioi_dataset,[m
[32m+[m[32m    ioi_dataset[:10],[m
     return_vals=True,[m
     show=["attn", "mlp"],[m
     dir_mode="IO - S",[m
[36m@@ -616,7 +1357,9 @@[m [mshow_attention_patterns(model, [(9, 9), (9, 6), (10, 0)], ioi_dataset[:1])[m
 # %%[m
 show_attention_patterns(model, [(11, 10), (10, 7)], ioi_dataset[:1])[m
 #%%[m
[31m-att = show_attention_patterns(model, [(8, 10)], abca_dataset[:2], return_mtx=True, mode="attn")[m
[32m+[m[32matt = show_attention_patterns([m
[32m+[m[32m    model, [(8, 10)], abca_dataset[:2], return_mtx=True, mode="attn"[m
[32m+[m[32m)[m
 [m
 [m
 # %% [markdown][m
[36m@@ -637,7 +1380,9 @@[m [mattn_vals = writing_direction_heatmap([m
 k = 20[m
 print(f"Top {k} heads (by magnitude):")[m
 all_grps = [][m
[31m-top_heads, vals = max_2d(torch.abs(attn_vals.T), k=k)  # remove abs to just get positive contributors[m
[32m+[m[32mtop_heads, vals = max_2d([m
[32m+[m[32m    torch.abs(attn_vals.T), k=k[m
[32m+[m[32m)  # remove abs to just get positive contributors[m
 for i in range(len(top_heads)):[m
     grp = "None"[m
     for gr in CIRCUIT.keys():[m
[36m@@ -701,7 +1446,9 @@[m [mfrom ioi_utils import get_heads_from_nodes[m
 model.reset_hooks()[m
 model, _ = do_circuit_extraction([m
     model=model,[m
[31m-    heads_to_remove=get_heads_from_nodes([((9, 9), "end"), ((10, 0), "end"), ((9, 6), "end")], ioi_dataset),  # C\J[m
[32m+[m[32m    heads_to_remove=get_heads_from_nodes([m
[32m+[m[32m        [((9, 9), "end"), ((10, 0), "end"), ((9, 6), "end")], ioi_dataset[m
[32m+[m[32m    ),  # C\J[m
     mlps_to_remove={},[m
     ioi_dataset=ioi_dataset,[m
     mean_dataset=abca_dataset,[m
[36m@@ -764,7 +1511,9 @@[m [mdef check_copy_circuit(model, layer, head, ioi_dataset, verbose=False, neg=False[m
         for word in ["IO", "S", "S2"]:[m
             pred_tokens = [[m
                 model.tokenizer.decode(token)[m
[31m-                for token in torch.topk(logits[seq_idx, ioi_dataset.word_idx[word][seq_idx]], k).indices[m
[32m+[m[32m                for token in torch.topk([m
[32m+[m[32m                    logits[seq_idx, ioi_dataset.word_idx[word][seq_idx]], k[m
[32m+[m[32m                ).indices[m
             ][m
             if "S" in word:[m
                 name = "S"[m
[36m@@ -783,7 +1532,9 @@[m [mdef check_copy_circuit(model, layer, head, ioi_dataset, verbose=False, neg=False[m
                                 f"({i+1}):{model.tokenizer.decode(token)}"[m
                                 for i, token in enumerate([m
                                     torch.topk([m
[31m-                                        logits[seq_idx, ioi_dataset.word_idx[word][seq_idx]],[m
[32m+[m[32m                                        logits[[m
[32m+[m[32m                                            seq_idx, ioi_dataset.word_idx[word][seq_idx][m
[32m+[m[32m                                        ],[m
                                         k,[m
                                     ).indices[m
                                 )[m
[36m@@ -791,7 +1542,9 @@[m [mdef check_copy_circuit(model, layer, head, ioi_dataset, verbose=False, neg=False[m
                         )[m
                     )[m
     percent_right = (n_right / (ioi_dataset.N * 3)) * 100[m
[31m-    print(f"Copy circuit for head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%")[m
[32m+[m[32m    print([m
[32m+[m[32m        f"Copy circuit for head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%"[m
[32m+[m[32m    )[m
     return percent_right[m
 [m
 [m
[36m@@ -807,9 +1560,15 @@[m [mcheck_copy_circuit(model, 10, 7, ioi_dataset, neg=neg_sign)[m
 check_copy_circuit(model, 11, 10, ioi_dataset, neg=neg_sign)[m
 [m
 print(" ---  Random heads for control ---  ")[m
[31m-check_copy_circuit(model, random.randint(0, 11), random.randint(0, 11), ioi_dataset, neg=neg_sign)[m
[31m-check_copy_circuit(model, random.randint(0, 11), random.randint(0, 11), ioi_dataset, neg=neg_sign)[m
[31m-check_copy_circuit(model, random.randint(0, 11), random.randint(0, 11), ioi_dataset, neg=neg_sign)[m
[32m+[m[32mcheck_copy_circuit([m
[32m+[m[32m    model, random.randint(0, 11), random.randint(0, 11), ioi_dataset, neg=neg_sign[m
[32m+[m[32m)[m
[32m+[m[32mcheck_copy_circuit([m
[32m+[m[32m    model, random.randint(0, 11), random.randint(0, 11), ioi_dataset, neg=neg_sign[m
[32m+[m[32m)[m
[32m+[m[32mcheck_copy_circuit([m
[32m+[m[32m    model, random.randint(0, 11), random.randint(0, 11), ioi_dataset, neg=neg_sign[m
[32m+[m[32m)[m
 #%% [markdown][m
 # For calibration heads, we observe a reverse trend to name movers, the more is pays attention to a name, the more it write in its *oposite* direction. Why is that?[m
 # You need to remember the training objective of the transformer: it has to predict accurate probability distribution over all the next tokens.[m
[36m@@ -818,8 +1577,12 @@[m [mcheck_copy_circuit(model, random.randint(0, 11), random.randint(0, 11), ioi_data[m
 [m
 # You can see this similarly as open loop / closed loop optimization. It's easier to make a good guess by making previous rough estimate more precise than making a good guess in one shot.[m
 #%%[m
[31m-scatter_attention_and_contribution(model, 10, 7, ioi_dataset.ioi_prompts[:500], gpt_model="gpt2")[m
[31m-scatter_attention_and_contribution(model, 11, 10, ioi_dataset.ioi_prompts[:500], gpt_model="gpt2")[m
[32m+[m[32mscatter_attention_and_contribution([m
[32m+[m[32m    model, 10, 7, ioi_dataset.ioi_prompts[:500], gpt_model="gpt2"[m
[32m+[m[32m)[m
[32m+[m[32mscatter_attention_and_contribution([m
[32m+[m[32m    model, 11, 10, ioi_dataset.ioi_prompts[:500], gpt_model="gpt2"[m
[32m+[m[32m)[m
 # %% [markdown][m
 # ### Patching experiments[m
 # %% [markdown][m
[36m@@ -843,12 +1606,16 @@[m [mdef attention_probs([m
 ):  # we have to redefine logit differences to use the new abba dataset[m
     """Difference between the IO and the S logits at the "to" token"""[m
     cache_patched = {}[m
[31m-    model.cache_some(cache_patched, lambda x: x in hook_names)  # we only cache the activation we're interested[m
[32m+[m[32m    model.cache_some([m
[32m+[m[32m        cache_patched, lambda x: x in hook_names[m
[32m+[m[32m    )  # we only cache the activation we're interested[m
     logits = model(text_prompts).detach()[m
     # we want to measure Mean(Patched/baseline) and not Mean(Patched)/Mean(baseline)[m
     model.reset_hooks()[m
     cache_baseline = {}[m
[31m-    model.cache_some(cache_baseline, lambda x: x in hook_names)  # we only cache the activation we're interested[m
[32m+[m[32m    model.cache_some([m
[32m+[m[32m        cache_baseline, lambda x: x in hook_names[m
[32m+[m[32m    )  # we only cache the activation we're interested[m
     logits = model(text_prompts).detach()[m
     # attn score of head HEAD at token "to" (end) to token IO[m
 [m
[36m@@ -872,14 +1639,22 @@[m [mdef attention_probs([m
                 ][m
                 if variation:[m
                     attn_probs_variation.append([m
[31m-                        ((attn_probs_patched - attn_probs_base) / attn_probs_base).mean().unsqueeze(dim=0)[m
[32m+[m[32m                        ((attn_probs_patched - attn_probs_base) / attn_probs_base)[m
[32m+[m[32m                        .mean()[m
[32m+[m[32m                        .unsqueeze(dim=0)[m
                     )[m
                 else:[m
[31m-                    attn_probs_variation.append(attn_probs_patched.mean().unsqueeze(dim=0))[m
[31m-        attn_probs_variation_by_keys.append(torch.cat(attn_probs_variation).mean(dim=0, keepdim=True))[m
[32m+[m[32m                    attn_probs_variation.append([m
[32m+[m[32m                        attn_probs_patched.mean().unsqueeze(dim=0)[m
[32m+[m[32m                    )[m
[32m+[m[32m        attn_probs_variation_by_keys.append([m
[32m+[m[32m            torch.cat(attn_probs_variation).mean(dim=0, keepdim=True)[m
[32m+[m[32m        )[m
 [m
     attn_probs_variation_by_keys = torch.cat(attn_probs_variation_by_keys, dim=0)[m
     return attn_probs_variation_by_keys.detach().cpu()[m
[32m+[m
[32m+[m
 # %%[m
 circuit = CIRCUIT.copy()[m
 average_changes = torch.zeros(size=(12, 12, 3))[m
[36m@@ -905,12 +1680,16 @@[m [mfor idx, (layer, head_idx) in enumerate(tqdm(circuit["name mover"])):[m
         head_circuit="result",  # we patch "result", the result of the attention head[m
         cache_act=True,[m
         verbose=False,[m
[31m-        patch_fn=partial(patch_particular_token, token_type="IO"),  # AND CHANGE THIS SHIT![m
[32m+[m[32m        patch_fn=partial([m
[32m+[m[32m            patch_particular_token, token_type="IO"[m
[32m+[m[32m        ),  # AND CHANGE THIS SHIT![m
         layers=(0, layer),[m
     )  # we stop at layer "LAYER" because it's useless to patch after layer 9 if what we measure is attention of a head at layer 9.[m
 [m
 # %%[m
[31m-def patch_positions(z, source_act, hook, positions=["S2"]):  # we patch at the "to" token[m
[32m+[m[32mdef patch_positions([m
[32m+[m[32m    z, source_act, hook, positions=["S2"][m
[32m+[m[32m):  # we patch at the "to" token[m
     for pos in positions:[m
         z[torch.arange(ioi_dataset.N), ioi_dataset.word_idx[pos]] = source_act[[m
             torch.arange(ioi_dataset.N), ioi_dataset.word_idx[pos][m
[36m@@ -932,7 +1711,9 @@[m [mconfig = PatchingConfig([m
     layers=(0, max(layers) - 1),[m
 )  # we stop at layer "LAYER" because it's useless to patch after layer 9 if what we measure is attention of a head at layer 9.[m
 [m
[31m-metric = ExperimentMetric(attention_probs, config.target_dataset, relative_metric=False, scalar_metric=False)[m
[32m+[m[32mmetric = ExperimentMetric([m
[32m+[m[32m    attention_probs, config.target_dataset, relative_metric=False, scalar_metric=False[m
[32m+[m[32m)[m
 [m
 patching = EasyPatching(model, config, metric)[m
 result = patching.run_patching()[m
[36m@@ -945,7 +1726,9 @@[m [mfor i, key in enumerate(["IO", "S", "S2"]):[m
         color_continuous_midpoint=0,[m
         color_continuous_scale="RdBu",[m
     )[m
[31m-    fig.write_image(f"svgs/variation_average_nm_attn_prob_key_{key}_patching_ABC_END.svg")[m
[32m+[m[32m    fig.write_image([m
[32m+[m[32m        f"svgs/variation_average_nm_attn_prob_key_{key}_patching_ABC_END.svg"[m
[32m+[m[32m    )[m
     fig.show()[m
 # %% [markdown][m
 # We can clearly identify the S2-inhibition heads: 8.6, 8.10, 7.3 and 7.9. Patching them with activation from ABC causes 9.9 to pay less attention to IO and more to S and S2. To have a a better sense of what is going on behind these plots, we can see how patching impact the attention patterns of 9.9 on sample sentences.[m
[36m@@ -970,7 +1753,12 @@[m [mabl_config = AblationConfig([m
     layers=(0, max(layers) - 1),[m
 )[m
 [m
[31m-metric = ExperimentMetric(attention_probs, ioi_dataset.text_prompts, relative_metric=False, scalar_metric=False)[m
[32m+[m[32mmetric = ExperimentMetric([m
[32m+[m[32m    attention_probs,[m
[32m+[m[32m    ioi_dataset.text_prompts,[m
[32m+[m[32m    relative_metric=False,[m
[32m+[m[32m    scalar_metric=False,[m
[32m+[m[32m)[m
 [m
 ablation = EasyAblation([m
     model,[m
[36m@@ -992,7 +1780,9 @@[m [mfor i, key in enumerate(["IO", "S", "S2"]):[m
         color_continuous_scale="RdBu",[m
     )[m
 [m
[31m-    fig.write_image(f"svgs/ABC-ablation at END average nm probs to key-{key} at {ctime()}.svg")[m
[32m+[m[32m    fig.write_image([m
[32m+[m[32m        f"svgs/ABC-ablation at END average nm probs to key-{key} at {ctime()}.svg"[m
[32m+[m[32m    )[m
     fig.show()[m
 # %%[m
 print_gpu_mem()[m
[36m@@ -1014,7 +1804,9 @@[m [mshow_attention_patterns([m
 # %%[m
 def one_sentence_patching(z, source_act, hook):  # we patch at the "to" token[m
     # print(source_act.shape, z.shape)[m
[31m-    z[0, ioi_dataset.word_idx["end"][IDX]] = source_act[0, ioi_dataset.word_idx["end"][IDX]][m
[32m+[m[32m    z[0, ioi_dataset.word_idx["end"][IDX]] = source_act[[m
[32m+[m[32m        0, ioi_dataset.word_idx["end"][IDX][m
[32m+[m[32m    ][m
     return z[m
 [m
 [m
[36m@@ -1056,7 +1848,9 @@[m [mshow_attention_patterns([m
 # Attention pattern of S2-inihibition heads. They seems to generally track the subject on key words such as "and" and "to".[m
 [m
 # %%[m
[31m-show_attention_patterns(model, [(7, 3), (7, 9), (8, 6), (8, 10)], ioi_dataset[IDX : IDX + 1])[m
[32m+[m[32mshow_attention_patterns([m
[32m+[m[32m    model, [(7, 3), (7, 9), (8, 6), (8, 10)], ioi_dataset[IDX : IDX + 1][m
[32m+[m[32m)[m
 [m
 # %% [markdown][m
 # #### Patching at S2[m
[36m@@ -1079,7 +1873,9 @@[m [mconfig = PatchingConfig([m
     layers=(0, max(layers) - 1),[m
 )[m
 [m
[31m-metric = ExperimentMetric(attention_probs, config.target_dataset, relative_metric=False, scalar_metric=False)[m
[32m+[m[32mmetric = ExperimentMetric([m
[32m+[m[32m    attention_probs, config.target_dataset, relative_metric=False, scalar_metric=False[m
[32m+[m[32m)[m
 patching = EasyPatching(model, config, metric)[m
 result = patching.run_patching()[m
 [m
[36m@@ -1104,7 +1900,9 @@[m [mHEAD = 5[m
 [m
 def metric_function(model, text_prompts):[m
     # return attention_on_token(model, ioi_dataset[:len(text_prompts)], layer=LAYER, head_idx=HEAD, token="S2")[m
[31m-    return attention_on_token(model, ioi_dataset[: len(text_prompts)], layer=LAYER, head_idx=HEAD, token="S+1")[m
[32m+[m[32m    return attention_on_token([m
[32m+[m[32m        model, ioi_dataset[: len(text_prompts)], layer=LAYER, head_idx=HEAD, token="S+1"[m
[32m+[m[32m    )[m
 [m
 [m
 # metric_function =[m
[36m@@ -1119,7 +1917,9 @@[m [mconfig = PatchingConfig([m
     patch_fn=patcher,[m
     layers=(0, LAYER - 1),[m
 )[m
[31m-metric = ExperimentMetric(metric_function, config.target_dataset, relative_metric=False, scalar_metric=False)[m
[32m+[m[32mmetric = ExperimentMetric([m
[32m+[m[32m    metric_function, config.target_dataset, relative_metric=False, scalar_metric=False[m
[32m+[m[32m)[m
 patching = EasyPatching(model, config, metric)[m
 result = patching.run_patching()[m
 #%%[m
[36m@@ -1156,7 +1956,12 @@[m [mabl_config = AblationConfig([m
     layers=(0, max(layers) - 1),[m
 )[m
 [m
[31m-metric = ExperimentMetric(attention_probs, ioi_dataset.text_prompts, relative_metric=False, scalar_metric=False)[m
[32m+[m[32mmetric = ExperimentMetric([m
[32m+[m[32m    attention_probs,[m
[32m+[m[32m    ioi_dataset.text_prompts,[m
[32m+[m[32m    relative_metric=False,[m
[32m+[m[32m    scalar_metric=False,[m
[32m+[m[32m)[m
 [m
 ablation = EasyAblation([m
     model,[m
[36m@@ -1178,7 +1983,9 @@[m [mfor i, key in enumerate(["IO", "S", "S2"]):[m
         color_continuous_scale="RdBu",[m
     )[m
 [m
[31m-    fig.write_image(f"svgs/ABC-ablation at S2 average nm probs to key-{key} at {ctime()}.svg")[m
[32m+[m[32m    fig.write_image([m
[32m+[m[32m        f"svgs/ABC-ablation at S2 average nm probs to key-{key} at {ctime()}.svg"[m
[32m+[m[32m    )[m
     fig.show()[m
 [m
 # %% [markdown][m
[36m@@ -1228,7 +2035,9 @@[m [mconfig = PatchingConfig([m
     layers=(0, max(layers) - 1),[m
 )[m
 [m
[31m-metric = ExperimentMetric(attention_probs, config.target_dataset, relative_metric=False, scalar_metric=False)[m
[32m+[m[32mmetric = ExperimentMetric([m
[32m+[m[32m    attention_probs, config.target_dataset, relative_metric=False, scalar_metric=False[m
[32m+[m[32m)[m
 [m
 patching = EasyPatching(model, config, metric)[m
 result = patching.run_patching()[m
[36m@@ -1263,7 +2072,12 @@[m [mabl_config = AblationConfig([m
     layers=(0, max(layers) - 1),[m
 )[m
 [m
[31m-metric = ExperimentMetric(attention_probs, ioi_dataset.text_prompts, relative_metric=False, scalar_metric=False)[m
[32m+[m[32mmetric = ExperimentMetric([m
[32m+[m[32m    attention_probs,[m
[32m+[m[32m    ioi_dataset.text_prompts,[m
[32m+[m[32m    relative_metric=False,[m
[32m+[m[32m    scalar_metric=False,[m
[32m+[m[32m)[m
 [m
 ablation = EasyAblation([m
     model,[m
[36m@@ -1285,13 +2099,17 @@[m [mfor i, key in enumerate(["IO", "S", "S2"]):[m
         color_continuous_scale="RdBu",[m
     )[m
 [m
[31m-    fig.write_image(f"svgs/ABC-ablation at S+1 average nm probs to key-{key} at {ctime()}.svg")[m
[32m+[m[32m    fig.write_image([m
[32m+[m[32m        f"svgs/ABC-ablation at S+1 average nm probs to key-{key} at {ctime()}.svg"[m
[32m+[m[32m    )[m
     fig.show()[m
 # %% [markdown][m
 # It seems that the heads 4.11, 4.7, 4.3, 2.2 and 5.6 are important. Let's look at their patterns. The majority of them look like they're attending to the previous tokens.[m
 [m
 # %%[m
[31m-show_attention_patterns(model, [(4, 7), (5, 6), (4, 11), (2, 2), (4, 3)], ioi_dataset[34:35])[m
[32m+[m[32mshow_attention_patterns([m
[32m+[m[32m    model, [(4, 7), (5, 6), (4, 11), (2, 2), (4, 3)], ioi_dataset[34:35][m
[32m+[m[32m)[m
 [m
 # %% [markdown][m
 # Here is the (approximative) story of what's going on here:[m
[36m@@ -1374,7 +2192,9 @@[m [mdef score(model, ioi_dataset, all=False, verbose=False):[m
     assert len(list(end_logits.shape)) == 2, end_logits.shape[m
     top_10s_standard = torch.topk(end_logits, dim=1, k=10).values[:, -1][m
     good_enough = end_logits > top_10s_standard.unsqueeze(-1)[m
[31m-    selected_logits = good_enough[torch.arange(len(text_prompts)), ioi_dataset.io_tokenIDs[:L]][m
[32m+[m[32m    selected_logits = good_enough[[m
[32m+[m[32m        torch.arange(len(text_prompts)), ioi_dataset.io_tokenIDs[:L][m
[32m+[m[32m    ][m
 [m
     # is IO > S ???[m
     IO_logits = logits[[m
[36m@@ -1390,7 +2210,9 @@[m [mdef score(model, ioi_dataset, all=False, verbose=False):[m
     IO_greater_than_S = (IO_logits - S_logits) > 0[m
 [m
     # calculate percentage passing both tests[m
[31m-    answer = torch.sum((selected_logits & IO_greater_than_S).float()).detach().cpu() / len(text_prompts)[m
[32m+[m[32m    answer = torch.sum([m
[32m+[m[32m        (selected_logits & IO_greater_than_S).float()[m
[32m+[m[32m    ).detach().cpu() / len(text_prompts)[m
 [m
     selected = torch.sum(selected_logits) / len(text_prompts)[m
     greater = torch.sum(IO_greater_than_S) / len(text_prompts)[m
[36m@@ -1437,13 +2259,17 @@[m [mfor ablate_negative in [[m
     ld_data = [][m
     for template_idx in tqdm(range(num_templates)):[m
         prompts = template_prompts[template_idx][m
[31m-        ioi_dataset = IOIDataset(prompt_type=template_type, N=N, symmetric=False, prompts=prompts)[m
[32m+[m[32m        ioi_dataset = IOIDataset([m
[32m+[m[32m            prompt_type=template_type, N=N, symmetric=False, prompts=prompts[m
[32m+[m[32m        )[m
         abca_dataset = ioi_dataset.gen_flipped_prompts(("S2", "RAND"))[m
         assert torch.all(ioi_dataset.toks != 50256)  # no padding anywhere[m
         assert len(ioi_dataset.sem_tok_idx.keys()) != 0, "no semantic tokens found"[m
         for key in ioi_dataset.sem_tok_idx.keys():[m
             idx = ioi_dataset.sem_tok_idx[key][0][m
[31m-            assert torch.all(ioi_dataset.sem_tok_idx[key] == idx), f"{key} {ioi_dataset.sem_tok_idx[key]}"[m
[32m+[m[32m            assert torch.all([m
[32m+[m[32m                ioi_dataset.sem_tok_idx[key] == idx[m
[32m+[m[32m            ), f"{key} {ioi_dataset.sem_tok_idx[key]}"[m
             # check that semantic ablation = normal ablation[m
 [m
         model.reset_hooks()[m
[36m@@ -1510,12 +2336,16 @@[m [mthree_d = torch.zeros(size=(num_templates, 12, 12))[m
 [m
 for template_idx in tqdm(range(num_templates)):[m
     prompts = template_prompts[template_idx][m
[31m-    ioi_dataset = IOIDataset(prompt_type=template_type, N=N, symmetric=False, prompts=prompts)[m
[32m+[m[32m    ioi_dataset = IOIDataset([m
[32m+[m[32m        prompt_type=template_type, N=N, symmetric=False, prompts=prompts[m
[32m+[m[32m    )[m
     assert torch.all(ioi_dataset.toks != 50256)  # no padding anywhere[m
     assert len(ioi_dataset.sem_tok_idx.keys()) != 0, "no semantic tokens found"[m
     for key in ioi_dataset.sem_tok_idx.keys():[m
         idx = ioi_dataset.sem_tok_idx[key][0][m
[31m-        assert torch.all(ioi_dataset.sem_tok_idx[key] == idx), f"{key} {ioi_dataset.sem_tok_idx[key]}"[m
[32m+[m[32m        assert torch.all([m
[32m+[m[32m            ioi_dataset.sem_tok_idx[key] == idx[m
[32m+[m[32m        ), f"{key} {ioi_dataset.sem_tok_idx[key]}"[m
         # check that semantic ablation = normal ablation[m
 [m
     attn_vals, mlp_vals = writing_direction_heatmap([m
[36m@@ -1589,11 +2419,15 @@[m [mdef score_target(model, ioi_dataset, k=1, target_dataset=None, all=False):[m
     end_logits = logits[[m
         torch.arange(len(text_prompts)), ioi_dataset.word_idx["end"][:L], :[m
     ]  # batch * sequence length * vocab_size[m
[31m-    io_logits = end_logits[torch.arange(len(text_prompts)), target_dataset.io_tokenIDs[:L]][m
[32m+[m[32m    io_logits = end_logits[[m
[32m+[m[32m        torch.arange(len(text_prompts)), target_dataset.io_tokenIDs[:L][m
[32m+[m[32m    ][m
     assert len(list(end_logits.shape)) == 2, end_logits.shape[m
     top_10s_standard = torch.topk(end_logits, dim=1, k=k).values[:, -1][m
     good_enough = end_logits >= top_10s_standard.unsqueeze(-1)[m
[31m-    selected_logits = good_enough[torch.arange(len(text_prompts)), target_dataset.io_tokenIDs[:L]][m
[32m+[m[32m    selected_logits = good_enough[[m
[32m+[m[32m        torch.arange(len(text_prompts)), target_dataset.io_tokenIDs[:L][m
[32m+[m[32m    ][m
     # print(torch.argmax(end_logits, dim=-1))[m
     # is IO > S ???[m
     IO_logits = logits[[m
[36m@@ -1609,7 +2443,9 @@[m [mdef score_target(model, ioi_dataset, k=1, target_dataset=None, all=False):[m
     IO_greater_than_S = (IO_logits - S_logits) > 0[m
 [m
     # calculate percentage passing both tests[m
[31m-    answer = torch.sum((selected_logits & IO_greater_than_S).float()).detach().cpu() / len(text_prompts)[m
[32m+[m[32m    answer = torch.sum([m
[32m+[m[32m        (selected_logits & IO_greater_than_S).float()[m
[32m+[m[32m    ).detach().cpu() / len(text_prompts)[m
 [m
     selected = torch.sum(selected_logits) / len(text_prompts)[m
     greater = torch.sum(IO_greater_than_S) / len(text_prompts)[m
[36m@@ -1623,14 +2459,19 @@[m [mdef print_top_k(model, ioi_dataset, K=1, n=10):[m
     end_logits = logits[[m
         torch.arange(len(ioi_dataset.text_prompts)), ioi_dataset.word_idx["end"], :[m
     ]  # batch * sequence length * vocab_size[m
[31m-    probs = np.around(torch.nn.functional.log_softmax(end_logits, dim=-1).cpu().numpy(), 2)[m
[32m+[m[32m    probs = np.around([m
[32m+[m[32m        torch.nn.functional.log_softmax(end_logits, dim=-1).cpu().numpy(), 2[m
[32m+[m[32m    )[m
     topk = torch.topk(end_logits, dim=1, k=K).indices[m
     for x in range(n):[m
         print("-------------------")[m
         print(ioi_dataset.text_prompts[x])[m
         print([m
             " ".join([m
[31m-                [f"({i+1}):{model.tokenizer.decode(token)} : {probs[x][token]}" for i, token in enumerate(topk[x])][m
[32m+[m[32m                [[m
[32m+[m[32m                    f"({i+1}):{model.tokenizer.decode(token)} : {probs[x][token]}"[m
[32m+[m[32m                    for i, token in enumerate(topk[x])[m
[32m+[m[32m                ][m
             )[m
         )[m
 [m
[36m@@ -1674,7 +2515,9 @@[m [mdf = pd.DataFrame([m
         "Random (for separation)": np.random.random(len(ldiff)),[m
         "beg": [prompt[:10] for prompt in ioi_dataset.text_prompts],[m
         "sentence": [prompt for prompt in ioi_dataset.text_prompts],[m
[31m-        "#tokens before first name": [prompt.count("Then") for prompt in ioi_dataset.text_prompts],[m
[32m+[m[32m        "#tokens before first name": [[m
[32m+[m[32m            prompt.count("Then") for prompt in ioi_dataset.text_prompts[m
[32m+[m[32m        ],[m
         "template": ioi_dataset.templates_by_prompt,[m
         "misc": [[m
             (str(prompt.count("Then")) + str(ioi_dataset.templates_by_prompt[i]))[m
[36m@@ -1787,15 +2630,21 @@[m [mtarget_heads_to_keep, target_mlps_to_keep = get_heads_circuit([m
 [m
 K = 1[m
 model.reset_hooks()[m
[31m-old_ld, old_std = logit_diff_target(model, target_ioi_dataset, target_dataset=target_ioi_dataset, all=True, std=True)[m
[32m+[m[32mold_ld, old_std = logit_diff_target([m
[32m+[m[32m    model, target_ioi_dataset, target_dataset=target_ioi_dataset, all=True, std=True[m
[32m+[m[32m)[m
 model.reset_hooks()[m
[31m-old_score = score_target(model, target_ioi_dataset, target_dataset=target_ioi_dataset, k=K)[m
[32m+[m[32mold_score = score_target([m
[32m+[m[32m    model, target_ioi_dataset, target_dataset=target_ioi_dataset, k=K[m
[32m+[m[32m)[m
 model.reset_hooks()[m
 old_ld_source, old_std_source = logit_diff_target([m
     model, target_ioi_dataset, target_dataset=source_ioi_dataset, all=True, std=True[m
 )[m
 model.reset_hooks()[m
[31m-old_score_source = score_target(model, target_ioi_dataset, target_dataset=source_ioi_dataset, k=K)[m
[32m+[m[32mold_score_source = score_target([m
[32m+[m[32m    model, target_ioi_dataset, target_dataset=source_ioi_dataset, k=K[m
[32m+[m[32m)[m
 model.reset_hooks()[m
 model, _ = do_global_patching([m
     source_mlps_to_patch=source_mlps_to_keep,[m
[36m@@ -1814,13 +2663,19 @@[m [mmodel, _ = do_global_patching([m
 ldiff_target, std_ldiff_target = logit_diff_target([m
     model, target_ioi_dataset, target_dataset=target_ioi_dataset, std=True, all=True[m
 )[m
[31m-score_target_result = score_target(model, target_ioi_dataset, target_dataset=target_ioi_dataset, k=K)[m
[32m+[m[32mscore_target_result = score_target([m
[32m+[m[32m    model, target_ioi_dataset, target_dataset=target_ioi_dataset, k=K[m
[32m+[m[32m)[m
 ldiff_source, std_ldiff_source = logit_diff_target([m
     model, target_ioi_dataset, target_dataset=source_ioi_dataset, std=True, all=True[m
 )[m
[31m-score_source = score_target(model, target_ioi_dataset, target_dataset=source_ioi_dataset, k=K)[m
[32m+[m[32mscore_source = score_target([m
[32m+[m[32m    model, target_ioi_dataset, target_dataset=source_ioi_dataset, k=K[m
[32m+[m[32m)[m
 # %%[m
[31m-print(f"Original logit_diff on TARGET dataset (no patching yet!) = {old_ld.mean()} +/- {old_std}. Score {old_score}")[m
[32m+[m[32mprint([m
[32m+[m[32m    f"Original logit_diff on TARGET dataset (no patching yet!) = {old_ld.mean()} +/- {old_std}. Score {old_score}"[m
[32m+[m[32m)[m
 print([m
     f"Original logit_diff on SOURCE dataset (no patching yet!) = {old_ld_source.mean()} +/- {old_std_source}. Score {old_score_source}"[m
 )[m
[36m@@ -1836,10 +2691,15 @@[m [mdf = pd.DataFrame([m
         "Random (for interactivity)": np.random.random(len(ldiff_source)),[m
         "beg": [prompt["text"][:10] for prompt in ioi_dataset.ioi_prompts],[m
         "sentence": [prompt["text"] for prompt in ioi_dataset.ioi_prompts],[m
[31m-        "#tokens before first name": [prompt["text"].count("Then") for prompt in ioi_dataset.ioi_prompts],[m
[32m+[m[32m        "#tokens before first name": [[m
[32m+[m[32m            prompt["text"].count("Then") for prompt in ioi_dataset.ioi_prompts[m
[32m+[m[32m        ],[m
         "template": ioi_dataset.templates_by_prompt,[m
         "misc": [[m
[31m-            (str(prompt["text"].count("Then")) + str(ioi_dataset.templates_by_prompt[i]))[m
[32m+[m[32m            ([m
[32m+[m[32m                str(prompt["text"].count("Then"))[m
[32m+[m[32m                + str(ioi_dataset.templates_by_prompt[i])[m
[32m+[m[32m            )[m
             for (i, prompt) in enumerate(ioi_dataset.ioi_prompts)[m
         ],[m
     }[m
[36m@@ -1875,9 +2735,13 @@[m [mfor layer, head_idx in [(7, 9), (8, 6), (7, 3), (8, 10)]:[m
     # use abl.mean_cache[m
     cur_tensor_name = f"blocks.{layer}.attn.hook_v"[m
     s2_token_idxs = get_extracted_idx(["S2"], ioi_dataset)[m
[31m-    mean_cached_values = abl.mean_cache[cur_tensor_name][:, :, head_idx, :].cpu().detach()[m
[32m+[m[32m    mean_cached_values = ([m
[32m+[m[32m        abl.mean_cache[cur_tensor_name][:, :, head_idx, :].cpu().detach()[m
[32m+[m[32m    )[m
 [m
[31m-    def s2_v_ablation_hook(z, act, hook):  # batch, seq, head dim, because get_act_hook hides scary things from us[m
[32m+[m[32m    def s2_v_ablation_hook([m
[32m+[m[32m        z, act, hook[m
[32m+[m[32m    ):  # batch, seq, head dim, because get_act_hook hides scary things from us[m
         cur_layer = int(hook.name.split(".")[1])[m
         cur_head_idx = hook.ctx["idx"][m
 [m
[36m@@ -1886,7 +2750,9 @@[m [mfor layer, head_idx in [(7, 9), (8, 6), (7, 3), (8, 10)]:[m
         assert list(z.shape) == list(act.shape), (z.shape, act.shape)[m
 [m
         true_s2_values = z[:, s2_token_idxs, :].clone()[m
[31m-        z = mean_cached_values.cuda()  # hope that we don't see chaning values of mean_cached_values...[m
[32m+[m[32m        z = ([m
[32m+[m[32m            mean_cached_values.cuda()[m
[32m+[m[32m        )  # hope that we don't see chaning values of mean_cached_values...[m
         # z[:, s2_token_idxs, :] = true_s2_values[m
 [m
         return z[m
[36m@@ -1919,7 +2785,9 @@[m [minduction_scores_array = np.zeros((model.cfg.n_layers, model.cfg.n_heads))[m
 def calc_induction_score(attn_pattern, hook):[m
     # Pattern has shape [batch, index, query_pos, key_pos][m
     induction_stripe = attn_pattern.diagonal(1 - seq_len, dim1=-2, dim2=-1)[m
[31m-    induction_scores = einops.reduce(induction_stripe, "batch index pos -> index", "mean")[m
[32m+[m[32m    induction_scores = einops.reduce([m
[32m+[m[32m        induction_stripe, "batch index pos -> index", "mean"[m
[32m+[m[32m    )[m
     # Store the scores in a common array[m
     induction_scores_array[hook.layer()] = induction_scores.detach().cpu().numpy()[m
 [m
[36m@@ -1929,8 +2797,14 @@[m [mdef filter_attn_hooks(hook_name):[m
     return split_name[-1] == "hook_attn"[m
 [m
 [m
[31m-induction_logits = model.run_with_hooks(rand_tokens_repeat, fwd_hooks=[(filter_attn_hooks, calc_induction_score)])[m
[31m-px.imshow(induction_scores_array, labels={"y": "Layer", "x": "Head"}, color_continuous_scale="Blues")[m
[32m+[m[32minduction_logits = model.run_with_hooks([m
[32m+[m[32m    rand_tokens_repeat, fwd_hooks=[(filter_attn_hooks, calc_induction_score)][m
[32m+[m[32m)[m
[32m+[m[32mpx.imshow([m
[32m+[m[32m    induction_scores_array,[m
[32m+[m[32m    labels={"y": "Layer", "x": "Head"},[m
[32m+[m[32m    color_continuous_scale="Blues",[m
[32m+[m[32m)[m
 [m
 [m
 # %%[m
[36m@@ -2000,7 +2874,8 @@[m [mfor k in range(len(induct_head) + 1):[m
     all_means.append(results.mean())[m
 [m
 fig = px.bar([m
[31m-    all_means, title="Loss on repeated random tokens sequences (average on 10 random set of KO heads) 5.5 excluded"[m
[32m+[m[32m    all_means,[m
[32m+[m[32m    title="Loss on repeated random tokens sequences (average on 10 random set of KO heads) 5.5 excluded",[m
 )[m
 [m
 [m
[36m@@ -2023,7 +2898,11 @@[m [mfor layer in range(12):[m
                 np.array([0]),[m
                 np.array([m
                     compute_next_tok_dot_prod([m
[31m-                        model, rand_tokens_repeat[IDX : IDX + 1], layer, head, seq_tokenized=True[m
[32m+[m[32m                        model,[m
[32m+[m[32m                        rand_tokens_repeat[IDX : IDX + 1],[m
[32m+[m[32m                        layer,[m
[32m+[m[32m                        head,[m
[32m+[m[32m                        seq_tokenized=True,[m
                     )[IDX][m
                 ),[m
             ][m
[36m@@ -2037,12 +2916,20 @@[m [mPOS = 130[m
 K = 1[m
 for x in np.random.randint(0, seq_len, K):[m
     print(f"Position {x}")[m
[31m-    fig = px.imshow(prod[:, :, x], labels={"y": "Layer", "x": "Head"}, color_continuous_scale="Blues")[m
[32m+[m[32m    fig = px.imshow([m
[32m+[m[32m        prod[:, :, x],[m
[32m+[m[32m        labels={"y": "Layer", "x": "Head"},[m
[32m+[m[32m        color_continuous_scale="Blues",[m
[32m+[m[32m    )[m
     fig.show()[m
 [m
 for x in np.random.randint(seq_len, 2 * seq_len, K):[m
     print(f"Position {x}")[m
[31m-    fig = px.imshow(prod[:, :, x], labels={"y": "Layer", "x": "Head"}, color_continuous_scale="Blues")[m
[32m+[m[32m    fig = px.imshow([m
[32m+[m[32m        prod[:, :, x],[m
[32m+[m[32m        labels={"y": "Layer", "x": "Head"},[m
[32m+[m[32m        color_continuous_scale="Blues",[m
[32m+[m[32m    )[m
     fig.show()[m
 [m
 # %%[m
[36m@@ -2084,7 +2971,9 @@[m [mfor n in CIRCUIT.keys():[m
 [m
 # %%[m
 [m
[31m-vals, idx = sort_mtx(np.abs(prod[:, :, seq_len:]).mean(axis=-1), return_idx=True, sort_by_abs=True)[m
[32m+[m[32mvals, idx = sort_mtx([m
[32m+[m[32m    np.abs(prod[:, :, seq_len:]).mean(axis=-1), return_idx=True, sort_by_abs=True[m
[32m+[m[32m)[m
 [m
 [m
 colors = [((l, h) in circuit_heads) for (l, h) in idx][m
[36m@@ -2096,14 +2985,172 @@[m [mfig = px.bar([m
     title="Average absolute dot prod with next token on repeated sequence (after zero-KO of 6.9 and 5.5)",[m
 )[m
 [m
[31m-fig.update_layout(xaxis={"categoryorder": "array", "categoryarray": [f"{x[0]}.{x[1]}" for x in idx]})[m
[32m+[m[32mfig.update_layout([m
[32m+[m[32m    xaxis={"categoryorder": "array", "categoryarray": [f"{x[0]}.{x[1]}" for x in idx]}[m
[32m+[m[32m)[m
 fig.show()[m
 [m
 # %%[m
[31m-heads = [(10, 10), (9, 6), (10, 6), (10, 7), (11, 10), (10, 1), (9, 9), (10, 2), (7, 2), (9, 1), (9, 0), (11, 7)][m
[32m+[m[32mheads = [[m
[32m+[m[32m    (10, 10),[m
[32m+[m[32m    (9, 6),[m
[32m+[m[32m    (10, 6),[m
[32m+[m[32m    (10, 7),[m
[32m+[m[32m    (11, 10),[m
[32m+[m[32m    (10, 1),[m
[32m+[m[32m    (9, 9),[m
[32m+[m[32m    (10, 2),[m
[32m+[m[32m    (7, 2),[m
[32m+[m[32m    (9, 1),[m
[32m+[m[32m    (9, 0),[m
[32m+[m[32m    (11, 7),[m
[32m+[m[32m][m
[32m+[m
[32m+[m[32mall_prods = {[m
[32m+[m[32m    f"{layer}.{head}": prod[layer, head, :] for (layer, head) in important_heads[m
[32m+[m[32m}[m
 [m
[31m-all_prods = {f"{layer}.{head}": prod[layer, head, :] for (layer, head) in important_heads}[m
[32m+[m[32mpx.line([m
[32m+[m[32m    all_prods,[m
[32m+[m[32m    labels={"index": "Position in sequence of random tokens", "value": "Dot product"},[m
[32m+[m[32m)[m
[32m+[m[32m#%% [markdown] Arthur messing around[m
[32m+[m[32mhook_names = [[m
[32m+[m[32m    "hook_embed",[m
[32m+[m[32m    "hook_pos_embed",[m
[32m+[m[32m    "blocks.0.hook_resid_pre",[m
[32m+[m[32m    "blocks.0.hook_mlp_out",[m
[32m+[m[32m    "blocks.0.hook_attn_out",[m
[32m+[m[32m    "blocks.0.hook_resid_post",[m
[32m+[m[32m][m
[32m+[m[32mmodel.reset_hooks()[m
[32m+[m[32mcache = {}[m
[32m+[m[32mmodel.cache_some(cache, lambda x: x in hook_names)[m
[32m+[m[32mlogits = model(ioi_dataset.toks.long())[m
[32m+[m
[32m+[m
[32m+[m[32mdef act_patch(z, hook):[m
[32m+[m[32m    # z -= cache["hook_embed"][m
[32m+[m[32m    # z -= cache["hook_pos_embed"][m
[32m+[m[32m    z -= cache[hook_names[-4]][m
[32m+[m[32m    return z[m
 [m
[31m-px.line(all_prods, labels={"index": "Position in sequence of random tokens", "value": "Dot product"})[m
[32m+[m
[32m+[m[32mhook = get_act_hook([m
[32m+[m[32m    act_patch,[m
[32m+[m[32m    alt_act=None,  # target_cache[hook_name],[m
[32m+[m[32m    idx=None,[m
[32m+[m[32m    dim=None,  # 2 if head_idx is not None else None,[m
[32m+[m[32m    name=hook_names[-1],[m
[32m+[m[32m)[m
[32m+[m[32mmodel.reset_hooks()[m
[32m+[m[32mmodel.add_hook(hook_names[-1], hook)[m
[32m+[m[32mcur_logit_diff = logit_diff(model, ioi_dataset)[m
[32m+[m[32mprint(cur_logit_diff)[m
[32m+[m[32m#%%[m
[32m+[m[32mmodel.reset_hooks()[m
[32m+[m[32mhooks = do_circuit_extraction([m
[32m+[m[32m    model=model,[m
[32m+[m[32m    heads_to_keep={},[m
[32m+[m[32m    mlps_to_keep={},[m
[32m+[m[32m    ioi_dataset=ioi_dataset,[m
[32m+[m[32m    mean_dataset=all_diff_dataset,[m
[32m+[m[32m    excluded=[(layer, head_idx) for layer in range(12) for head_idx in range(12)],[m
[32m+[m[32m    return_hooks=True,[m
[32m+[m[32m)[m
[32m+[m
[32m+[m[32mmodel.reset_hooks()[m
[32m+[m[32mdefault_logit_diff = logit_diff(model, ioi_dataset)[m
[32m+[m
[32m+[m[32mmodel.reset_hooks()[m
[32m+[m[32mfor i in range(1, 12):[m
[32m+[m[32m    model.add_hook(*hooks[i])[m
[32m+[m[32mcur_logit_diff = logit_diff(model, ioi_dataset)[m
[32m+[m[32mprint(f"Layer {i} logit diff: {cur_logit_diff} {default_logit_diff}")[m
 [m
 # %%[m
[32m+[m[32m#%% [markdown] weird patching[m
[32m+[m
[32m+[m[32mfrom ioi_dataset import BABA_LONG_TEMPLATES[m
[32m+[m
[32m+[m[32mearly_dataset = IOIDataset(prompt_type=BABA_TEMPLATES, N=100)[m
[32m+[m[32mlate_dataset = IOIDataset.construct_from_ioi_prompts_metadata([m
[32m+[m[32m    templates=BABA_LONG_TEMPLATES,[m
[32m+[m[32m    ioi_prompts_data=deepcopy(early_dataset.ioi_prompts),[m
[32m+[m[32m    N=100,[m
[32m+[m[32m)[m
[32m+[m[32m#%%[m
[32m+[m[32mdef patch_positions([m
[32m+[m[32m    z, source_act, hook, positions=["END"][m
[32m+[m[32m):  # we patch at the "to" token[m
[32m+[m[32m    for pos in positions:[m
[32m+[m[32m        z[torch.arange(early_dataset.N), early_dataset.word_idx[pos]] = source_act[[m
[32m+[m[32m            torch.arange(late_dataset.N), late_dataset.word_idx[pos][m
[32m+[m[32m        ][m
[32m+[m[32m    return z[m
[32m+[m
[32m+[m
[32m+[m[32mpatch_last_tokens = partial(patch_positions, positions=["end"])[m
[32m+[m[32mpatch_s2 = partial(patch_positions, positions=["S2"])[m
[32m+[m
[32m+[m
[32m+[m[32mconfig = PatchingConfig([m
[32m+[m[32m    source_dataset=late_dataset.text_prompts,[m
[32m+[m[32m    target_dataset=early_dataset.text_prompts,[m
[32m+[m[32m    target_module="attn_head",[m
[32m+[m[32m    head_circuit="result",[m
[32m+[m[32m    cache_act=True,[m
[32m+[m[32m    verbose=False,[m
[32m+[m[32m    patch_fn=patch_s2,[m
[32m+[m[32m    layers=(0, 8),[m
[32m+[m[32m)  # we stop at layer "LAYER" because it's useless to patch after layer 9 if what we measure is attention of a head at layer 9.[m
[32m+[m[32mmetric = ExperimentMetric([m
[32m+[m[32m    logit_diff,[m
[32m+[m[32m    # partial(attention_probs, scale=False),[m
[32m+[m[32m    config.target_dataset,[m
[32m+[m[32m    relative_metric=False,[m
[32m+[m[32m    scalar_metric=False,[m
[32m+[m[32m)[m
[32m+[m[32mpatching = EasyPatching(model, config, metric)[m
[32m+[m[32m#%%[m
[32m+[m[32mmodel.reset_hooks()[m
[32m+[m[32m# new_heads_to_keep = get_heads_circuit(ioi_dataset, circuit=circuit)[m
[32m+[m[32m# model, _ = do_circuit_extraction([m
[32m+[m[32m#     model=model,[m
[32m+[m[32m#     heads_to_keep=new_heads_to_keep,[m
[32m+[m[32m#     mlps_to_remove={},[m
[32m+[m[32m#     ioi_dataset=ioi_dataset,[m
[32m+[m[32m#     mean_dataset=abca_dataset,[m
[32m+[m[32m# )[m
[32m+[m
[32m+[m[32mcircuit = deepcopy(CIRCUIT)[m
[32m+[m[32mfrom ioi_circuit_extraction import RELEVANT_TOKENS[m
[32m+[m
[32m+[m[32mfor idx, head_set in enumerate([m
[32m+[m[32m    [[m
[32m+[m[32m        [],[m
[32m+[m[32m        ["duplicate token"],[m
[32m+[m[32m        ["induction"],[m
[32m+[m[32m        ["s2 inhibition"],[m
[32m+[m[32m        ["induction", "duplicate token"],[m
[32m+[m[32m        ["s2 inhibition"] + ["induction"] + ["duplicate token"],[m
[32m+[m[32m    ][m
[32m+[m[32m):[m
[32m+[m[32m    model.reset_hooks()[m
[32m+[m[32m    heads = [][m
[32m+[m[32m    for circuit_class in head_set:[m
[32m+[m[32m        heads += circuit[circuit_class][m
[32m+[m[32m    for layer, head_idx in heads:[m
[32m+[m[32m        hook = patching.get_hook([m
[32m+[m[32m            layer,[m
[32m+[m[32m            head_idx,[m
[32m+[m[32m            patch_fn=partial([m
[32m+[m[32m                patch_positions, positions=RELEVANT_TOKENS[(layer, head_idx)][m
[32m+[m[32m            ),[m
[32m+[m[32m        )[m
[32m+[m[32m        model.add_hook(*hook)[m
[32m+[m
[32m+[m[32m    # print(f"{head_set}, IO S S2, {att_probs}")  # print("IO S S2")[m
[32m+[m[32m    cur_logit_diff = logit_diff(model, ioi_dataset)[m
[32m+[m[32m    # cur_io_probs = probs(model, ioi_dataset)[m
[32m+[m[32m    print(f"{idx} {cur_logit_diff} ")  # {cur_io_probs}")[m
[1mdiff --git a/ioi_utils.py b/ioi_utils.py[m
[1mindex 40dda5f..d7ed65d 100644[m
[1m--- a/ioi_utils.py[m
[1m+++ b/ioi_utils.py[m
[36m@@ -177,7 +177,8 @@[m [mdef show_attention_patterns([m
             model.cache_some([m
                 cache=cache, names=lambda x: x in good_names[m
             )  # shape: batch head_no seq_len seq_len[m
[31m-            logits = model(ioi_dataset.text_prompts)[m
[32m+[m[32m            logits = model(ioi_dataset.toks.long())[m
[32m+[m[32m            print(cache.keys(), good_names)[m
         else:[m
             cache = precomputed_cache[m
         attn_results = torch.zeros([m
[36m@@ -285,6 +286,7 @@[m [mdef scatter_attention_and_contribution([m
     for each input sequence with the attention paid to IO and S[m
     and the amount that is written in the IO and S directions[m
     """[m
[32m+[m[32m    warnings.warn("See new_ioi_notebook scatter (with direct effect)")[m
     n_heads = model.cfg.n_heads[m
     n_layers = model.cfg.n_layers[m
     model_unembed = model.unembed.W_U.detach().cpu()[m
[36m@@ -380,7 +382,7 @@[m [mdef logit_diff([m
     ioi_dataset,[m
     all=False,[m
     std=False,[m
[31m-    both=True,[m
[32m+[m[32m    both=False,[m
 ):  # changed by Arthur to take dataset object, :pray: no big backwards compatibility issues[m
     """[m
     Difference between the IO and the S logits at the "to" token[m
[36m@@ -453,8 +455,9 @@[m [mdef posses(model, ioi_dataset, all=False, std=False):[m
     """[m
     text_prompts = ioi_dataset.text_prompts[m
     logits = model(text_prompts).detach().cpu()  # batch * sequence length * vocab_size[m
[32m+[m[32m    warnings.warn("+1ing")[m
     end_logits = logits[[m
[31m-        torch.arange(len(text_prompts)), ioi_dataset.word_idx["end"], :[m
[32m+[m[32m        torch.arange(len(text_prompts)), ioi_dataset.word_idx["end"] + 1, :[m
     ]  # batch * vocab_size[m
 [m
     positions = torch.argsort(end_logits, dim=1)[m
[36m@@ -463,16 +466,19 @@[m [mdef posses(model, ioi_dataset, all=False, std=False):[m
     return handle_all_and_std(io_positions, all, std)[m
 [m
 [m
[31m-def probs(model, ioi_dataset, all=False, std=False, type="io"):[m
[32m+[m[32mdef probs(model, ioi_dataset, all=False, std=False, type="io", verbose=False):[m
     """[m
     IO probs[m
     """[m
 [m
[31m-    text_prompts = ioi_dataset.text_prompts[m
[31m-    logits = model(text_prompts).detach().cpu()  # batch * sequence length * vocab_size[m
[32m+[m[32m    logits = model([m
[32m+[m[32m        ioi_dataset.toks.long()[m
[32m+[m[32m    ).detach()  # batch * sequence length * vocab_size[m
[32m+[m[32m    warnings.warn("Not +1ing")[m
     end_logits = logits[[m
[31m-        torch.arange(len(text_prompts)), ioi_dataset.word_idx["end"], :[m
[32m+[m[32m        torch.arange(len(ioi_dataset)), ioi_dataset.word_idx["end"], :[m
     ]  # batch * vocab_size[m
[32m+[m
     end_probs = torch.softmax(end_logits, dim=1)[m
 [m
     if type == "io":[m
[36m@@ -481,11 +487,24 @@[m [mdef probs(model, ioi_dataset, all=False, std=False, type="io"):[m
         token_ids = ioi_dataset.s_tokenIDs[m
     else:[m
         raise ValueError("type must be io or s")[m
[31m-    io_probs = end_probs[torch.arange(ioi_dataset.N), token_ids][m
 [m
[32m+[m[32m    assert len(end_probs.shape) == 2[m
[32m+[m[32m    io_probs = end_probs[torch.arange(ioi_dataset.N), token_ids][m
[32m+[m[32m    if verbose:[m
[32m+[m[32m        print(io_probs)[m
     return handle_all_and_std(io_probs, all, std)[m
 [m
 [m
[32m+[m[32mdef get_top_tokens_and_probs(model, text_prompt):[m
[32m+[m[32m    logits, tokens = model([m
[32m+[m[32m        text_prompt, prepend_bos=False, return_type="logits_and_tokens"[m
[32m+[m[32m    )[m
[32m+[m[32m    logits = logits.squeeze(0)[m
[32m+[m[32m    end_probs = torch.softmax(logits, dim=1)[m
[32m+[m[32m    # topk = torch.topk(end_probs[])[m
[32m+[m[32m    return end_probs, tokens[m
[32m+[m
[32m+[m
 def all_subsets(L: List) -> List[List]:[m
     """[m
     Returns all subsets of L[m
[1mdiff --git a/myfile.zip b/myfile.zip[m
[1mnew file mode 100644[m
[1mindex 0000000..e2fac92[m
Binary files /dev/null and b/myfile.zip differ
[1mdiff --git a/new_ioi_notebook.py b/new_ioi_notebook.py[m
[1mindex ede0021..63517db 100644[m
[1m--- a/new_ioi_notebook.py[m
[1m+++ b/new_ioi_notebook.py[m
[36m@@ -134,8 +134,9 @@[m [mdef e(mess=""):[m
 [m
 [m
 #%% [markdown] The model, and loads and loads of datasets[m
[31m-model = EasyTransformer("gpt2", use_attn_result=True).cuda()[m
[31m-N = 500[m
[32m+[m[32mmodel = EasyTransformer.from_pretrained("gpt2").cuda()[m
[32m+[m[32mmodel.set_use_attn_result(True)[m
[32m+[m[32mN = 100[m
 ioi_dataset = IOIDataset(prompt_type="mixed", N=N, tokenizer=model.tokenizer)[m
 abca_dataset = ioi_dataset.gen_flipped_prompts([m
     ("S2", "RAND")[m
[36m@@ -257,7 +258,7 @@[m [mdef direct_patch_and_freeze([m
                 hook_name = hook_template.format(layer)[m
 [m
                 if hook_name in receiver_hook_names:[m
[31m-                    continue[m
[32m+[m[32m                    continue  # TODO maybe this should be break. No I don't think so[m
 [m
                 hook = get_act_hook([m
                     patch_all,[m
[36m@@ -288,7 +289,9 @@[m [mdef direct_patch_and_freeze([m
 [m
     # measure the receiver heads' values[m
     receiver_cache = {}[m
[31m-    model.cache_some(receiver_cache, lambda x: x in receiver_hook_names)[m
[32m+[m[32m    model.cache_some([m
[32m+[m[32m        receiver_cache, lambda x: x in receiver_hook_names[m
[32m+[m[32m    )  # TODO check that this doesn't do the annoying thing of measuring the overwritten stuff[m
     receiver_logits = model(target_dataset.text_prompts)[m
 [m
     # patch these values in[m
[36m@@ -340,6 +343,7 @@[m [mmodel.reset_hooks()[m
 default_logit_diff = logit_diff(model, ioi_dataset)[m
 [m
 # for pos in ["S+1", "S", "IO", "S2", "end"]:[m
[32m+[m[32m# WHYYY BROKENS????[m
 for pos in ["end"]:[m
     print(pos)[m
     results = torch.zeros(size=(12, 12))[m
[36m@@ -365,6 +369,7 @@[m [mfor pos in ["end"]:[m
                 positions=[pos],[m
                 verbose=False,[m
                 return_hooks=False,[m
[32m+[m[32m                freeze_mlps=False,[m
             )[m
 [m
             cur_logit_diff = logit_diff(model, ioi_dataset)[m
[36m@@ -405,8 +410,6 @@[m [mfor pos in ["end"]:[m
                 fig.write_image(fname + ".png")[m
                 fig.write_image(fname + ".svg")[m
                 fig.show()[m
[31m-[m
[31m-[m
 #%% [markdown] MLP indirect effect[m
 [m
 mlp_hooks = do_circuit_extraction([m
[36m@@ -726,7 +729,7 @@[m [mfor add_hooks in [False, True]:[m
     cache = {}[m
     model.cache_some(cache, lambda name: name == attention_hook_name)[m
 [m
[31m-    io_logits, s_logits = logit_diff(model, ioi_dataset, all=True)[m
[32m+[m[32m    io_logits, s_logits = logit_diff(model, ioi_dataset, all=True, both=True)[m
     io_logits = io_logits.detach().cpu()[m
     s_logits = s_logits.detach().cpu()[m
 [m
[36m@@ -769,7 +772,7 @@[m [mdf = pd.concat([m
         pd.DataFrame([m
             {[m
                 "attention": all_s_attentions[0],[m
[31m-                "change": all_s_logits[1] - all_s_logits[0],[m
[32m+[m[32m                "change": -(all_s_logits[1] - all_s_logits[0]),[m
                 "token": "S",[m
                 "text": ioi_dataset.text_prompts,[m
                 # "change": (all_io_logits[0] - all_s_logits[0])[m
[36m@@ -779,7 +782,7 @@[m [mdf = pd.concat([m
         pd.DataFrame([m
             {[m
                 "attention": all_io_attentions[0],[m
[31m-                "change": all_io_logits[1] - all_io_logits[0],[m
[32m+[m[32m                "change": -(all_io_logits[1] - all_io_logits[0]),[m
                 "token": "IO",[m
                 "text": ioi_dataset.text_prompts,[m
             }[m
[36m@@ -795,18 +798,23 @@[m [mfig = px.scatter([m
     color="token",[m
     hover_data=["text"],[m
     color_discrete_sequence=["rgb(114,255,100)", "rgb(201,165,247)"],[m
[31m-    title=f"How {layer}.{head_idx} affects logit difference (change after patch-and-freeze)",[m
[32m+[m[32m    title=f"How {layer}.{head_idx} affects logits (change after patch-and-freeze)",[m
 )[m
 [m
 # update y axis label[m
[31m-fig.update_yaxes(title_text="Change in logit difference")[m
[32m+[m[32my_label = "Change in logits"[m
[32m+[m[32mfig.update_yaxes(title_text=y_label)[m
 fig.update_xaxes(title_text="Attention on token")[m
 [m
[31m-fig.write_image(f"svgs/attention_scatter_{ctime()}.svg")[m
[31m-fig.write_image(f"svgs/attention_scatter_{ctime()}.png")[m
[31m-[m
[32m+[m[32mfig.write_image(f"svgs/attention_scatter_{y_label}_{ctime()}.svg")[m
[32m+[m[32mfig.write_image(f"svgs/attention_scatter_{y_label}_{ctime()}.png")[m
 fig.show()[m
 [m
 #%%[m
[31m-d2 = {"col1": [12341, 1242], "col2": [12343, 1234], "col3": 2}[m
[31m-df2 = pd.DataFrame(data=d2)[m
[32m+[m
[32m+[m[32m# select where token == "IO" in df[m
[32m+[m[32mxs = df[df["token"] == "IO"]["attention"][m
[32m+[m[32mys = df[df["token"] == "IO"]["change"][m
[32m+[m
[32m+[m[32m# correlation coefficient[m
[32m+[m[32mnp.corrcoef(xs, ys)[m
[1mdiff --git a/real_notebook.py b/real_notebook.py[m
[1mnew file mode 100644[m
[1mindex 0000000..5468303[m
[1m--- /dev/null[m
[1m+++ b/real_notebook.py[m
[36m@@ -0,0 +1,183 @@[m
[32m+[m[32m#%% [markdown][m
[32m+[m[32m## Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small[m
[32m+[m[32m# <h1><b>Intro</b></h1>[m
[32m+[m
[32m+[m[32m# This notebook implements all experiments in our paper (which is available on arXiv).[m
[32m+[m
[32m+[m[32m# For background on the task, see the paper.[m
[32m+[m
[32m+[m[32m# Refer to the demo of the <a href="https://github.com/neelnanda-io/Easy-Transformer">Easy-Transformer</a> library here: <a href="https://github.com/neelnanda-io/Easy-Transformer/blob/main/EasyTransformer_Demo.ipynb">demo with ablation and patching</a>.[m
[32m+[m[32m#[m
[32m+[m[32m# Reminder of the circuit:[m
[32m+[m[32m# <img src="https://i.imgur.com/arokEMj.png">[m
[32m+[m
[32m+[m[32m#%% [markdown] Setup # TODO cut extras[m
[32m+[m[32mfrom copy import deepcopy[m
[32m+[m[32mimport os[m
[32m+[m[32mimport torch[m
[32m+[m
[32m+[m[32mif os.environ["USER"] in ["exx", "arthur"]:  # so Arthur can safely use octobox[m
[32m+[m[32m    os.environ["CUDA_VISIBLE_DEVICES"] = "2"[m
[32m+[m[32massert torch.cuda.device_count() == 1[m
[32m+[m[32mfrom easy_transformer.EasyTransformer import LayerNormPre[m
[32m+[m[32mfrom tqdm import tqdm[m
[32m+[m[32mimport pandas as pd[m
[32m+[m[32mimport torch[m
[32m+[m[32mimport torch as t[m
[32m+[m[32mfrom easy_transformer.utils import ([m
[32m+[m[32m    gelu_new,[m
[32m+[m[32m    to_numpy,[m
[32m+[m[32m    get_corner,[m
[32m+[m[32m    print_gpu_mem,[m
[32m+[m[32m)  # helper functions[m
[32m+[m[32mfrom easy_transformer.hook_points import HookedRootModule, HookPoint[m
[32m+[m[32mfrom easy_transformer.EasyTransformer import ([m
[32m+[m[32m    EasyTransformer,[m
[32m+[m[32m    TransformerBlock,[m
[32m+[m[32m    MLP,[m
[32m+[m[32m    Attention,[m
[32m+[m[32m    LayerNormPre,[m
[32m+[m[32m    PosEmbed,[m
[32m+[m[32m    Unembed,[m
[32m+[m[32m    Embed,[m
[32m+[m[32m)[m
[32m+[m[32mfrom easy_transformer.experiments import ([m
[32m+[m[32m    ExperimentMetric,[m
[32m+[m[32m    AblationConfig,[m
[32m+[m[32m    EasyAblation,[m
[32m+[m[32m    EasyPatching,[m
[32m+[m[32m    PatchingConfig,[m
[32m+[m[32m    get_act_hook,[m
[32m+[m[32m)[m
[32m+[m[32mfrom time import ctime[m
[32m+[m[32mfrom functools import partial[m
[32m+[m[32mfrom typing import Any, Callable, Dict, List, Set, Tuple, Union, Optional, Iterable[m
[32m+[m[32mimport itertools[m
[32m+[m[32mimport numpy as np[m
[32m+[m[32mfrom tqdm import tqdm[m
[32m+[m[32mimport pandas as pd[m
[32m+[m[32mimport plotly.express as px[m
[32m+[m[32mimport plotly.io as pio[m
[32m+[m[32mfrom plotly.subplots import make_subplots[m
[32m+[m[32mimport plotly.graph_objects as go[m
[32m+[m[32mimport warnings[m
[32m+[m[32mimport plotly[m
[32m+[m[32mfrom sklearn.linear_model import LinearRegression[m
[32m+[m[32mfrom transformers import AutoModelForCausalLM, AutoTokenizer[m
[32m+[m[32mimport random[m
[32m+[m[32mimport spacy[m
[32m+[m[32mimport re[m
[32m+[m[32mfrom einops import rearrange[m
[32m+[m[32mimport einops[m
[32m+[m[32mfrom pprint import pprint[m
[32m+[m[32mimport gc[m
[32m+[m[32mfrom datasets import load_dataset[m
[32m+[m[32mfrom IPython import get_ipython[m
[32m+[m[32mimport matplotlib.pyplot as plt[m
[32m+[m[32mimport random as rd[m
[32m+[m[32mfrom copy import deepcopy[m
[32m+[m[32mfrom ioi_dataset import ([m
[32m+[m[32m    IOIDataset,[m
[32m+[m[32m    NOUNS_DICT,[m
[32m+[m[32m    NAMES,[m
[32m+[m[32m    gen_flipped_prompts,[m
[32m+[m[32m    gen_prompt_uniform,[m
[32m+[m[32m    BABA_TEMPLATES,[m
[32m+[m[32m    ABBA_TEMPLATES,[m
[32m+[m[32m)[m
[32m+[m[32mfrom ioi_utils import ([m
[32m+[m[32m    max_2d,[m
[32m+[m[32m    CLASS_COLORS,[m
[32m+[m[32m    all_subsets,[m
[32m+[m[32m    clear_gpu_mem,[m
[32m+[m[32m    show_tokens,[m
[32m+[m[32m    show_pp,[m
[32m+[m[32m    show_attention_patterns,[m
[32m+[m[32m    safe_del,[m
[32m+[m[32m)[m
[32m+[m[32mfrom random import randint as ri[m
[32m+[m[32mfrom ioi_circuit_extraction import ([m
[32m+[m[32m    do_circuit_extraction,[m
[32m+[m[32m    gen_prompt_uniform,[m
[32m+[m[32m    get_act_hook,[m
[32m+[m[32m    get_circuit_replacement_hook,[m
[32m+[m[32m    get_extracted_idx,[m
[32m+[m[32m    get_heads_circuit,[m
[32m+[m[32m    join_lists,[m
[32m+[m[32m    list_diff,[m
[32m+[m[32m    process_heads_and_mlps,[m
[32m+[m[32m    turn_keep_into_rmv,[m
[32m+[m[32m    CIRCUIT,[m
[32m+[m[32m    ARTHUR_CIRCUIT,[m
[32m+[m[32m)[m
[32m+[m[32mfrom ioi_utils import logit_diff, probs[m
[32m+[m[32mfrom ioi_utils import get_top_tokens_and_probs as g[m
[32m+[m
[32m+[m[32mipython = get_ipython()[m
[32m+[m[32mif ipython is not None:[m
[32m+[m[32m    ipython.magic("load_ext autoreload")[m
[32m+[m[32m    ipython.magic("autoreload 2")[m
[32m+[m[32m#%% [markdown] Model and Dataset (use larger N or fewer templates for no warnings about in-template ablation)[m
[32m+[m[32mmodel = EasyTransformer.from_pretrained("gpt2").cuda()[m
[32m+[m[32mmodel.set_use_attn_result(True)[m
[32m+[m[32mN = 100[m
[32m+[m[32mioi_dataset = IOIDataset([m
[32m+[m[32m    prompt_type="mixed",[m
[32m+[m[32m    N=N,[m
[32m+[m[32m    tokenizer=model.tokenizer,[m
[32m+[m[32m    prepend_bos=False,[m
[32m+[m[32m)[m
[32m+[m[32mmodel_logit_diff = logit_diff(model, ioi_dataset)[m
[32m+[m
[32m+[m[32mprint([m
[32m+[m[32m    f"The model gets average logit difference {model_logit_diff.item()} over {N} examples"[m
[32m+[m[32m)[m
[32m+[m[32m#%% [markdown] The Circuit[m
[32m+[m
[32m+[m[32mcircuit = deepcopy(CIRCUIT)[m
[32m+[m
[32m+[m[32m# we make the ABC dataset in order to knockout other model components[m
[32m+[m[32mabc_dataset = ([m
[32m+[m[32m    ioi_dataset.gen_flipped_prompts(("IO", "RAND"))[m
[32m+[m[32m    .gen_flipped_prompts(("S", "RAND"))[m
[32m+[m[32m    .gen_flipped_prompts(("S1", "RAND"), manual_word_idx=ioi_dataset.word_idx)[m
[32m+[m[32m)[m
[32m+[m
[32m+[m[32m# we then add hooks to the model to knockout all the heads except the circuit[m
[32m+[m[32mmodel.reset_hooks()[m
[32m+[m[32mmodel, _ = do_circuit_extraction([m
[32m+[m[32m    model=model,[m
[32m+[m[32m    heads_to_keep=get_heads_circuit(ioi_dataset=ioi_dataset, circuit=circuit),[m
[32m+[m[32m    mlps_to_remove={},[m
[32m+[m[32m    ioi_dataset=ioi_dataset,[m
[32m+[m[32m    mean_dataset=abc_dataset,[m
[32m+[m[32m)[m
[32m+[m
[32m+[m[32mcircuit_logit_diff = logit_diff(model, ioi_dataset)[m
[32m+[m[32mprint([m
[32m+[m[32m    f"The circuit gets average logit difference {circuit_logit_diff.item()} over {N} examples"[m
[32m+[m[32m)[m
[32m+[m[32m#%% Delete this: hacky stuff[m
[32m+[m[32m#%% [markdown] IOI dataset with prepend_bos...[m
[32m+[m
[32m+[m[32mioi_dataset_2 = IOIDataset([m
[32m+[m[32m    prompt_type="mixed",[m
[32m+[m[32m    N=N,[m
[32m+[m[32m    tokenizer=model.tokenizer,[m
[32m+[m[32m    prepend_bos=True,[m
[32m+[m[32m)[m
[32m+[m
[32m+[m[32m#%%[m
[32m+[m[32mfor new_N in range(1, 3):[m
[32m+[m[32m    # d = IOIDataset(prompt_type="mixed", N=new_N, tokenizer=model.tokenizer, prepend_bos=True, has_start_padding_and_start_is_end=True)[m
[32m+[m[32m    d = ioi_dataset[m
[32m+[m[32m    print(f"new_N={new_N}")[m
[32m+[m[32m    for i in range(new_N):[m
[32m+[m[32m        for key in d.word_idx.keys():[m
[32m+[m[32m            print([m
[32m+[m[32m                f"key={key} {int(d.word_idx[key][i])} {d.tokenizer.decode(d.toks[i][d.word_idx[key][i]])}"[m
[32m+[m[32m            )[m
[32m+[m[32mprint("Seem fine?")[m
[32m+[m[32m# %%[m
[32m+[m
[32m+[m[32mmy_ioi_dataset = IOIDataset()[m
[1mdiff --git a/setup.py b/setup.py[m
[1mindex efb0bc2..f065b80 100644[m
[1m--- a/setup.py[m
[1m+++ b/setup.py[m
[36m@@ -8,12 +8,15 @@[m [msetup([m
     description="An implementation of transformers tailored for mechanistic interpretability.",[m
     long_description=open("README.md").read(),[m
     install_requires=[[m
[31m-        'einops',[m
[31m-        'numpy',[m
[31m-        'torch',[m
[31m-        'datasets',[m
[31m-        'transformers',[m
[31m-        'tqdm',[m
[31m-        'pandas'[m
[31m-    ][m
[32m+[m[32m        "einops",[m
[32m+[m[32m        "numpy",[m
[32m+[m[32m        "torch",[m
[32m+[m[32m        "datasets",[m
[32m+[m[32m        "transformers",[m
[32m+[m[32m        "tqdm",[m
[32m+[m[32m        "pandas",[m
[32m+[m[32m        "datasets",[m
[32m+[m[32m        "wandb",[m
[32m+[m[32m        "fancy_einsum",[m
[32m+[m[32m    ],[m
 )[m
